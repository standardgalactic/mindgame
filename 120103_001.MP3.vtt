WEBVTT

00:00.000 --> 00:10.920
The Blue Pill. Self-improving AI. Self-improving AI is a meme that has been circulating since

00:10.920 --> 00:18.760
the 1980s. Current proponents of the idea include Wastram and Omihandro. My own summary goes

00:18.760 --> 00:25.920
something like this. If we get any kind of AGI going, no matter how slow it is and how

00:25.920 --> 00:31.480
buggy it is, we can give it access to its own source code and let it analyze it and

00:31.480 --> 00:37.280
clean up and fix the bugs and then rewrite its code to be as good as it can make it.

00:37.280 --> 00:43.760
We then start up the slightly smarter AGI and repeat the process until the AGI's get

00:43.760 --> 00:51.560
super intelligent. On the surface, this is irrefutable. We already have examples of systems

00:51.560 --> 00:58.240
improving themselves. We can buy a cheap 3D printer and then quite cheaply print out parts

00:58.240 --> 01:05.000
for a much better 3D printer. Or to make computer chips that go into computers that design better

01:05.000 --> 01:12.160
computer chips. Not to mention evolution of all species in nature. I look at it from an

01:12.160 --> 01:17.880
epistemologist point of view and say, that's a hard line reductionist idea that should

01:17.880 --> 01:25.640
not have made it out of the 20th century. The idea, as its inception, imagined an AGI

01:25.640 --> 01:31.480
as something that was written by teams of human programmers using software development

01:31.480 --> 01:37.800
tools and mathematical equations. What I think the only thing that even approximates this

01:37.800 --> 01:43.760
outcome is that the code is perfect, and humans as well as machines all agree there are no

01:43.760 --> 01:51.120
more improvements to be made. And the resulting AGI's are still not super intelligent. The

01:51.120 --> 01:56.480
most likely outcome is that we all realize the folly in this argument and won't even

01:56.480 --> 02:03.800
try. It's not about the code. The number of lines of code in AI related projects has been

02:03.800 --> 02:26.000
declining rapidly. 2012. 34,000 lines.py.kudukrzebski et al. for ImageNet. 2013. 1571 lines of

02:26.000 --> 02:37.400
Lua to Play Atari games. 2017. 196 lines of Keras to Implement Deep Dream. 2018. Less

02:37.400 --> 02:45.520
than 100 lines of Keras for research paper-level results. And all of these, except Saig, included

02:45.520 --> 02:52.000
as the most famous example of a 20th century reductionist AI system, demonstrates new levels

02:52.000 --> 02:58.680
of power of machine learning. The limits to intelligence are not in the code. In fact,

02:58.680 --> 03:05.320
they are not even technological. The limit of intelligence is the complexity of the

03:05.320 --> 03:12.080
world. Admission is unavailable. The main purpose of intelligence is to guess, to jump

03:12.080 --> 03:18.440
to conclusions on scant evidence, and to do it well, based on a large set of historical

03:18.440 --> 03:24.840
patterns of problems and their solutions or events and their consequences. Because scant

03:24.840 --> 03:31.600
evidence is all we will ever have, we don't even know what goes on behind our back. And

03:31.600 --> 03:37.240
because our intelligence is guessing, I have repeatedly claimed that, all intelligences

03:37.240 --> 03:43.480
are fallible. We are already making machines that are better than humans in some aspect

03:43.480 --> 03:49.960
of guessing. Protein folding and playing go are examples of this. And these machines

03:49.960 --> 03:55.120
will get bigger and better at what they do and will be superhuman in various ways and

03:55.120 --> 04:02.520
in many problem domains, simply based on larger capacity to hold, look up, or search useful

04:02.520 --> 04:09.080
patterns. The code doing that can be hand optimized to the point where any AI improvement

04:09.080 --> 04:15.280
would be insignificant. My own code in the inner loop for understanding any language

04:15.280 --> 04:22.200
on the planet, once it has learned it, in inference mode is about 90 lines of Java.

04:22.200 --> 04:29.040
We can expect a best minor improvements to efficiency and speed. It comes down to the

04:29.040 --> 04:38.240
corpus. In my domain, NLU, simple tests can be scored at 100% after a few minutes of learning

04:38.240 --> 04:44.840
on a laptop. Continue learning for days and weeks would provide a larger sample set of

04:44.840 --> 04:50.160
vocabulary in appropriate contexts, which would mainly correct misunderstandings in

04:50.160 --> 04:58.240
corner cases. But these corporal are not comparable by several orders of magnitude, to the gathered

04:58.240 --> 05:05.280
life experience of a human at age 25. The main limit of intelligence is corpus size

05:05.280 --> 05:12.440
in ML situation. Future artificial intelligences will be nothing like what AGI fans have been

05:12.440 --> 05:19.520
fearmongering about. These are 20th century reductionist AI ideas. The components are

05:19.520 --> 05:26.800
blind to the most fundamental basics of epistemology. Reductionist good old fashioned AI has been

05:26.800 --> 05:32.480
demonstrated to being inferior in their own domains to even semi-trivial machine learning

05:32.480 --> 05:42.200
methods. We need AGL, not AGI. Machines learning to code. As of this writing, there are a handful

05:42.200 --> 05:47.480
of available code writing systems based on ML technology that has learned from large

05:47.480 --> 05:55.360
quantities of open source code. For example GitHub Copilot, OpenAI Codex, and Amazon Code

05:55.360 --> 06:02.800
Whisperer. They have not yet surpassed human programmers. But it's not about writing code

06:02.800 --> 06:09.040
either. AI's writing code is about as silly as AI magazine covers with pictures of robots

06:09.040 --> 06:16.840
typing, wink wink. In the future, if we want the computer to do something, we will have

06:16.840 --> 06:23.800
a conversation, speaking and listening, with the computer. The conversation is at the level

06:23.800 --> 06:30.560
of discussing a problem with a competent coworker or professional. It may spontaneously ask

06:30.560 --> 06:39.080
clarifying questions. I call this, contiguously rolling topic, mixed initiative dialogue, others

06:39.080 --> 06:46.080
talk of these bots as dialogue Asians. But this will go beyond Siri or Alexa, and when

06:46.080 --> 06:53.080
the computer understands exactly what you want done. It just does it. Why would reductionist

06:53.080 --> 07:00.000
style programming be a necessary step? Yes, there will still be lots of places where we

07:00.000 --> 07:05.960
want to use code. But whether that code is written by humans or AI's will make much

07:05.960 --> 07:12.120
less of a difference than we might expect based on today's use of computers.

07:12.120 --> 07:21.160
The Pink Pill. The Wisdom Salon. Wisdom Salon is an online world cafe. The World Cafe protocol

07:21.160 --> 07:28.720
is a recipe for organizing conversations that matter on a large scale. Thousands of people

07:28.720 --> 07:36.040
can cooperate in order to bring clarity to complex issues. This is a post-mortem summary

07:36.040 --> 07:43.280
for my interrupted wisdom salon project. I have all the code in an archive, but it requires

07:43.280 --> 07:49.680
a complete rewrite in order to fix the two biggest problems. The switch from flash,

07:49.680 --> 07:57.400
hack, to HTML5 for video and the cost of video connections. I know how to fix these but I'm

07:57.400 --> 08:04.040
busy working on understanding machines. At the moment, I am looking for someone to take

08:04.040 --> 08:10.960
this over. I also observe that there is a need for something like this. I see things discussed

08:10.960 --> 08:17.360
on Quora that would make good topics for a wisdom salon. I happen to believe video in

08:17.360 --> 08:21.720
spoken words are an important component for many reasons.

08:21.720 --> 08:30.720
Wisdom. Knowledge and information can easily be found on the web. But what about wisdom?

08:30.720 --> 08:36.440
Intelligence is based on gathered knowledge. Wisdom is based on gathered experience. To

08:36.440 --> 08:46.280
get wiser, seek out more experiences. Engage yourself. Do more stuff. Travel. Talk to people

08:46.280 --> 08:53.440
to share their experiences. Conversation with others is the easiest way to gain wisdom.

08:53.440 --> 09:00.400
But not all conversations are equal. We want conversations that matter. The World Café

09:00.400 --> 09:07.360
Protocol. The World Café Protocol is a recipe for organizing such conversations that matter

09:07.360 --> 09:14.160
on a large scale. Thousands of people can cooperate in order to bring clarity to complex

09:14.160 --> 09:21.440
issues. To find out more, buy the book or study the World Café website. But this is

09:21.440 --> 09:28.320
how it typically works. In some conference facilities or gymnasium, the organizers provide

09:28.320 --> 09:35.800
dozens to hundreds of square tables. Each has four chairs, a box of crayons, and a piece

09:35.800 --> 09:42.600
of butcher paper as a tablecloth. Stakeholders from all walks of life get invited and sit

09:42.600 --> 09:50.240
down at the tables. This could be a mixture of farmers, teachers, politicians, in corporate

09:50.240 --> 09:57.800
environments. Sometimes this is everybody in the company. Organizers now unveil a carefully

09:57.800 --> 10:03.960
phrased focusing question as the topic of the conversations. It is important that the

10:03.960 --> 10:11.160
question is positive and focusing. For education reform, don't ask, what is wrong with our

10:11.160 --> 10:18.480
education system? Instead, ask, what could a great school also be? The four people at

10:18.480 --> 10:24.600
each table now start a conversation around the question. Everyone takes notes on the

10:24.600 --> 10:33.400
butcher paper, using the crayons. After 20 minutes, a gong rings. Three people. Everyone

10:33.400 --> 10:40.040
except south in duplicate bridge terms. At each table get up and move to other tables

10:40.040 --> 10:47.280
at random. Through fresh random people sit down at each table. South now first explains

10:47.280 --> 10:53.360
to the newcomers what the notes on the tablecloth mean. This provides a kind of lightweight

10:53.360 --> 10:59.480
continuity from the previous conversation at this table. The three newcomers comment

10:59.480 --> 11:05.160
on these notes and add fresh comments. The best parts of what was said at their previous

11:05.160 --> 11:12.920
tables. These conversations unfold very naturally. Four strangers can easily have a friendly

11:12.920 --> 11:19.880
conversation about complex things that matter. They don't even have to introduce themselves.

11:19.880 --> 11:27.680
They contribute their wisdom and experiences. Not their resumes. Conversations now continue

11:27.680 --> 11:35.440
for another 20 minutes. The gong rings again, and the shuffling repeats. After two to three

11:35.440 --> 11:40.880
hours, the session is over and the butcher papers are gathered by the organizers into

11:40.880 --> 11:48.920
what is called the harvest. They are summarized in some time later. Perhaps, after lunch,

11:48.920 --> 11:56.560
the results are shared with all the stakeholders. Why this works so well? Someone pushing a

11:56.560 --> 12:03.760
bad idea of theirs at every table can spam at worst 27 people in three hours. A good

12:03.760 --> 12:10.680
idea. Introduced at the first table and repeated by all participants at subsequent tables will

12:10.680 --> 12:17.840
reach over 100,000 people or the majority of the audience, whichever is smaller. This

12:17.840 --> 12:24.000
is the filtering power of the World Café protocol. Wisdom Salon is an online World

12:24.000 --> 12:31.400
Café. Sadly, the Wisdom Salon project has been suspended because of changing infrastructure

12:31.400 --> 12:38.440
and cost structure for online video transmissions, and because of lack of time on my part. It

12:38.440 --> 12:44.680
is possible to restart the project using current video technology and with funding and a larger

12:44.680 --> 12:52.280
team. If interested in contributing to this, please get in touch. What follows is the original

12:52.280 --> 13:00.280
high-level design specification, written in the present tense, design specification.

13:00.280 --> 13:08.840
The Wisdom Salon is a 24-7 online World Café implemented as a video chat site. Conversations

13:08.840 --> 13:14.640
have four participants, but each conversation can also have a passive and quiet audience

13:14.640 --> 13:22.800
of any size. All conversations are always public. All conversation participants are

13:22.800 --> 13:29.960
known by their login identities. Why would anyone want to participate? The main purpose

13:29.960 --> 13:36.640
of Wisdom Salon is increased wisdom and improved clarity and complex issues for the participants.

13:36.640 --> 13:43.360
This is your main benefit. This is why you would want to participate. You will not get

13:43.360 --> 13:49.920
lags, but you might earn a local currency, called, Influence, that you can selectively

13:49.920 --> 13:52.600
use to extend your influence.

13:52.600 --> 13:59.720
Goal. The goal is specifically not to find the best grains of wisdom in the harvest.

13:59.720 --> 14:05.920
The grains are there mainly to provide continuity and shorten the time to get to talking about

14:05.920 --> 14:12.200
things that matter. The system is there to provide the users a chance to analyze large

14:12.200 --> 14:20.720
and complex issues with others in conversation and in exchange of experiences. Do not underestimate

14:20.720 --> 14:27.520
how different an interactive conversation is from a web search or reading a book. Have

14:27.520 --> 14:34.040
you ever spent days studying something without getting it only to have someone set you straight

14:34.040 --> 14:40.280
in two minutes of conversation? Have you ever been in a meeting where the resolution is

14:40.280 --> 14:46.000
something none of the participants even understood when the meeting started?

14:46.000 --> 14:53.040
Sample questions. What kinds of questions demonstrate the power of the Wisdom Salon?

14:53.040 --> 15:01.080
Consider these samples. I am considering a midlife career change. What matters? Where

15:01.080 --> 15:09.560
should I retire, and why there? Should I pursue a career in engineering or medicine?

15:09.560 --> 15:16.400
Lifestyle design in interesting times. What is the true promise of genetics research and

15:16.400 --> 15:23.800
why should I care? What movies should I let my children watch, and why?

15:23.800 --> 15:32.720
Musical education for my child. What matters? What instruments, and why? What is it really

15:32.720 --> 15:41.640
like to be a soldier in places like Afghanistan and Iraq? Should I retire in Costa Rica? User

15:41.640 --> 15:49.000
experience. People arrive when they want and leave when they want. They can engage in multiple

15:49.000 --> 15:58.000
ways. Upon entering the site, users are presented with the, at the moment, most popular conversation,

15:58.000 --> 16:04.640
the one with the largest audience. Below the conversation, there will be a list of other

16:04.640 --> 16:11.800
popular conversations, headed by conversations and topics the user may have watched or previously

16:11.800 --> 16:18.800
participated in. They can browse all ongoing conversations much like watching talk shows

16:18.800 --> 16:25.640
on television. They can select from hundreds of questions to find something that interests

16:25.640 --> 16:33.000
them, or add their own. Instead of a butcher paper, they can leave notes on each question

16:33.000 --> 16:39.680
known as, grains of wisdom, to provide the lightweight continuity from table to table.

16:39.680 --> 16:47.120
They can vote on these grains of wisdom so that they better result rise to the top. Results

16:47.120 --> 16:54.080
are immediately visible to all. They can observe what other people say and how they behave

16:54.080 --> 16:59.880
and modify their own social graph to improve their chances of interaction with the best

16:59.880 --> 17:06.880
people. A local currency is earned by passive engagement per hour, more of it is earned

17:06.880 --> 17:12.600
by participating in conversations, and the currency is used to pay for the privilege

17:12.600 --> 17:19.400
of posting a comment, because posting cost currency, spelling the grains of wisdom will

17:19.400 --> 17:26.400
be limited. A topic without currently active conversations still allows you to browse the

17:26.400 --> 17:33.800
grains of wisdom on the topic, and if you have influence, you can vote on the grains or notes

17:33.800 --> 17:40.000
that you like or otherwise agree with, and you can restart the topic by creating a table

17:40.000 --> 17:47.640
and hope others will join. Four main uses of wisdom salon. The site enables, but doesn't

17:47.640 --> 17:55.200
enforce the World Cafe protocol. You can use the site for several different purposes.

17:55.200 --> 18:01.520
As entertainment and education, passively watching conversations among your peers,

18:01.520 --> 18:08.080
much like flipping channels on television. To get both factual information and broad

18:08.080 --> 18:16.080
ranging personalized advice from experts. To share your expertise in fields you understand.

18:16.080 --> 18:24.080
To do micromantering. To find an audience for storytelling and sharing personal experiences

18:24.080 --> 18:32.640
from your life. To gain wisdom and personal clarity in complex issues. To debate the major

18:32.640 --> 18:39.680
issues of the day in person and productively selected and well behaved groups. To find

18:39.680 --> 18:45.600
new interesting and competent friends by observing their behavior and then befriending them,

18:45.600 --> 18:54.320
much like other social media. Any active conversation starts a 20 minute clock bar moving. You

18:54.320 --> 19:01.560
can leave anytime. System provides some incentive to stay the full 20 minutes. On the other

19:01.560 --> 19:09.000
hand, you don't have to leave after 20 minutes. If you like, you can continue conversation

19:09.000 --> 19:16.920
along as you want. But we expect a large fraction of people to adhere to the protocol. We believe

19:16.920 --> 19:24.240
this maximizes the wisdom gain per session. Without the right people, the system is worthless.

19:24.240 --> 19:32.000
Do not be discouraged. Facebook would be worthless with only 10 people on it. Wisdom salon really

19:32.000 --> 19:38.480
requires at least 50 people to be on the system before you are likely to find a conversation

19:38.480 --> 19:44.960
around a question you actually care about anytime you join. So nobody knows if this

19:44.960 --> 19:50.600
will work or not, and it may take a while before the system matures enough to attract

19:50.600 --> 19:56.400
a sufficient repeat audience to become what I designed it for. If you don't like it

19:56.400 --> 20:03.000
at first, please try again. It might well improve, and you might get lucky to get into

20:03.000 --> 20:10.360
an amazing conversation when you least expect it. Welcome to my experiment.

20:10.360 --> 20:19.360
The lavender pill. Model free AI. Don't model the world. Just model the mind. It's a lot

20:19.360 --> 20:29.120
easier. With some poetic freedom, I'd like to claim 1. Model the world. 10 billion lines

20:29.120 --> 20:40.960
of code. 2. Model the brain. 10 million lines of code. 3. Model the mind. 10,000 lines of

20:40.960 --> 20:48.640
code. Number one is regular programming. We make computers perform actions in a context

20:48.640 --> 20:54.480
that matches the programmer's mental model of some relevant parts of the world. Number

20:54.480 --> 21:01.160
two is neuroscience-based models of neurons, synapses and other biological structures and

21:01.160 --> 21:09.920
systems in brains. The number three is epistemology-based models of learning, understanding, reasoning,

21:09.920 --> 21:17.160
prediction, abstraction, and other holistic and emergent phenomena. Epistemology-based

21:17.160 --> 21:23.440
methods require a rather minimal infrastructure to support whatever operations these concepts

21:23.440 --> 21:30.680
require. I put models within irony quotes because they are strictly speaking metamodels

21:30.680 --> 21:37.360
because they are used in metascales. They are not about skills, such as English or folding

21:37.360 --> 21:44.280
proteins. They are about how to acquire such skills by learning from our mistakes.

21:44.280 --> 21:52.840
The purple pill. Corpus congruence. Understanding in brains and machines can be defined and

21:52.840 --> 22:01.960
measured as corpus congruence. Corpus congruence as a metric spans up almost all of NLP. Understanding

22:01.960 --> 22:08.440
in brains and machines can be defined and measured as corpus congruence. Let's consider

22:08.440 --> 22:16.040
this in the machine learning sense. If a machine is model-free, holistic, as all general understanders

22:16.040 --> 22:21.960
have to be in order to not get trapped into a limited model, then all it ever knows comes

22:21.960 --> 22:28.080
from the corpus it was trained on. And all it really can say is, this is more like my

22:28.080 --> 22:35.480
corpus than that. Or, this is more like these documents in my corpus than those corpus congruence

22:35.480 --> 22:44.160
as a metric spans up almost all of NLP. Because most of NLP is doxen in various guises. Given

22:44.160 --> 22:50.880
two documents A and B in some corpus, a classifier can say that an unknown document, which we

22:50.880 --> 22:59.360
can call U, is more like it than B given this capability we can build. Classification and

22:59.360 --> 23:09.640
clustering by using A, B, up to N as defining classes. Filtering by using A, wanted dox

23:09.640 --> 23:21.440
and B, unwanted dox. Summoned analysis by using A, negative dox and B, positive dox.

23:21.440 --> 23:29.440
Entity extraction by softly matching termed against lists of known entities. Doxen, find

23:29.440 --> 23:37.080
me more documents like this one. Reductionist and NLP uses all of these at the bag of words

23:37.080 --> 23:45.320
or word count levels for things like web search, span filtering, and clustering. Holistic

23:45.320 --> 23:52.600
NLU aims to do the same based on the meanings expressed in sentences and paragraphs. But

23:52.600 --> 23:59.720
semantic corpus congruence is still corpus congruence. Common sense now becomes, is the

23:59.720 --> 24:06.000
proposition before me congruent with my entire world model, as required by learning things

24:06.000 --> 24:13.440
from my training corpus. If it is well known, then we can likely ignore it this time, and

24:13.440 --> 24:18.960
if it is not, then the next question will be, is it close enough that it might be worth

24:18.960 --> 24:25.320
while extending the world model with this information? If the answer is no, then the

24:25.320 --> 24:32.160
input is by its definition nonsense. Otherwise it is either a new fact or a lie, but since

24:32.160 --> 24:39.760
we cannot tell, we have to accept it, possibly with a note that this is fresh, untested knowledge

24:39.760 --> 24:47.360
that may turn out to be irrelevant, false, counterproductive, or noise. Next we can note

24:47.360 --> 24:54.040
that it doesn't matter whether documents are text or images, or input from a point cloud

24:54.040 --> 25:00.280
of sensors for robots or autonomous vehicle sensors. And finally we can note that this

25:00.280 --> 25:07.640
definition also holds for humans if we take our corpus to be everything we've experienced

25:07.640 --> 25:09.640
since birth.

25:09.640 --> 25:12.640
Monika's Little Pills

25:12.640 --> 25:14.160
Chapter 1

25:14.160 --> 25:17.760
Why I Works

25:17.760 --> 25:24.760
Intelligence equals understanding plus reasoning. Interest in artificial intelligence is exploding,

25:24.760 --> 25:31.760
and for good reasons, computers and cars, phone apps, and on the web can do amazing

25:31.760 --> 25:39.280
things that we simply could not do before 2012. What's going on? This is an attempt

25:39.280 --> 25:45.840
to explain the current state of AI to a general audience without using mathematics, computer

25:45.840 --> 25:53.480
science, or neuroscience, discussions at these levels with focus on how AI works. Here I

25:53.480 --> 26:01.040
will discuss this at the level of epistemology and will try to explain why it works. Epistemology

26:01.040 --> 26:08.440
sounds scary, but it really isn't. It's mostly scary because it is unknown, it is not taught

26:08.440 --> 26:15.840
in schools anymore, which is a problem, because we now desperately need this branch of philosophy

26:15.840 --> 26:24.200
to guide our AI development. Epistemology discusses things like reasoning, understanding, learning,

26:24.200 --> 26:31.960
novelty, problem solving in the abstract, how to create models of the world, etc. These

26:31.960 --> 26:38.080
are all concepts one would think would be useful when working with artificial intelligences,

26:38.080 --> 26:44.600
but most practitioners enter the field of AI without any exposure to epistemology which

26:44.600 --> 26:51.320
makes their work more mysterious and frustrating than it has to be. I think of it epistemology

26:51.320 --> 26:57.640
as the general base for everything related to knowledge and problem solving. Science forms

26:57.640 --> 27:03.320
a small special case subset domain where we solve well-formed problems of the kind that

27:03.320 --> 27:10.040
science is best at. In the epistemology outside of science we are free to productively also

27:10.040 --> 27:16.400
discuss pre-scientific problem solving strategies, which is what brains are using most of the

27:16.400 --> 27:24.840
time. More later, intelligence equals understanding plus reasoning. In his book, Thinking Fast

27:24.840 --> 27:31.800
and Slow, Daniel Kahneman discusses the idea that human minds use two different and complementary

27:31.800 --> 27:38.440
processes, two different modes of thinking, which we call understanding and reasoning.

27:38.440 --> 27:44.600
The idea has been discussed for decades and has been verified using psychological studies

27:44.600 --> 27:52.200
and by neuroscience. Subconscious intuitive understanding is the full name of the fast

27:52.200 --> 27:59.280
thinking or system one thinking. It is fast because the brain can perform many parts of

27:59.280 --> 28:06.240
this task in parallel. The brain spends a lot of effort on this task. Conscious logical

28:06.240 --> 28:13.840
reasoning is the full name of slow thinking or system two thinking. To many people's

28:13.840 --> 28:21.440
surprise, this is very rarely used in practice. By soundbite for this is, you can make breakfast

28:21.440 --> 28:27.840
without reasoning. Almost everything we do on a daily basis in our rich mundane reality

28:27.840 --> 28:33.800
is done without a need to reason about it. We just repeat whatever worked last time we

28:33.800 --> 28:41.960
performed this task. Real experience driven. Intuitive means that the system can very quickly

28:41.960 --> 28:48.400
provide solutions to very complex problems but those solutions may not be correct every

28:48.400 --> 28:54.960
time. Logical means that answers are always correct as long as input data is correct and

28:54.960 --> 29:02.240
sufficient, which is not true in our rich mundane reality. It can only be true in a mathematically

29:02.240 --> 29:11.120
pure model space. If you like logic, you must also like models. Subconscious means we have

29:11.120 --> 29:18.020
no conscious, introspective access to these processes. You are reading this sentence

29:18.020 --> 29:24.320
and you understand it fully but you cannot explain to anyone, including yourself, how

29:24.320 --> 29:30.840
or why you understand it. Conscious means we are aware of the thought, we can access

29:30.840 --> 29:37.960
it through introspection and we may find reasons to why we believe a certain idea. Expensive

29:37.960 --> 29:44.160
is on the list because brains spend most of their effort on this understanding part. We

29:44.160 --> 29:51.800
really shouldn't be surprised that AI now requires very powerful computers. More later.

29:51.800 --> 29:58.240
In contrast, reasoning is efficient. It is most useful when you are stuck in a novel

29:58.240 --> 30:05.480
situation or experience and understanding doesn't help you. Or perhaps you need to plan ahead

30:05.480 --> 30:12.600
or need to find reasons for why something happened after the fact. It is used at a formal level

30:12.600 --> 30:20.960
in the sciences. Reasoning is important but just rarely needed or used. Finally, understanding

30:20.960 --> 30:28.480
is model-free and reasoning is model-based. This is likely the most important distinction

30:28.480 --> 30:33.880
to people who are implementing intelligent systems since it provides a way to keep the

30:33.880 --> 30:40.080
implementation on the correct path when the going gets rough. We cannot discuss these

30:40.080 --> 30:47.440
issues quite yet but if you are curious you can watch the videos at Vimeo.com which discuss

30:47.440 --> 30:53.920
this distinction at length. Think of the appearance in this table as a kind of foreshadowing.

30:53.920 --> 31:00.680
All of this groundwork allows me to state the main point of this section. We have known

31:00.680 --> 31:07.880
for a long time that brains use these two modes. But the AI research community has been spending

31:07.880 --> 31:13.560
over much effort on the reasoning part and has been ignoring the understanding part for

31:13.560 --> 31:21.400
60 years. We had several good reasons for this. Until quite recently, our machines were too

31:21.400 --> 31:28.320
small to run any useful sized neural network. And also, we didn't have a clue about how

31:28.320 --> 31:35.440
to implement this understanding. But that is exactly what changed in 2012 when a group

31:35.440 --> 31:40.960
of AI researchers from Toronto effectively demonstrated that deep neural networks could

31:40.960 --> 31:47.320
provide a simple kind of shallow and hollow proto-understanding. Well, they didn't call

31:47.320 --> 31:54.480
it that, but I do. I will look just a little into the future and overstate this just a

31:54.480 --> 32:02.440
little in order to make it more memorable. Deep neural networks can provide understanding.

32:02.440 --> 32:08.480
This new phase of AI took decades to develop, but it would never have happened without people

32:08.480 --> 32:15.600
like the group led by Jeffrey Hinton at the University of Toronto, who spent 34 plus years

32:15.600 --> 32:22.800
to develop the deep neural network technology we now call, deep learning. A number of breakthroughs

32:22.800 --> 32:31.160
from 1997 to 2006 led to a number of successful demonstrations, including first prizes in

32:31.160 --> 32:38.160
AI competitions in 2012. And we therefore count this year as the birth year of machine

32:38.160 --> 32:45.160
understanding. To an outsider, it may look like an older program or phone app might be

32:45.160 --> 32:51.200
understanding whatever the app is doing, but that understanding really only happened in

32:51.200 --> 32:57.480
the mind of the programmer creating the app. The programmer first simplified the problem

32:57.480 --> 33:04.640
in their own head by discarding a lot of irrelevant detail using programmer's understanding.

33:04.640 --> 33:10.640
The simplified mental model of the problem domain could then be explained to a computer

33:10.640 --> 33:16.720
in the form of a computer program. What is changing is that computers are now making

33:16.720 --> 33:24.520
these models themselves. The first bullet point describes regular programming, including

33:24.520 --> 33:34.120
old style AI programs. AI has, since 1955, provided many novel and brilliant algorithms

33:34.120 --> 33:40.800
that we now use in programs everywhere. But when you contrast old style AI to understanding

33:40.800 --> 33:47.960
systems, the old kind of AI is basically indistinguishable from any other kind of programming we do

33:47.960 --> 33:54.960
nowadays. The second bullet point describes the recent developments. Deep neural networks

33:54.960 --> 34:00.160
are so different from regular programs that we have to acknowledge them as a different

34:00.160 --> 34:07.160
computational paradigm. This is why they took almost four decades to develop. And the

34:07.160 --> 34:14.440
paradigm, being pre-scientific and model-free, is difficult to grasp if you receive a solid

34:14.440 --> 34:21.440
reductionist and model-based education. It takes a long time for an established AI practitioners

34:21.440 --> 34:27.840
or experienced programmer to switch. People who are just starting out in AI have an easier

34:27.840 --> 34:33.000
time assimilating this new paradigm since they haven't had a full career's worth of

34:33.000 --> 34:39.320
experience and success using old style AI techniques. The amount of work we have to

34:39.320 --> 34:45.720
do to get a deep neural network to understand is surprisingly small, and companies like

34:45.720 --> 34:51.000
Google and Cintiens are working on eliminating the remaining effort of programming neural

34:51.000 --> 34:58.680
networks. This is where things will get really weird. When the deep neural network, DNN,

34:58.680 --> 35:04.040
understands enough about the world and about the problem it is faced with, then we no longer

35:04.040 --> 35:11.560
need a programmer to acquire this understanding. Let me elaborate. Programmers are employed

35:11.560 --> 35:17.760
to bridge two different domains. They first have to study whatever application domain

35:17.760 --> 35:23.720
they are working on. For instance, if they are writing an airline ticket reservation

35:23.720 --> 35:30.520
system they will have to learn a lot of detailed information about airlines, airline tickets,

35:30.520 --> 35:38.400
flights, luggage, etc. and then know to provide features for unusual cases such as cancelled

35:38.400 --> 35:44.720
flights. And then the programmer uses their understanding of the problem domain to explain

35:44.720 --> 35:50.440
to a computer how it can reason about these things, but the programmer cannot make the

35:50.440 --> 35:56.640
system understand, it can only put in the hollow and fragile kind of reasoning, as a

35:56.640 --> 36:04.080
program with many of thin cases, and any misunderstandings the programmer has about the problem domain

36:04.080 --> 36:12.040
will become bugs in the computer program. Notice the shift in terminology. More later.

36:12.040 --> 36:20.040
But today, for certain classes of moderately complex problems, we can use a DNN to automatically

36:20.040 --> 36:26.760
learn for itself how to understand the problem, which means we no longer need a programmer

36:26.760 --> 36:33.520
to understand the problem. We have delegated our understanding to a machine, and if you

36:33.520 --> 36:39.880
think about that for a minute you will see that that's exactly what an AI should be doing.

36:39.880 --> 36:46.000
It should understand all kinds of things, so that we humans won't have to. And there

36:46.000 --> 36:52.000
are two common situations where this will be a really good idea. One is when we have

36:52.000 --> 36:58.960
a problem we cannot understand ourselves. We know a lot of those, starting with cellular

36:58.960 --> 37:05.440
biology. The other common case will be when we understand the problem well, but making

37:05.440 --> 37:10.920
the machine understand it well enough to get the job done is cheaper and easier than any

37:10.920 --> 37:17.800
alternative. MoonBoss accomplish this level of using old style AI methods, but I predict

37:17.800 --> 37:23.840
we will one day be flooded with similar, but DNN based devices that understand several

37:23.840 --> 37:31.400
aspects of domestic maintenance, as well as we do. Do machines really understand? If we

37:31.400 --> 37:37.840
give a picture like this to a DNN trained on images it will identify the important objects

37:37.840 --> 37:44.680
in the image and provide the rectangles, called, owning boxes as approximations to where the

37:44.680 --> 37:51.320
objects are. The text on the right says, woman in white dress standing with tennis racket

37:51.320 --> 37:58.280
to people in green behind her, which is not a bad description of the image. It could be

37:58.280 --> 38:05.160
used as the basis for a test for English skill level for adult education placement, for all

38:05.160 --> 38:12.480
practical purposes. This is understanding. We had no idea how to make our computers do

38:12.480 --> 38:21.400
this before 2012. This is a really big deal. This feat requires not only a new algorithm,

38:21.400 --> 38:28.840
it requires a new computational paradigm and images to a computer, a single long sequence

38:28.840 --> 38:37.560
of numbers denoting values for red, blue and green colors and values from 0 to 255. It

38:37.560 --> 38:44.480
also knows how wide the image is. How does it get from this very low level representation

38:44.480 --> 38:49.800
to knowing that there is a woman with a tennis racket in the image? This is what William

38:49.800 --> 38:56.240
Calvin has called, a river that flows uphill. There are very few mechanisms that can go

38:56.240 --> 39:03.680
in this direction, from low levels to high levels. Calvin used the term to describe evolution,

39:03.680 --> 39:11.120
and I can use this quote to describe understanding. I like to think of evolution as, nature's

39:11.120 --> 39:17.880
understanding because the phenomena are very similar at several levels. Evolution of species

39:17.880 --> 39:23.480
can bring forth advanced species starting from simpler species in the same manner that

39:23.480 --> 39:29.480
understanding is the discovery and reuse of high level concepts and low level input.

39:29.480 --> 39:36.640
In contrast, reasoning proceeds by breaking problems into sub-problems and solving those,

39:36.640 --> 39:44.600
which is a, flowing downhill, kind of strategy. In mathematics we accept, and many mathematicians

39:44.600 --> 39:51.040
only accept this reluctantly, that we need to use induction to move uphill in abstractions,

39:51.040 --> 39:58.240
and that's a very limited uphill movement at that. Epistemology allows for much stronger

39:58.240 --> 40:05.000
uphill moves. This is known as, jumping to conclusions on scant evidence and it's allowed

40:05.000 --> 40:11.800
in epistemology based pre-scientific systems. As an aside, here's a pretty deep related

40:11.800 --> 40:19.400
thought. In nature, evolution reuses anything that works. I like to think that understanding

40:19.400 --> 40:26.560
is a spandrel of evolution itself. Neural Darwinism certainly straddles this gap. Could

40:26.560 --> 40:33.880
be coincidence, or the only answer that will work at all. More later, we doubled our AI

40:33.880 --> 40:41.080
toolkit in 2012. We can now use these deep neural networks as components in our systems

40:41.080 --> 40:47.160
to provide understanding of certain things like vision, speech, and other problems that

40:47.160 --> 40:54.240
require that we discover high level concepts and low level data. The technical, epistemology

40:54.240 --> 41:00.840
level name for this uphill flow in processes, reduction, and we'll be using that term later

41:00.840 --> 41:06.360
after we explain what it means. Let's look at what the industry is doing with their new

41:06.360 --> 41:14.080
found toys. This is my view of what I think Tesla is doing, based on public sources in

41:14.080 --> 41:21.040
their self-driving, autopilot, cars, cameras feed vision understanding components based

41:21.040 --> 41:27.480
on deep learning, and radar feeds to radar understanding components. These supply bounding

41:27.480 --> 41:33.880
boxes in 2D or 3D with additional information like, there's a woman with a tennis racket

41:33.880 --> 41:39.800
ahead to a traffic reasoning component that uses regular programming, or some old style

41:39.800 --> 41:46.640
AI like a rule based system to actually control the car based on the vision and radar inputs,

41:46.640 --> 41:54.120
and the driver's desires. But this is not the only possible configuration. George Hopps

41:54.120 --> 42:01.480
at Comma.ai, a team at NVIDIA Corporation, and the deep Tesla class at MIT are using

42:01.480 --> 42:06.840
a simpler architecture with just a neural network that implements lane following and

42:06.840 --> 42:12.840
other simple driving behaviors directly in one single deep neural network. There's room

42:12.840 --> 42:20.360
for improvement, but there a big step in the direction we want to move in. Future automotive

42:20.360 --> 42:26.400
systems will likely integrate everything about driving into one single neural network, or

42:26.400 --> 42:33.240
something that effectively behaves as one. Vision, traffic, the car itself including

42:33.240 --> 42:40.080
various functionality like windscreen wipers, lights, and entertainment, how to drive in

42:40.080 --> 42:47.720
a safe and polite manner, and to understand also the drivers or car owners desires. And

42:47.720 --> 42:53.760
if we've gotten that far, then it is a given that we will have speech input and output

42:53.760 --> 42:59.320
so that the driver can have a conversation with the car while driving, and can just

42:59.320 --> 43:05.920
advise it in case it does something wrong. We are nowhere close to this today. But after

43:05.920 --> 43:13.240
a DNM breakthrough or two, who knows how quickly these kinds of systems become available. We

43:13.240 --> 43:20.520
can already see an increasing stream of new features built using understanding components.

43:20.520 --> 43:27.520
This article, and the next, are expansions of a talk given on June 10, 2017 at the San

43:27.520 --> 43:35.280
Francisco Bill Conference. A decade ago I created artificialintuition.com. I now have

43:35.280 --> 43:42.120
a lot more to say, but I need to split this meme package into digestible chunks. This

43:42.120 --> 43:48.120
takes a lot of effort to get right. If you liked this article and would like to see more

43:48.120 --> 43:55.120
like it then you can support my writing and my research in many ways, small to large,

43:55.120 --> 44:00.760
like and share these ideas with someone who might want to invest in sentience incorporated

44:00.760 --> 44:06.200
or might be otherwise interested in my research on a novel language understanding technology

44:06.200 --> 44:13.720
called organic learning. More on that later. I do not receive external funding from any

44:13.720 --> 44:20.220
investors for this research. You can support my research and writing directly at the donation

44:20.220 --> 44:29.400
section at artificialintuition.com. Chapter 2. Our Greatest Invention, Model Based Problem

44:29.400 --> 44:37.560
Solving. The first chapter, why AI works, provided the big picture of AI and understanding

44:37.560 --> 44:44.640
machines. Next we will focus on how to actually implement understanding in a computer. But

44:44.640 --> 44:50.600
before we can attack that core issue, we need to simplify the journey a bit by defining

44:50.600 --> 44:58.120
four important words and concepts. I'll define one in this section, two in the next, and

44:58.120 --> 45:04.400
the concept of reduction after that. We can then discuss the epistemology level algorithm

45:04.400 --> 45:11.040
for understanding itself. If you are already familiar with these concepts, just check the

45:11.040 --> 45:17.120
headings and definitions that follow in order to ensure we are using these words roughly

45:17.120 --> 45:24.400
the way you use them. You may have noticed I write certain, sometimes common words,

45:24.400 --> 45:31.600
such as model, with an uppercase first letter. This means I am using the word in a technical,

45:31.600 --> 45:38.920
well-defined, unchanging sense. I will define all such technical terms over time and I will

45:38.920 --> 45:46.040
try not to use these terms until I have defined them. We define 11 such terms in the first

45:46.040 --> 45:54.560
chapter, starting with understanding and reasoning. A dictionary of defined terms is in the works.

45:54.560 --> 46:01.000
Models are simplifications of reality in epistemology and science. Models are simplifications

46:01.000 --> 46:08.640
of reality. A rich mundane reality is too complex to land itself directly to computation.

46:08.640 --> 46:15.920
In OTB science fiction shows, we would sometimes hear. And then we fed all the information

46:15.920 --> 46:23.400
into the computer and this is what came out. Well, not anymore. Audiences now know that's

46:23.400 --> 46:32.200
not how regular computers work. Consider an automobile. It consists of thousands of parts,

46:32.200 --> 46:39.800
each with properties like materials, size, color, function, and sometimes complex interactions

46:39.800 --> 46:47.360
with other parts. What's all the information here? We can just feed all of those properties

46:47.360 --> 46:54.240
and measurements and facts into a computer and expect to get an answer. We need to ask

46:54.240 --> 47:01.200
a question and we also need to simplify the problem so that we can feed in just the facts

47:01.200 --> 47:07.720
or numbers that matter so that our question can be answered with minimum effort. How do

47:07.720 --> 47:15.500
we do that? We must identify or create, first in our minds, a very simple model of some

47:15.500 --> 47:22.240
sort of a generic automobile, and use that model for our computation. After we get the

47:22.240 --> 47:29.160
answer for the pure and simple model case, we apply the answer, with some care, back

47:29.160 --> 47:36.400
to our complex reality where the real automobile and the problem exists. What kind of model

47:36.400 --> 47:43.520
we choose depends on our goals. As an example of a model, Newton's second law states that

47:43.520 --> 47:51.560
force equals mass times acceleration, f equals ma. This equation is a classical scientific

47:51.560 --> 47:58.400
model. If we measure mass and acceleration of a car, then we can estimate how many horsepower

47:58.400 --> 48:06.360
the engine has. To use this equation, we engineers would model, in our minds, the car as a single

48:06.360 --> 48:13.200
small point mass with all the mass of the car in that point. Because if we don't, then

48:13.200 --> 48:19.440
we'd have to worry about the car rotating and other problems. This is how model-based

48:19.440 --> 48:27.080
science works. One or more scientists somehow derive a model for some phenomenon. The model

48:27.080 --> 48:34.600
is published as an equation, a formula, or a computer program. Scientists and engineers

48:34.600 --> 48:40.600
anywhere can now use this equation program model, treating it as a quick shortcut that

48:40.600 --> 48:46.720
works every time, as long as they have correct input data and are confidently applying the

48:46.720 --> 48:54.280
formula to a suitable problem in their reality. Our greatest invention, model-based problem

48:54.280 --> 49:01.200
solving, aka reductionism, is the greatest invention our species has ever made. The

49:01.200 --> 49:07.080
general strategy of simplifying problems before solving them must be tens of thousands of

49:07.080 --> 49:14.280
years old. In some sense, it is a prerequisite for all other inventions, including the use

49:14.280 --> 49:21.520
of fire. If you see a forest fire then you need to first imagine the utility of fire.

49:21.520 --> 49:27.320
As a model, before you can figure out that it might be useful to carry home a burning

49:27.320 --> 49:33.840
branch, we don't think of this problem solving strategy as an invention because it is already

49:33.840 --> 49:40.080
ubiquitous in our lives. We are all taught how to use model-based problem solving in

49:40.080 --> 49:46.320
school when we start solving story problems in math class, but most people never learn

49:46.320 --> 49:52.640
the names of these strategies and are missing the big epistemology level picture. This rarely

49:52.640 --> 49:59.440
matters until you start working with AI, where lack of an epistemological drowning may lead

49:59.440 --> 50:06.360
you astray into failing strategies. These little pills are an attempt to remedy that.

50:06.360 --> 50:13.360
Model-based methods were examined and refined into scientific methods over the past 450

50:13.360 --> 50:19.920
years. Science is now a collection of thousands of models that taken together allow science

50:19.920 --> 50:25.840
competent people to solve problems quickly and efficiently without having to redo all

50:25.840 --> 50:31.840
the work that scientists, like Newton, put into creating these models in the first place.

50:31.840 --> 50:38.640
And the sum total of those models covers many problems we want to solve scientifically,

50:38.640 --> 50:44.840
such as how to build a bridge or travel to the moon. This reuse is what makes science

50:44.840 --> 50:52.040
so effective, but not all sciences can benefit equally from this model-making. It works well

50:52.040 --> 51:00.440
for physics, chemistry, and most of biochemistry. As I'm fond of saying, physics is for simple

51:00.440 --> 51:06.960
problems, but as you get to more and more complex sciences, as you get further away

51:06.960 --> 51:14.320
from physics and closer to life, it gets harder to make decent models. The models used by

51:14.320 --> 51:21.560
for instance psychology, ecology, physiology, and medicine are generally more complex but

51:21.560 --> 51:28.960
also less powerful than models in physics. Given some solid data, a physicist can compute

51:28.960 --> 51:34.920
the mass of the proton to six decimal places, but we would have a harder time predicting

51:34.920 --> 51:40.480
the number of muskrats in New England next summer because that outcome depends on millions

51:40.480 --> 51:47.400
of parameters. The life sciences base many of their models on statistics. Statistical

51:47.400 --> 51:53.680
models are among the weakest models used in science. These statistical models when more

51:53.680 --> 52:00.880
powerful models with better predictive capabilities cannot be used for complexity reasons. Models

52:00.880 --> 52:11.960
are apothesis, unverified models, scientific theories, models verified by peer review,

52:11.960 --> 52:24.600
equations, formulas, complex scientific models, simulations of climate, weather, etc. Naive

52:24.600 --> 52:33.400
models that we create to simplify our own lives. Computer programs, and what is mathematics?

52:33.400 --> 52:40.720
It is a system that allows us to manipulate our models to cover more cases. Mathematics

52:40.720 --> 52:48.720
is the purest, most context free of all scientific disciplines. As such, its greatest value to

52:48.720 --> 52:55.080
humanity is in its role as a help discipline to all other disciplines. Einstein's famous

52:55.080 --> 53:01.440
equals MC squared model was derived using mathematical manipulation of other models

53:01.440 --> 53:08.240
known to Einstein at the time. But perhaps mathematics isn't as much a scientific discipline

53:08.240 --> 53:16.240
as an epistemological one. I may explore this aside later. Model use requires understanding.

53:16.240 --> 53:24.000
A good model is context free, since it maximizes the number of contexts it can be applied in.

53:24.000 --> 53:32.240
Newton's second law, F equals MA, works pretty much everywhere. We have forces, masses, and

53:32.240 --> 53:39.200
accelerations. The trade-off for this flexibility is that we ourselves need to understand the

53:39.200 --> 53:47.840
problem domain. In rocket science, when maneuvering in space, F equals MA will often work perfectly,

53:47.840 --> 53:53.680
but when you are applying it to the acceleration of your car, you need to account for lots of

53:53.680 --> 54:00.320
effects like friction between the road and the wheels, wind resistance, and the like.

54:00.320 --> 54:08.080
So, F equals MA, applied naively would give you the wrong answer if friction is involved.

54:08.080 --> 54:14.960
This demonstrates the main disadvantage with models. They require that both the model maker,

54:14.960 --> 54:21.680
scientists like Newton and the model users, STEM competent people everywhere, understand

54:21.680 --> 54:28.720
enough about the problem domain to know whether the model is applicable or not, and how to use it.

54:28.720 --> 54:34.560
This understanding is the expensive part of science, since using science requires first

54:34.560 --> 54:39.760
getting a solid science education in order to avoid mistakes when using models.

54:40.400 --> 54:45.920
And since models require understanding, they cannot be used to create understanding.

54:46.640 --> 54:52.080
This is a major problem for AI implementers. Chapter 3

54:52.080 --> 54:57.120
2 Dirty Words Reductionism is the use of models.

54:57.840 --> 55:04.240
Holism is the avoidance of models. Matters are scientific models, theories,

55:04.240 --> 55:11.760
hypotheses, formulas, equations, superstitions, and most computer programs.

55:13.040 --> 55:20.000
Reductionism and Holism. After having sorted out what models are, we can now discuss two

55:20.000 --> 55:27.280
complementary problem-solving strategies, or perhaps meta-strategies. There are in many ways

55:27.280 --> 55:33.360
each other's opposites, but the classification can become an argument about novel levels and

55:33.360 --> 55:40.560
definitions. I will initially pretend the division is clear and obvious, and will elaborate later.

55:41.200 --> 55:48.320
Reductionism is the use of models. In this series we will use exactly the above definition of the

55:48.320 --> 55:55.600
word, reductionism. If you look up the definition elsewhere you may find that some sources divide

55:55.600 --> 56:02.640
the strategy into sub-strategies. They also seem to miss the most important sub-strategy,

56:02.640 --> 56:08.720
which we'll discuss later. But what all these sub-strategies have in common is that they all

56:08.720 --> 56:15.440
provide ways to simplify observations of fragments of our rich mundane reality into much simpler

56:15.440 --> 56:23.840
models, which we use for reasoning, computation, and sharing. Reductionism is so central to how

56:23.840 --> 56:31.440
we do science, the heavy reliance on models, such as theories, equations and formulas,

56:31.440 --> 56:41.040
and physics, chemistry, etc. That we can speak of model-based sciences or reductionist sciences

56:41.040 --> 56:48.320
where such model-making is easy and effective, and this classification excludes those sciences,

56:48.320 --> 56:54.480
like psychology, where such model-making is difficult and less often rewarded with reliable

56:54.480 --> 57:01.760
results. After considering all the advantages of models we might wonder why we even bother

57:01.760 --> 57:09.360
discussing it. Too many people, especially those with a solid stem, science, technology,

57:09.360 --> 57:15.360
engineering, and mathematics education, it may well look like the only choice,

57:16.080 --> 57:23.040
but there's also the other strategy. Homism is the avoidance of models. This is where the

57:23.040 --> 57:30.000
questions start. This is where the paradox is surface. This is where your worldview may get

57:30.000 --> 57:38.000
shaken up. Seriously, especially if you are a scientist or engineer with a solid stem education

57:38.000 --> 57:46.240
and decades of professional success using science and models. In some sense, the goal of this entire

57:46.240 --> 57:52.080
series is to demonstrate that we need to use both problem-solving strategies when creating

57:52.080 --> 57:58.640
our artificial intelligences, because that is what it is going to take. We need holistic

57:58.640 --> 58:05.760
understanding. We established that in the first chapter, as a sample of the new ideas that we

58:05.760 --> 58:14.480
will have to deal with I will just mention, reasoning is reductionist. Understanding is holistic.

58:15.760 --> 58:23.120
Newer networks are holistic. Holistic systems can jump to conclusions on scant evidence.

58:24.400 --> 58:28.960
Holistic systems can themselves know what is important and what isn't.

58:28.960 --> 58:36.720
Holistic systems can solve problems we ourselves cannot or don't care to understand.

58:36.720 --> 58:45.360
Holistic systems are model-free. We do not use any a priori models of any problem domain.

58:46.560 --> 58:51.200
Reasoning systems inherit all problems and benefits of reductionism.

58:52.480 --> 58:57.120
Understanding systems inherit all problems and benefits of holism.

58:57.120 --> 59:04.000
Humans are born holistic. Humans each solve thousands of little

59:04.000 --> 59:11.840
problems every day, and we are solving almost all these problems holistically, using understanding,

59:11.840 --> 59:16.960
and without a need to reason at all. This includes fluent language use.

59:18.160 --> 59:24.000
A stem education instills a strict reductionist discipline in order to mitigate problems

59:24.000 --> 59:30.000
with fallibility of holistic human minds. Our intelligences are fallible.

59:30.800 --> 59:36.880
These claims all deserve individual treatments, and we'll get to all of them in later sections.

59:37.600 --> 59:43.360
But the major theme is clear. Humans are mainly holistic problem solvers.

59:44.000 --> 59:51.840
This must be true for our artificial intelligences. We had several reasons for focusing on reductionist

59:51.840 --> 59:59.280
methods, models, and reasoning during the first 60 years of AI. Our computers were too small to

59:59.280 --> 01:00:07.200
make neural networks work at all. But there were also ideological reasons. AI was born out of the

01:00:07.200 --> 01:00:13.520
math and computer science departments of our universities, and therefore most of the people

01:00:13.520 --> 01:00:19.440
working on AI were solidly oriented towards the goal of creating a logic-based reductionist

01:00:19.440 --> 01:00:26.640
infallible artificial mind. To build early AIs, like expert systems, we entered rules

01:00:26.640 --> 01:00:33.520
or programmed in lots of facts to reason about. But this was budding reductionist castles in the

01:00:33.520 --> 01:00:40.720
air, comprised of unanchored facts that didn't tie to any understanding whatsoever. The troubles

01:00:40.720 --> 01:00:47.280
with classical AI, such as bitterness, the tendency to make spectacular and expensive

01:00:47.280 --> 01:00:53.520
mistakes at the edges of their competence, can be directly traced to the lack of foundational

01:00:53.520 --> 01:00:59.360
understanding to support these attempts at reasoning. Understanding machines will not

01:00:59.360 --> 01:01:05.120
suffer from this brittleness, but will fail gracefully at the edges of their competence,

01:01:05.120 --> 01:01:12.240
much like humans. Most of the time they will know the answer beyond that they will guess,

01:01:12.240 --> 01:01:18.560
and the guesses they make are based on a lifetime of experience, gained through learning from a

01:01:18.560 --> 01:01:25.280
large corpus and so they have a good chance of being at least a workable choice, if not perfect.

01:01:25.920 --> 01:01:32.640
How can anyone solve problems without using models? A lot of people coming from a STEM

01:01:32.640 --> 01:01:38.960
background cannot even imagine how to solve problems without using models. But it's not

01:01:38.960 --> 01:01:45.760
hard, once you understand the difference, mostly it's a matter of doing what worked last time.

01:01:46.480 --> 01:01:52.400
The problem is now figuring out whether we are in a situation that's similar enough that it will

01:01:52.400 --> 01:02:00.400
work again. This is mostly a pattern matching problem. More later, what's the result? The

01:02:00.400 --> 01:02:07.120
holistic answer is a quick guess at the best action, based on experience with similar situations.

01:02:07.120 --> 01:02:13.360
Most of the time it's correct, sometimes it's a little wrong, and every now and then,

01:02:13.360 --> 01:02:20.080
there's a noticeable mistake. And if we get things a little wrong, we may notice the outcome

01:02:20.080 --> 01:02:26.560
and correct the action. We learn from our mistakes. If we practice something a lot,

01:02:26.560 --> 01:02:33.520
we will start doing it effectively and perfectly every time. Do we learn faster if we make more

01:02:33.520 --> 01:02:41.920
mistakes? Should we make mistakes on purpose? More later, in situations where you cannot use

01:02:41.920 --> 01:02:48.800
models, which are more common than many realize, the holistic guess may also be your only option.

01:02:49.600 --> 01:02:57.840
Conversely, if you have an adequately well-working model-based solution, just use that. My video,

01:02:57.840 --> 01:03:03.920
Model-Free Methods Workshop demonstrates how the group solves four different problems

01:03:03.920 --> 01:03:11.200
at a high level, using both reductionist and holistic methods. Why are these dirty words?

01:03:11.920 --> 01:03:19.040
Well, they are not dirty to epistemologists. Reductionism has been the default problem-solving

01:03:19.040 --> 01:03:25.600
paradigm because it's the one that has to be taught. We are born with a holistic problem-solving

01:03:25.600 --> 01:03:32.480
apparatus. But reductionist science doesn't come naturally. Therefore, it has to be taught in

01:03:32.480 --> 01:03:39.760
schools, practiced, and carried out according to certain rules. Perhaps that's why the sciences

01:03:39.760 --> 01:03:46.400
are called disciplines, because following the ideal scientific method requires practice and

01:03:46.400 --> 01:03:55.760
constant vigilance. J. C. Smutsbrook, Holism and Evolution, 1926 established the terminology in the

01:03:55.760 --> 01:04:04.640
epistemological literature. And no inchrodinger wrote, what is life, 1944, questioning the power

01:04:04.640 --> 01:04:11.840
of physics to provide useful explanations to the life sciences. Percy Grote, zen and the art of

01:04:11.840 --> 01:04:19.840
motorcycle maintenance, 1974, had contrast something very holistic, zen Buddhism, with

01:04:19.840 --> 01:04:27.920
something very reductionist, motorcycle maintenance. So the chasm between the strategies was identified

01:04:27.920 --> 01:04:36.240
a long time ago. The strategies are each other's opposites. H-O-L-E-L-I-S-M-based strategies for

01:04:36.240 --> 01:04:42.640
understanding can handle many important kinds of complexity and can quickly provide a guest

01:04:42.640 --> 01:04:49.840
answer. But these guesses are fallible, and often more expensive to compute. Reductionist

01:04:49.840 --> 01:04:55.840
education and strategies brought benefits of cheap model reuse and formal rigor to improve

01:04:55.840 --> 01:05:01.440
correctness, but cannot handle complexity and is therefore dependent on an external

01:05:01.440 --> 01:05:06.720
understander to determine applicability in real-world complexity rich situations.

01:05:07.440 --> 01:05:14.800
And as part of that education, we are told that holistic methods, such as jumping to conclusions

01:05:14.800 --> 01:05:21.600
unscanned evidence, are bad, in spite of the fact that our brains use holistic methods thousands

01:05:21.600 --> 01:05:28.240
of times each day to successfully understand the environment we live in. We can all use

01:05:28.240 --> 01:05:35.040
either strategy as appropriate. If we don't have a STEM education, we will still sometimes make

01:05:35.040 --> 01:05:41.520
naive models. But sometimes there is a choice and different people may prefer one or the other.

01:05:42.160 --> 01:05:48.640
When playing pool, some people estimate and compute bouncing angles and some people shoot

01:05:48.640 --> 01:05:55.920
by feel. But we have our preferences, and it may be tempting to label a person with an overly

01:05:55.920 --> 01:06:04.160
strong preference as a holistic or a reductionist. This is sometimes received badly, if perceived

01:06:04.160 --> 01:06:12.880
as a limitation. Some dictionaries even flag reductionist as derogatory. And yet, some people

01:06:12.880 --> 01:06:20.640
use it as a self-assigned label. I try to use these terms only as shorthand for a person with a

01:06:20.640 --> 01:06:28.240
stated strong preference for holistic or reductionist methods. The two terms were very useful in

01:06:28.240 --> 01:06:36.400
epistemology. But then someone invented the concept of holistic medicine. Instead of just shooting

01:06:36.400 --> 01:06:44.400
a single medical problem, you analyze the patient's entire situation, attempting to account for diet,

01:06:44.400 --> 01:06:53.680
exercise, sleep, work, habits, stress levels, allergies, family, friends, and environmental

01:06:53.680 --> 01:07:01.920
poisons. A good idea, in general. But the wide scope was unmanageable by the, traditionally

01:07:01.920 --> 01:07:09.760
reductionist medical establishment and the idea faded away. Instead, the whole idea of holism

01:07:09.760 --> 01:07:17.040
became tainted as woo-woo in the term, holistic medicine, became associated with woo-woo merchants

01:07:17.040 --> 01:07:24.560
selling crystals and aromatherapy. As explained above, holism is the avoidance of models,

01:07:24.560 --> 01:07:31.360
or better phrased, holism is the metastrategy of avoiding a priori models of the problem domain.

01:07:32.000 --> 01:07:38.720
That extra precision rarely matters. There's nothing woo-woo about it. It does say,

01:07:38.720 --> 01:07:45.360
science not required, but, you can make breakfast without reasoning. It is important to note that

01:07:45.360 --> 01:07:51.680
holistic methods are based on a lifetime of experience, in humans and a training corpus

01:07:51.680 --> 01:07:58.560
worth of experience, in neural networks. When you're making breakfast, you are relying on this

01:07:58.560 --> 01:08:05.520
experience, mostly repeating whatever worked yesterday. Some people claim they use reasoning

01:08:05.520 --> 01:08:11.360
while making breakfast, but they can make their breakfast while speaking to someone else on the

01:08:11.360 --> 01:08:17.440
phone. And as they hang up, they find themselves suddenly sitting at the breakfast table with

01:08:17.440 --> 01:08:24.800
their coffee and hot oatmeal. Same thing when driving to work. You may get lost in thought,

01:08:24.800 --> 01:08:31.520
and then you find yourself parked at work. You didn't need to reason, since all sub-problems

01:08:31.520 --> 01:08:36.480
that occur in driving had been solved multiple times, during years of driving.

01:08:37.120 --> 01:08:43.280
Sub-conscious understanding is used for simple things like sequencing our leg muscles as we

01:08:43.280 --> 01:08:52.000
walk. You have no idea how you are doing that, it just works. Same thing with vision. You understand

01:08:52.000 --> 01:08:58.640
that you are looking at a chair, but you do not have conscious access to the 15th rod cone pixel

01:08:58.640 --> 01:09:04.320
to the left of your center of vision, and have no idea how this understanding works.

01:09:05.040 --> 01:09:11.200
Same thing with understanding and generating language. You do not have any explanation for

01:09:11.200 --> 01:09:17.360
how you are able to understand the meaning of this sentence. Understanding is sub-conscious

01:09:17.360 --> 01:09:24.240
and holistic. So for the majority of things we do every day, we do not need reasoning or

01:09:24.240 --> 01:09:31.360
reductionist methods. Some people would like to think they are, logical thinkers, immune to

01:09:31.360 --> 01:09:38.560
most cognitive fallacies, but whether they are or not, at the lower levels, everyone is solving

01:09:38.560 --> 01:09:45.600
most of their problems holistically. I claim that reductionist reasoning requires holistic

01:09:45.600 --> 01:09:53.120
understanding. In other words, I need to understand the problem domain at hand before I can create

01:09:53.120 --> 01:10:00.160
and reuse models to enable me to reason about the domain. So holistic understanding is much

01:10:00.160 --> 01:10:06.560
more important than reductionist reasoning because it is the most used strategy, by far,

01:10:06.560 --> 01:10:13.600
and the former is also a prerequisite for the latter. But the fallibility of holistic understanding

01:10:13.600 --> 01:10:20.800
forced us to create reductionist science and to teach it in STEM education. It is as if the purpose

01:10:20.800 --> 01:10:27.920
of science is to keep holistic guessing in check, but this aversion to fallibility has a cost,

01:10:27.920 --> 01:10:35.440
because it means complexity bound and irreducible problems cannot be solved. Like language

01:10:35.440 --> 01:10:43.440
understanding, global resource allocation, and social interactions, reductionism and model-based

01:10:43.440 --> 01:10:51.760
science appeared around 1650 after a century of gestation. Excluding minor romantic interludes,

01:10:51.760 --> 01:10:58.800
it has held its position as the dominant paradigm for about 400 years. This is changing.

01:10:59.600 --> 01:11:06.000
The reductionist train is running out of track. The remaining hard problems facing humanity

01:11:06.000 --> 01:11:11.920
are problems of irreducible complexity in domains where reductionist model-based methods

01:11:11.920 --> 01:11:19.360
simply cannot work. Whether we like the idea or not, we need to accept these holistic methods

01:11:19.360 --> 01:11:26.480
into our AI toolkits. Starting now, we will use these methods either in their raw form,

01:11:26.480 --> 01:11:32.800
as model-free methods, or as understanding machines at any level from component to robot

01:11:32.800 --> 01:11:42.560
co-worker. Chapter 4. Reduction. Epistemic reduction is a process that discovers higher-level

01:11:42.560 --> 01:11:48.800
abstractions and lower-level data by discarding everything at the lower layer that it recognizes

01:11:48.800 --> 01:11:56.560
as irrelevant. We have seen the power of models. We have introduced the two problem-solving

01:11:56.560 --> 01:12:03.440
meta-strategies of reductionism and holism. We also noted that the creation and use of models

01:12:03.440 --> 01:12:10.320
requires an intelligent agent that understands the problem domain. Someone or something has to

01:12:10.320 --> 01:12:19.760
perform the reduction. I will now discuss reduction in some detail. Until 2012, only humans and other

01:12:19.760 --> 01:12:26.880
animals with brains could perform reduction. Now our deep neural networks, DNN, can perform

01:12:26.880 --> 01:12:34.640
limited reduction. How do brains and DNNs accomplish this? And how can we improve these algorithms?

01:12:35.360 --> 01:12:42.160
This may be, to some readers, the most rewarding part of this series, because it provides you

01:12:42.160 --> 01:12:48.960
the opportunity to learn a new and useful skill. Most people never think about the world at this

01:12:48.960 --> 01:12:55.760
level. Knowledge of reduction provides a new point of view that you can use to better understand

01:12:55.760 --> 01:13:01.440
your environment, other intelligent agents around you, and modern AI systems.

01:13:02.240 --> 01:13:08.880
Definition of reduction. Reduction is a process that discovers higher-level abstractions and

01:13:08.880 --> 01:13:14.800
lower-level data. We will initially note that reduction is exactly the same as abstraction.

01:13:14.800 --> 01:13:21.680
Why do we need a new word? Because the term abstraction is mostly used

01:13:21.680 --> 01:13:28.080
by scientists already operating in a pure model space, seeking a higher level of abstraction

01:13:28.080 --> 01:13:34.880
in that space. But to them, abstraction is something that just magically happens in their

01:13:34.880 --> 01:13:41.760
heads, since there are no scientific theories for how abstraction works. There cannot be,

01:13:41.760 --> 01:13:49.280
since abstraction is a concept in epistemology, not science. AI researchers are starting from

01:13:49.280 --> 01:13:55.600
something much closer to a rich mundane reality, where there is a lot of confounding context.

01:13:56.160 --> 01:14:01.920
We are solving the metal problem of how to move from there into a space that is sufficiently

01:14:01.920 --> 01:14:08.720
abstract to solve the problem at hand. Here, reduction is a much more appropriate term.

01:14:08.720 --> 01:14:15.600
We can abstract the red pixel or the letter B, but we can reduce a rich context containing

01:14:15.600 --> 01:14:21.600
that pixel or letter into a higher-level concept. We are swimming in reduction.

01:14:21.600 --> 01:14:27.600
Paradoxically, one of the hardest things about teaching reduction is that we don't see the

01:14:27.600 --> 01:14:33.920
need to learn about it because we all do it all the time, every millisecond, and the resulting

01:14:33.920 --> 01:14:42.240
reductions, models, become available to our conscious minds as if, by magic, brains reduce

01:14:42.240 --> 01:14:51.200
away 99.999% of their sensory input, but this process is subconscious and hence invisible to us.

01:14:51.920 --> 01:15:00.240
The situation is much like, supposedly, a fish swimming in water. We are all masters of reduction,

01:15:00.240 --> 01:15:06.160
but we don't know how we do it or that we even do it. We didn't know this would ever matter.

01:15:06.800 --> 01:15:14.560
And generally, it doesn't. Well, it matters in epistemology, and it matters in AI,

01:15:14.560 --> 01:15:21.840
since we need to actually implement that magic. We as epistemologists must know how abstraction

01:15:21.840 --> 01:15:28.560
is actually performed, and we give the epistemology-level equivalent of abstraction the name

01:15:28.560 --> 01:15:34.880
reduction, because that's the recipe for how to accomplish it. We reduce our rich mundane

01:15:34.880 --> 01:15:42.400
reality by discarding, reducing away, what's irrelevant. And by using the name reduction,

01:15:42.400 --> 01:15:48.480
we, as AI epistemologists, keep reminding ourselves how it is properly done.

01:15:49.120 --> 01:15:56.160
Consider the following descriptions of a car. The slide is meant to be read from the bottom up,

01:15:56.160 --> 01:16:05.360
to match abstraction levels from low to high. If I'm driving to work, I better be driving my car.

01:16:06.000 --> 01:16:12.640
If the police are looking for a stolen car, they would be looking for red 2010 Toyota Celica.

01:16:13.280 --> 01:16:18.640
If I'm buying a new car, then I might be looking for just a new Toyota Celica.

01:16:19.200 --> 01:16:25.600
And a self-driving car would likely only need to understand whether an obstacle is a vehicle or

01:16:25.600 --> 01:16:33.120
not, in order to model maximum speed for future movement. We see that we want to pick the appropriate

01:16:33.120 --> 01:16:39.120
level of abstraction to deal with the same object, or topic, in different situations.

01:16:39.840 --> 01:16:45.280
But more importantly, we see that we can get from a more detailed description,

01:16:45.280 --> 01:16:51.200
at the bottom, to a more generic one, higher up, by simply discarding some detail.

01:16:51.200 --> 01:16:57.520
I hasten to point out that reduction is more complicated than this simple example of decreasing

01:16:57.520 --> 01:17:04.480
specificity shows. What we need to start somewhere in this image allows us to form intuitions that

01:17:04.480 --> 01:17:11.680
will serve for a while. True reduction involves operations like shifting from syntax to semantics

01:17:11.680 --> 01:17:19.680
or from instance to type. The appearance of car as an abstraction of Toyota, and the step from

01:17:19.680 --> 01:17:27.840
my Toyota to a Toyota illustrates these steps. Algorithms for these things are known.

01:17:28.640 --> 01:17:36.000
Salience, part of the trick is to know what to discard. At each level of abstraction,

01:17:36.000 --> 01:17:43.760
something can typically be identified as the least important property. Red and Celica are more

01:17:43.760 --> 01:17:53.280
significant than 2010 for anyone looking for a car. If we had started from my red 2010 Toyota

01:17:53.280 --> 01:18:01.360
truck, then the word truck would not be discarded until the top level. Reduction requires understanding

01:18:01.360 --> 01:18:09.440
what's relevant. In reduction we keep that which is salient. More later, partial reductions.

01:18:09.440 --> 01:18:17.360
Most of the time we do not perform reduction all the way to models. I cannot stress this enough.

01:18:18.000 --> 01:18:25.120
We discuss reduction to models for pedagogical reasons. It is easy to initially see the context

01:18:25.120 --> 01:18:32.960
free model as the goal of reduction. In reality, in brains, we can stop reducing the moment we

01:18:32.960 --> 01:18:39.680
recognize that we have a working answer or response, such as a command to contract some muscle or

01:18:39.680 --> 01:18:46.000
having understood the meaning of a sentence subconsciously. At this point, there is still

01:18:46.000 --> 01:18:52.160
some residual context but we use that context productively rather than discard it to move

01:18:52.160 --> 01:19:00.160
to higher levels. Some people claim we use models for all our thinking, but I'm using capital M

01:19:00.160 --> 01:19:08.160
model only to describe a completely context free abstraction. F equals M A is an example of that.

01:19:08.880 --> 01:19:15.920
There is no need to check whether a car is a red car or a Toyota. The equation works not only for

01:19:15.920 --> 01:19:23.680
all cars but for all forces, masses and accelerations. We might come up with a special equation for

01:19:23.680 --> 01:19:29.760
acceleration of Tesla cars which would require different inputs like battery charge level

01:19:29.760 --> 01:19:36.480
and software settings. That would not be a context free model since it would not work on a Toyota.

01:19:37.120 --> 01:19:45.280
For almost all tasks, basically, in everything except science and even there, only rarely,

01:19:45.280 --> 01:19:52.800
we only perform as much reduction as is necessary to get the job done. When learning to ski,

01:19:52.800 --> 01:19:58.560
you only figure out how you yourself need to perform given your body and equipment.

01:19:58.560 --> 01:20:04.880
We do not need to parameterize our skiing skills for someone with twice the body mass

01:20:04.880 --> 01:20:11.040
because that would be useless to us for the purpose of our own skiing. But a scientist would

01:20:11.040 --> 01:20:17.840
have to go that far in order to parameterize away one more piece of context from the model

01:20:17.840 --> 01:20:24.720
they are creating. For instance, when creating a skiing video game or designing a new ski,

01:20:24.720 --> 01:20:30.960
if we consider the enormous amount of subconscious activity that happens in the brain,

01:20:30.960 --> 01:20:37.520
we can safely say that partial reductions are the most common reductions. For instance,

01:20:37.520 --> 01:20:44.320
when we take a step forward, our subconscious has analyzed our posture and velocity by using

01:20:44.320 --> 01:20:50.000
reduction based on low level nerve signals and is commanding leg muscles to contract an

01:20:50.000 --> 01:20:57.040
up precisely timed sequence. This activity is something we are unaware of. Most of us don't

01:20:57.040 --> 01:21:04.000
even know what leg muscles we have. And there would be no time to perform reduction all the way to

01:21:04.000 --> 01:21:10.560
models. That process takes a minimum of a half second and you don't have that kind of time

01:21:10.560 --> 01:21:17.040
available to respond to an imbalance when walking or skiing. Reduction in society.

01:21:17.040 --> 01:21:24.240
Most of us get paid to understand whatever we need to understand in order to perform our jobs.

01:21:24.880 --> 01:21:31.760
In other words, most of us get paid to do reduction. If you are approving building permits,

01:21:31.760 --> 01:21:40.160
you reduce a stack of forms to a one bit verdict of approved or rejected. We accelerate reduction,

01:21:40.160 --> 01:21:44.720
and this is the main reason most of us haven't been replaced by robots.

01:21:44.720 --> 01:21:51.440
But we see that when future understanding machines can perform reduction by themselves,

01:21:51.440 --> 01:21:56.160
then we are unlikely to get paid for it. Levels of reduction.

01:21:56.960 --> 01:22:03.040
Suppose a young man and a young woman fall in love, something happens to mess it all up,

01:22:03.040 --> 01:22:08.400
and then they sort this out and reunite. This is what happened in the man's,

01:22:08.400 --> 01:22:15.440
which mundane reality. Suppose the man wants to share this experience, because there was some

01:22:15.440 --> 01:22:20.800
moral to the story that he thinks would be interesting to others and possibly important.

01:22:21.440 --> 01:22:27.120
He could analyze what happened and figure out which were the key events in the saga and then

01:22:27.120 --> 01:22:34.160
have actors on a stage re-enact the story as a play. This is a reduction because the boring parts

01:22:34.160 --> 01:22:41.040
of the story would not be part of the play. They are discarded as irrelevant, but the story would

01:22:41.040 --> 01:22:48.080
be acted out by real people in front of a live audience. If you are in the audience, you can move

01:22:48.080 --> 01:22:54.480
your head to see behind any actor on the stage and you can clearly see everything on the stage,

01:22:54.480 --> 01:23:02.080
not just one actor speaking at a time. He can make a movie about it. Now your point of view

01:23:02.080 --> 01:23:09.520
is pre-defined by the camera angle and cropping. You can no longer see behind an actor, and you

01:23:09.520 --> 01:23:16.320
can often only see those actors that are involved in the main action. He could write a book about it.

01:23:16.960 --> 01:23:23.040
We no longer can see even the people described in the book, except in our imagination.

01:23:23.760 --> 01:23:31.280
A critic review in the theater play may reduce it to, Boy meets girl, Boy loses girl, Boy gets

01:23:31.280 --> 01:23:38.640
girl. A drama school graduate may summarize it as a double reversal plot. This is a description

01:23:38.640 --> 01:23:46.160
that is so free from context, doesn't even specify boys or girls that it could be argued it qualifies

01:23:46.160 --> 01:23:55.360
to be called a model. Plays, movies, books, stories, tropes, etc. are all partial reductions of

01:23:55.360 --> 01:24:03.920
reality, and some are more reduced than others. Just like in the red Toyota case, we need to find

01:24:03.920 --> 01:24:10.960
the appropriate level of abstraction to work with. The young man in the example, when writing a

01:24:10.960 --> 01:24:17.600
book or a screenplay, has much in common with a scientist trying to describe something in nature

01:24:17.600 --> 01:24:24.560
in a reusable context free manner by reducing it to a model. They are model makers, or are at

01:24:24.560 --> 01:24:31.360
least performing partial reduction. They are discarding the irrelevant bits. The opposite of

01:24:31.360 --> 01:24:38.640
reduction. We also need to be able to move in the opposite direction, from models to reality,

01:24:39.280 --> 01:24:46.240
or at least from more abstract partial models to partial models closer to reality. When an actor

01:24:46.240 --> 01:24:52.560
is given a screenplay, they know it only contains rough directions for what to do and what lines

01:24:52.560 --> 01:25:00.160
to say. The actor's job is to give a little of themselves to flesh out the screenplay to actual

01:25:00.160 --> 01:25:08.720
actions, including creating, synthesizing, the appropriate display of emotions, tone of voice,

01:25:08.720 --> 01:25:16.160
and body language. They use their experience as people and as actors. They use elements of their

01:25:16.160 --> 01:25:22.320
past lives and skills they have acquired by training to create something people in the audience

01:25:22.320 --> 01:25:30.080
might relate to. For example, they may repurpose a personal experience. He is sad as when my

01:25:30.080 --> 01:25:38.880
hamster died. Things they learned in drama school, such as speaking, singing, dancing, and swordplay,

01:25:38.880 --> 01:25:46.720
from other actors, what would bogart do, from fiction, from other movies and plays, etc.

01:25:46.720 --> 01:25:54.880
The actor's artist who convey whatever the script intends to convey, emotions, a morality cookie,

01:25:54.880 --> 01:26:02.800
a political position, titillation, surprise, and so on. Starting from the simple model,

01:26:02.800 --> 01:26:09.360
the screenplay, their job is similar to an engineer's when they are faced with a problem

01:26:09.360 --> 01:26:15.440
and use a model to solve it. The engineer would use their experience to decide that

01:26:15.440 --> 01:26:22.560
M is the mass of the car and not the tire pressure. The actor decides that sadness

01:26:22.560 --> 01:26:31.280
is more appropriate than grief for a certain scene, etc. I call this process, which is the

01:26:31.280 --> 01:26:39.280
opposite of reduction by the name it is used in problem solving application. We use a model to

01:26:39.280 --> 01:26:47.360
simplify a problem situation, moving it into an abstract and pure model space. We solve the

01:26:47.360 --> 01:26:55.040
problem there by performing math, perhaps, and then apply the answer to our rich reality

01:26:55.040 --> 01:27:02.080
to the problem we are trying to solve. Many of you may recognize the word application or

01:27:02.080 --> 01:27:09.840
its abbreviation, app. That's not as far-fetched as it might seem. Apps are software-based models.

01:27:10.560 --> 01:27:16.240
Reduction in application and brains. Back to the issue of partial reductions.

01:27:16.960 --> 01:27:24.400
Consider the actor reading a screenplay. They are using their eyes to gather pixels of color

01:27:24.400 --> 01:27:32.320
and orientation. The brain then performs pattern matching, reduction, from these low-level signals

01:27:32.320 --> 01:27:40.160
to letters, words, to language, to high-level concepts like love and separation, and eventually

01:27:40.160 --> 01:27:46.560
to a high-level understanding of the playwright's intents. The actor then takes this high-level

01:27:46.560 --> 01:27:52.640
understanding and by performing application, they add their own experience to the script

01:27:52.640 --> 01:27:59.360
to get closer to reality and their performance. Our brains are capable of moving up and down

01:27:59.360 --> 01:28:06.080
many levels of abstraction at once. Perhaps it tracks all of them simultaneously,

01:28:06.080 --> 01:28:12.080
keeping layers of abstraction separate. This is a clue for why deep neural networks

01:28:12.080 --> 01:28:16.960
perform better than shallow ones. Which is what we'll discuss next.

01:28:16.960 --> 01:28:25.120
Chapter 5. Why Deep Learning Works. Deep learning performs epistemic reduction.

01:28:26.320 --> 01:28:32.720
A math-free computer science-free description of why deep learning works. We have now built

01:28:32.720 --> 01:28:39.440
a base of theory for why AI works, what models are, and how to create them, what reductionism

01:28:39.440 --> 01:28:47.040
and holism are, and what the process of reduction is. These are the fundamentals of AI epistemology.

01:28:47.760 --> 01:28:53.760
This base allows us to discuss various strategies to move towards understanding machines in a

01:28:53.760 --> 01:29:00.240
well-understood and controlled manner. We are now ready to discuss why deep learning,

01:29:00.240 --> 01:29:07.920
DL, works. This is the fifth and last entry in the AI epistemology primer. Deep learning

01:29:07.920 --> 01:29:14.080
performs reduction. This is an unsurprising claim, considering the preceding chapters.

01:29:14.800 --> 01:29:21.680
There are several mutually compatible theories for how deep learning works. But just as in

01:29:21.680 --> 01:29:28.640
the first chapter, we will now discuss the epistemological aspects, why it works,

01:29:28.640 --> 01:29:35.520
from several viewpoints and levels, starting from the bottom. We would use examples from the

01:29:35.520 --> 01:29:42.720
TensorFlow system and API as a library, as a stand-in for all deep learning family algorithms

01:29:42.720 --> 01:29:49.680
and TF programs, because the available API functions heavily shape and constrain solutions

01:29:49.680 --> 01:29:55.040
that can be implemented in this space. And the generalization should be straightforward enough.

01:29:55.760 --> 01:30:01.280
Consider the following illustration of image understanding using Keras, an excellent

01:30:01.280 --> 01:30:08.400
abstraction layer on top of TensorFlow. I like to refer to the input layer as being

01:30:08.400 --> 01:30:14.880
on the bottom rather than at the far left as in this image. When viewing it my way,

01:30:14.880 --> 01:30:20.560
the low to high dimension we use in my rotated version of the image can be mentally mapped

01:30:20.560 --> 01:30:27.360
to a low to high stack of abstraction levels. I'm not the only one using this dimension this way.

01:30:27.360 --> 01:30:33.920
I hope this rotation isn't too confusing. We can see that there is an obvious data reduction

01:30:33.920 --> 01:30:40.400
and an obvious complexity reduction. Can we determine whether the system is also performing

01:30:40.400 --> 01:30:47.360
what I'd like to call the epistemic reduction? Is it reducing a way that which is unimportant?

01:30:47.360 --> 01:30:53.840
And if so, how does it accomplish this? How does an operator in a deep learning stack

01:30:53.840 --> 01:31:01.120
know what makes something important? Salient, up your data, reduction of sorts could be

01:31:01.120 --> 01:31:09.200
accomplished by compression schemes or even random deletion. This is undesirable. We need to discard

01:31:09.200 --> 01:31:16.640
the non-salient parts so that in the end, we are left with what is salient. Some people have not

01:31:16.640 --> 01:31:22.400
understood the importance of salient's based reduction and useless compression power of

01:31:22.400 --> 01:31:28.560
reversible algorithms as a measurement of intelligence, which is no more useful than

01:31:28.560 --> 01:31:36.080
believing a simple video camera can understand what it sees. So let me conjure up a bit like in

01:31:36.080 --> 01:31:43.440
the movie, Inside Out, a fairy tale of what goes on in a deep learning network, except we'll do it,

01:31:43.440 --> 01:31:50.560
bottom up. Suppose we have built a system for finding faces in an image with the intent of

01:31:50.560 --> 01:31:56.800
incorporating that as a feature in a camera. Many cameras have this feature already,

01:31:56.800 --> 01:32:03.600
so this is not a far-fetched example. We implement an image understanding neural network,

01:32:03.600 --> 01:32:10.720
show the system many kinds of images for a few days, perhaps using so-called supervised learning

01:32:10.720 --> 01:32:17.200
in order to improve this story, and then we show it an image of a family having a picnic in a park

01:32:17.200 --> 01:32:23.520
and ask the system to outline where the faces are so that the camera can focus sharply on them.

01:32:24.240 --> 01:32:30.800
The input image is converted from RGB color values to an input array and the data in this array is

01:32:30.800 --> 01:32:37.280
then shuffled through many layers of operators. And for many of these layers, there are fewer

01:32:37.280 --> 01:32:44.320
outputs than there are inputs, as you can see above, which means some things have to be discarded

01:32:44.320 --> 01:32:52.320
by the processing. Each layer receives initially signals, from below, that is, from the input,

01:32:52.320 --> 01:32:58.960
or from lower levels of abstraction, and produces some reduced output to send to the next layer

01:32:58.960 --> 01:33:06.080
operator above. To continue detail, at some early level, some operator is given a few

01:33:06.080 --> 01:33:12.000
adjacent pixels and determines that there is a vertical, slightly curved line dividing the

01:33:12.000 --> 01:33:19.440
darker green area from the lighter green area. So it tells the operator above the simpler line

01:33:19.440 --> 01:33:26.080
or color-based description using some encoding we don't really care about. The operator at the

01:33:26.080 --> 01:33:32.000
level above might have gotten another matching curve and says, these match what I saw a lot of

01:33:32.000 --> 01:33:39.040
when the label blade of grass was given as a ground truth label during supervised learning.

01:33:39.040 --> 01:33:44.640
If no label is known, then we again assume some other uninteresting representation.

01:33:44.640 --> 01:33:51.680
It is okay to propagate results without human-labeled signals because whatever signaling scheme is

01:33:51.680 --> 01:33:58.240
used will be learned by the level above. The operator above that says, when I get lots of

01:33:58.240 --> 01:34:04.320
blades of grass signals, I reduce all of that to a long signal as I send it upward.

01:34:04.320 --> 01:34:10.720
And eventually we reach the higher operator layers and someone there says, we are a face-finder

01:34:10.720 --> 01:34:17.440
application. We are completely uninterested in lawns and discards the lawn as non-cellient.

01:34:17.440 --> 01:34:24.320
What remains after you discard all non-faces are the faces. You cannot discard anything

01:34:24.320 --> 01:34:31.120
until you know what it is, or can at least estimate whether it's worth learning. Specifically,

01:34:31.120 --> 01:34:39.040
until you understand it at the level of abstraction you are operating at. The low-level blade of

01:34:39.040 --> 01:34:44.800
grass recognizers could not discard the grass because they had no clue about the high-level

01:34:44.800 --> 01:34:51.680
saliencies of lawn or not in face or not that the higher layers specialize in. You can only tell

01:34:51.680 --> 01:34:58.000
what salient or not, important or not at the level of understanding and abstraction you are

01:34:58.000 --> 01:35:05.280
operating at. Each layer receives lower-level descriptions from below, discards what it

01:35:05.280 --> 01:35:11.840
recognizes as irrelevant, and sends its own version of higher-level descriptions upward

01:35:11.840 --> 01:35:17.760
until we reach someone who knows what we are really looking for. This is of course why deep

01:35:17.760 --> 01:35:26.640
learning is deep. This idea itself is not new. It was discussed by Oliver Selfridge in 1959.

01:35:26.640 --> 01:35:33.920
He described an idea called, Pandemonium, which was largely ignored by the AI community because of

01:35:33.920 --> 01:35:40.480
its radical departure from the logic-based AI promoted by people like John McCarthy and Marvin

01:35:40.480 --> 01:35:48.400
Minsky. But Pandemonium presaged, by almost 60 years, the layer-by-layer architecture with

01:35:48.400 --> 01:35:55.120
signals passing up and down that is used today in all deep neural networks. This is the reason my

01:35:55.120 --> 01:36:02.720
online handle is at Pandemonica. So do any TensorFlow operators support this reduction?

01:36:03.360 --> 01:36:11.040
Let's start by examining the pooling operators. There are a few in the diagram. They are conceptually

01:36:11.040 --> 01:36:18.000
simple. There are over 50 pooling operators in TensorFlow. There is an operator named

01:36:18.000 --> 01:36:27.040
2x2 Max Pool operator. In the diagram, it is used four times. It is given four inputs with

01:36:27.040 --> 01:36:34.000
varying values and propagates the highest value of those as its only output. Close to the input

01:36:34.000 --> 01:36:39.680
layer of these four values may be four adjacent pixels where their values might be a brightness

01:36:39.680 --> 01:36:47.440
in some color channel, but higher up they mean whatever they mean. In effect, the Max Pool 2x2

01:36:47.440 --> 01:36:55.680
discards the least important 75% of its input data, preserving and propagating only one

01:36:55.680 --> 01:37:04.000
highest value. In the case of pixels, it might mean the brightest color value. In the case of blades

01:37:04.000 --> 01:37:11.200
of grass, it might mean there is at least one blade of grass here. The interpretation of what is

01:37:11.200 --> 01:37:19.200
discarded depends on the layer, because in a very real sense, layers represent levels of reduction,

01:37:19.200 --> 01:37:26.400
abstraction levels, if you prefer that term. And we should now be clearly seeing one of the most

01:37:26.400 --> 01:37:32.640
important ideas in deep neural networks, the reduction has to be done at multiple levels

01:37:32.640 --> 01:37:39.360
of abstraction. Each set of decisions about what is reduced away as irrelevant and what is kept as

01:37:39.360 --> 01:37:46.480
possibly relevant can only be made at an appropriate abstraction level. We cannot yet abstract away

01:37:46.480 --> 01:37:53.120
the lawn if all we know is there are dark and light green areas levels. This is a simplification.

01:37:53.680 --> 01:37:59.840
Decisions made in this manner will be heated only if they have contributed to positive outcomes in

01:37:59.840 --> 01:38:06.880
learning. Unreliable and useless decision makers will be ignored using any of several mechanisms

01:38:06.880 --> 01:38:15.040
that we may apply during learning. More later, for now, we continue by examining the most popular

01:38:15.040 --> 01:38:22.800
subset of all TensorFlow operators. The convolution family from the TensorFlow manual,

01:38:22.800 --> 01:38:28.960
note that although these ops are called convolution, they are strictly speaking cross

01:38:28.960 --> 01:38:35.840
correlation. Convolution layers discover cross correlations and co-occurrences of various kinds.

01:38:35.840 --> 01:38:42.640
Co-occurrences to known patterns in the image at various locations. Spatial relationships

01:38:42.640 --> 01:38:49.200
within an image itself, like Jeff Hinton's recent example of the mouth normally being found below

01:38:49.200 --> 01:38:56.560
the nose. And more obviously, in the supervised learning case, correlations between discovered

01:38:56.560 --> 01:39:02.800
patterns and the available meta-information, tags, labels that correlate with the patterns

01:39:02.800 --> 01:39:09.360
the system may discover. This is what allows an image-understander to tag the occurrence of a

01:39:09.360 --> 01:39:16.720
nose in an image with the text string nose. Beyond this, such systems may learn to understand

01:39:16.720 --> 01:39:24.320
concepts like behind and under. The information that is propagated to the higher levels in the

01:39:24.320 --> 01:39:31.440
network now describes these correlations. Uncorrelated information is viewed as non-salient

01:39:31.440 --> 01:39:38.880
and is discarded. In the Crescent diagram, this discarding is done by a max pooling layer after

01:39:38.880 --> 01:39:46.560
the convolution plus ReLU layers. ReLU is a kind of layer operator that discards negative values,

01:39:46.560 --> 01:39:53.120
introducing a non-linearity that is important for DL but not really important for our analysis.

01:39:53.120 --> 01:40:01.440
This pattern of three layers, convolution, then ReLU, then a pooling layer, is quite popular

01:40:01.440 --> 01:40:07.760
because this combination is performing one reliable reduction step. These three-layer types

01:40:07.760 --> 01:40:15.120
in this packaged sequence may appear many times in a DL computational graph. In each of these

01:40:15.120 --> 01:40:21.200
three-layer packages is reducing away things that levels below had no chance of evaluating

01:40:21.200 --> 01:40:28.160
for saliency because they didn't understand their input at the correct level. Again,

01:40:28.160 --> 01:40:34.560
this is why deep learning is deep because you can only do reduction by discarding the irrelevant

01:40:34.560 --> 01:40:41.280
if you understand what is relevant and irrelevant at each different level of abstraction. Is

01:40:41.280 --> 01:40:48.000
deep learning science or not? While the deep learning process can be described using mathematical

01:40:48.000 --> 01:40:56.480
notation, mostly using linear algebra, the process itself isn't scientific. We cannot explain how

01:40:56.480 --> 01:41:03.360
this system is capable of forming any kind of understanding by just staring at these equations,

01:41:03.360 --> 01:41:08.480
since understanding is an emergent effect of repeated reductions over many layers.

01:41:08.480 --> 01:41:17.680
Consider the convolution operators. As the TF manual quote clearly states, convolution layers discover

01:41:17.680 --> 01:41:25.920
correlations. Many blades of grass together typically means a lawn. In TF, a lot of cycles

01:41:25.920 --> 01:41:32.320
are spent on discovering these correlations. Once found, the correlation leads to some

01:41:32.320 --> 01:41:37.760
adjustments of some way to make the correct reduction more likely to be rediscovered

01:41:37.760 --> 01:41:43.280
the next round, because this reduction is done multiple times. But in essence,

01:41:43.280 --> 01:41:48.640
all correlations are forgotten and have to be rediscovered in every path through the deep

01:41:48.640 --> 01:41:54.160
learning loop of upward signaling and downward gradient descent with minute adjustments to

01:41:54.160 --> 01:42:01.200
erring variables. This system is in effect learning from its mistakes, which is a good sign,

01:42:01.200 --> 01:42:06.560
since that may well be the only way to learn anything. At least at these levels.

01:42:06.560 --> 01:42:13.840
This up and down may be repeated many times for each image in the learning set. This up and down

01:42:13.840 --> 01:42:20.400
makes some sense for image understanding. Some are using the same algorithms for text.

01:42:21.120 --> 01:42:27.680
Fortunately, in the text case, there are very efficient alternatives to this ridiculously

01:42:27.680 --> 01:42:35.200
expensive algorithm. For starters, we can represent the discovered correlations explicitly,

01:42:35.200 --> 01:42:41.520
using regular pointers or object references in our programming languages.

01:42:42.320 --> 01:42:50.320
Or, synapses in brains. This software neuron correlates with that software neuron says a

01:42:50.320 --> 01:42:56.800
synapse or reference connecting this to that. We shall discuss such systems in the section on

01:42:56.800 --> 01:43:03.760
organic learning, which is coming up next. Then either the deep learning family of algorithms,

01:43:03.760 --> 01:43:10.480
or organic learning, are scientific in any meaningful way. They jump to conclusions on

01:43:10.480 --> 01:43:17.600
scant evidence and trust correlations without insisting on provable causality. This is disallowed

01:43:17.600 --> 01:43:23.280
in scientific theory, where absolutely reliable causality is the coin of the realm.

01:43:24.000 --> 01:43:30.960
F equals m a or go home. The most deep neural network programming is uncomfortably close to

01:43:30.960 --> 01:43:37.840
trial and error, with only minor clues about how to improve the system when reaching mediocre results.

01:43:38.480 --> 01:43:45.120
Adding more layers doesn't always help. These kinds of problems are the everyday reality to

01:43:45.120 --> 01:43:52.080
most practitioners of deep neural networks. With no a priori models, there will be no a priori

01:43:52.080 --> 01:43:58.480
guarantees. The best estimate of the reliability and correctness of any deep neural network,

01:43:58.480 --> 01:44:04.560
or even any holistic system we can ever devise, is going to be extensive testing.

01:44:05.200 --> 01:44:11.440
We're on this later. Why would we ever use engineered systems that cannot be guaranteed

01:44:11.440 --> 01:44:18.880
to provide the correct answer? Because we have no choice. We only use holistic methods when the

01:44:18.880 --> 01:44:25.920
reliable reductionist methods are unavailable. As is the case when the task requires the ability

01:44:25.920 --> 01:44:32.320
to perform autonomous reduction of context rich slices of our rich complex reality as a whole.

01:44:33.040 --> 01:44:39.840
When the task requires understanding, don't we have an alternative to these under liable machines?

01:44:40.480 --> 01:44:47.520
Sure we do. There are billions of humans on the planet that are already masters of this complex

01:44:47.520 --> 01:44:54.800
task because they live in the rich world and need skills that are unavailable with reductionist methods,

01:44:54.800 --> 01:45:01.120
starting with low level things like object permanence. So you can replace a well performing

01:45:01.120 --> 01:45:07.680
but theoretically unproven contraption, a holistic understanding machine built out of deep neural

01:45:07.680 --> 01:45:14.080
networks, with a well performing human being using a deeply mystical kind of understanding

01:45:14.080 --> 01:45:20.800
hidden in their opaque heads. Who earns much more per hour. This doesn't look like much of an

01:45:20.800 --> 01:45:27.600
improvement. The machine cannot be proven correct because it doesn't function like normal computers.

01:45:28.160 --> 01:45:36.000
It is performing reduction, the skill formally restricted to animals. A holistic skill. My

01:45:36.000 --> 01:45:43.280
favorite soundbite is a mere corollary to the frame problem by McCarthy and Hayes. You have seen

01:45:43.280 --> 01:45:49.280
it and you will see it again, since it is one of the stronger results of AI epistemology.

01:45:49.280 --> 01:45:56.560
But we will, in but a few years, agree on a definition of intelligence that makes autonomous

01:45:56.560 --> 01:46:04.240
reduction a requirement. This once semi-heretic soundbite will then be obvious to all. If it

01:46:04.240 --> 01:46:13.600
isn't already, our intelligences are fallible. Chapter 6. Experimental Epistemology for AI

01:46:13.600 --> 01:46:21.040
We can now create computer based experimental implementations to epistemology level theories

01:46:21.040 --> 01:46:28.160
in order to test them and learn from the outcomes. Experimental epistemology is the use of the

01:46:28.160 --> 01:46:34.640
experimental methods of the cognitive sciences to shed light on debates within epistemology,

01:46:34.640 --> 01:46:42.080
the philosophical study of knowledge and rationally justified belief. Some skeptics contend that

01:46:42.080 --> 01:46:49.600
experimental epistemology or experimental philosophy more generally is an oxymoron.

01:46:50.320 --> 01:46:57.600
If you are doing experiments, they say, you are not doing philosophy. You are doing psychology

01:46:57.600 --> 01:47:04.080
or some other scientific activity. It is true that the part of experimental philosophy that is

01:47:04.080 --> 01:47:10.240
devoted to carrying out experiments and performing statistical analyses on the data obtained is

01:47:10.240 --> 01:47:17.600
primarily a scientific rather than a philosophical activity. However, because the experiments are

01:47:17.600 --> 01:47:24.560
designed to shed light on debates within philosophy, the experiments themselves grow out of mainstream

01:47:24.560 --> 01:47:30.480
philosophical debate and their results are injected back into the debate, with an item

01:47:30.480 --> 01:47:37.040
moving the debate forward. This part of experimental philosophy is indeed philosophy,

01:47:37.040 --> 01:47:44.960
not philosophy as usual perhaps, but philosophy nonetheless. Experimental epistemology by James

01:47:44.960 --> 01:47:51.680
R. B. B. Traditional experimental epistemology conducted experiments on interviews and psychological

01:47:51.680 --> 01:47:58.640
tests on human volunteers or relied on population statistics. As one of the newer branches of

01:47:58.640 --> 01:48:04.560
cognitive science, machine learning has now provided us with a very different approach

01:48:04.560 --> 01:48:11.440
to this domain. We can now create computer-based experimental implementations to epistemology

01:48:11.440 --> 01:48:17.840
level theories in order to test them and learn from the outcomes. In machine learning, the most

01:48:17.840 --> 01:48:24.400
important epistemology level concepts and hypotheses are about reasoning, understanding,

01:48:24.400 --> 01:48:32.480
learning, epistemic reduction, abstraction, creativity, prediction, attention, instincts,

01:48:32.480 --> 01:48:40.480
intuitions, concepts, resiliency, models, reductionism, wholism, and other things all

01:48:40.480 --> 01:48:49.040
sharing these features. One, science has no equations, formulas, or other models for how

01:48:49.040 --> 01:48:58.080
they work. They're epistemology level concepts, not science level concepts. Two, our theories

01:48:58.080 --> 01:49:04.720
about these concepts have to be sufficiently solid and detailed to allow for computer implementations.

01:49:05.440 --> 01:49:12.480
This is because science itself is built on top of epistemology level concepts, and practitioners

01:49:12.480 --> 01:49:18.480
need to be aware of this or they will experience cognitive dissonance-induced confusion and stress.

01:49:19.120 --> 01:49:24.720
The red pill of machine learning confronts the elephant in the room of machine learning.

01:49:24.720 --> 01:49:32.160
Machine learning is not scientific. What can we learn from AI epistemology? An excerpt from the

01:49:32.160 --> 01:49:39.040
red pill can say the following statements from the domain of epistemology and how each of them

01:49:39.040 --> 01:49:45.760
can be viewed as an implementation hint for AI designers. We are already able to measure

01:49:45.760 --> 01:49:52.000
their effects and system competence. You can only learn that which you already almost know.

01:49:52.000 --> 01:50:01.200
Patrick Winston, MIT. Our intelligences are fallible. Monica Anderson. In order to detect

01:50:01.200 --> 01:50:08.080
that something is new, you need to recognize everything old. Monica Anderson. You cannot

01:50:08.080 --> 01:50:15.120
reason about that which you do not understand. Monica Anderson. You are known by the company

01:50:15.120 --> 01:50:22.400
you keep, simple version of the Yanida Lemur from Category Theory and the justification for embeddings

01:50:22.400 --> 01:50:28.800
in deep learning. All useful novelty in the universe is due to processes of variation and

01:50:28.800 --> 01:50:37.200
selection. The selectionist manifesto. Selectionism is the generalization of Darwinism. This is

01:50:37.200 --> 01:50:45.920
right genetic algorithms work. Science has no equations for concepts like understanding, reasoning,

01:50:45.920 --> 01:50:51.760
learning, abstraction, or modeling since they are all epistemology level concepts.

01:50:52.400 --> 01:50:59.280
We cannot even start using science until we have decided what model to use. We must use our

01:50:59.280 --> 01:51:05.840
experience to perform epistemic reductions, discarding the irrelevant, starting from the messy

01:51:05.840 --> 01:51:12.800
real world problem situation until we are left with a scientific model we can use, such as an

01:51:12.800 --> 01:51:20.160
equation. The focus in AI research should be on exactly how we can get our machines to perform

01:51:20.160 --> 01:51:27.680
this pre-scientific epistemic reduction by themselves and the answer to that cannot be found inside

01:51:27.680 --> 01:51:36.000
of science. Chapter 7. The Red Pill of Machine Learning. Reductionism is the use of models.

01:51:36.640 --> 01:51:45.440
Holism is the avoidance of models. Models are scientific models, theories, hypotheses, formulas,

01:51:45.440 --> 01:51:52.480
equations, naive models based on personal experiences, superstitions, and traditional

01:51:52.480 --> 01:52:00.800
computer programs. The deep learning revolution of 2012 changed how we think about artificial

01:52:00.800 --> 01:52:08.080
intelligence, machine learning, and deep neural networks. What changed, and what does this mean

01:52:08.080 --> 01:52:14.800
going forward? The new cognitive capabilities in our machines are the result of a shift in the way

01:52:14.800 --> 01:52:21.600
we think about problem solving. It is the most significant change ever in artificial intelligence

01:52:21.600 --> 01:52:29.360
AI, if not in science as a whole. Machine learning, ML based systems are successfully

01:52:29.360 --> 01:52:36.000
attacking both simple and complex problems using novel methods that only became available after

01:52:36.000 --> 01:52:43.760
2012. We are experiencing a revolution at the level of epistemology which will affect much more

01:52:43.760 --> 01:52:50.080
than just the field of machine learning. We want to add more of these novel methods to our

01:52:50.080 --> 01:52:56.480
standard problem solving toolkit, but we need to understand the trade-offs and the conflict.

01:52:57.040 --> 01:53:04.240
I argue that understanding deep neural networks, DNNs, and other ML technologies requires that

01:53:04.240 --> 01:53:11.040
practitioners adopt a holistic stance which is, at important levels, blatantly incompatible with

01:53:11.040 --> 01:53:17.760
the reductionist stance of modern science. As ML practitioners we have to make hard choices

01:53:17.760 --> 01:53:24.160
that seemingly contradict many of our core scientific convictions. As a result we may get

01:53:24.160 --> 01:53:30.960
the feeling something is wrong. The conflict is real and important and the seemingly counter-intuitive

01:53:30.960 --> 01:53:38.160
choices make sense only when viewed in the light of epistemology. Improved clarity in these matters

01:53:38.160 --> 01:53:43.760
should alleviate the cognitive dissonance experienced by some ML practitioners and should

01:53:43.760 --> 01:53:49.680
accelerate progress in these fields. The title refers to the eye-opening clarity

01:53:49.680 --> 01:53:56.400
some machine learning practitioners achieve when adopting a holistic stance. Parallel dichotomies

01:53:56.400 --> 01:54:03.520
sentient sync research is natural language understanding, NLU. We are creating novel

01:54:03.520 --> 01:54:10.000
systems that allow computers to learn to understand human natural languages. Any one of them,

01:54:10.000 --> 01:54:17.440
we use deep neural networks of our own design. The goal is to achieve some kind of human-like

01:54:17.440 --> 01:54:24.000
but not necessarily human-level understanding. This is very different from traditional natural

01:54:24.000 --> 01:54:31.840
language processing, NLP, which relies on human-made models of some language, such as English,

01:54:31.840 --> 01:54:38.800
and perhaps models of fragments of the world. The NLP and NLU disciplines have chosen

01:54:38.800 --> 01:54:45.440
opposite answers to their difficult two-way choices. They are now defined by these choices,

01:54:45.440 --> 01:54:51.520
and we can use their stances to highlight the main conflict. The split is so deep

01:54:51.520 --> 01:54:58.080
that it cuts through many layers of our reality. The following dichotomies are all manifestations

01:54:58.080 --> 01:55:05.360
of this incompatibility at different levels, listed by impact, but discussed in no particular order.

01:55:05.360 --> 01:55:15.280
The main science, the complex, including the mundane, epistemology, reductionism,

01:55:16.000 --> 01:55:27.040
realism, meanings, reasoning, understanding, problem solving, plan it, then do it, just do it.

01:55:27.680 --> 01:55:34.960
Artificial intelligence, 20th century, good, old-fashioned AI machine learning,

01:55:34.960 --> 01:55:44.720
deep neural networks, natural language and computers, NLP, NLU. The problem-solving level

01:55:44.720 --> 01:55:52.080
provides many familiar examples of these issues. In our mundane lives, we solve many kinds of

01:55:52.080 --> 01:55:58.400
problems every day but our strategies for solving them fall into just those two categories.

01:55:58.400 --> 01:56:06.720
For any complicated problem, we had better have a plan before we start, but most problems

01:56:06.720 --> 01:56:13.920
the brain deals with every day are things we never have to think about because we do not need to plan

01:56:13.920 --> 01:56:20.000
a reason about them. These are the millions of low-level problems we encountered in our

01:56:20.000 --> 01:56:25.840
mundane life every day, and this is the world that our AIs will have to operate in.

01:56:25.840 --> 01:56:33.280
Consider someone walking across the floor. Their brain signals their leg muscles to contract in

01:56:33.280 --> 01:56:40.240
the correct cadence. Do they need to consciously plan each step? Do they reason about how to

01:56:40.240 --> 01:56:46.880
maintain their balance? No. They probably don't even know what leg muscles they have.

01:56:47.600 --> 01:56:54.160
Consider understanding this sentence. Did you use reasoning? Did you use grammar?

01:56:54.160 --> 01:57:00.640
If you are a fluent speaker, you do not need grammars to understand or produce language,

01:57:01.280 --> 01:57:08.400
and you do not have time to reason about language while hearing it spoken. Reasoning is slow,

01:57:08.400 --> 01:57:14.240
but understanding is instantaneous. Consider someone braking for a stoplight.

01:57:14.960 --> 01:57:21.440
How hard should they push on the brake pedal? Do they compute the required differential equation?

01:57:21.440 --> 01:57:25.840
Should such equations be part of the driver's license test?

01:57:26.560 --> 01:57:33.520
Consider someone making breakfast. Did they have to reason about anything or plan anything,

01:57:33.520 --> 01:57:40.240
or did they just do what worked yesterday, without thinking about it? Without consciously planning

01:57:40.240 --> 01:57:48.240
it? Walking and talking, braking and breakfasting, like almost everything we use our brains for,

01:57:48.240 --> 01:57:54.480
rely on learning from our experiences in order to reuse anything that has worked in the past,

01:57:55.120 --> 01:58:03.040
and, over time, we learn to correct our mistakes. These strategies are simple enough that we can

01:58:03.040 --> 01:58:09.360
identify them in other life forms. Dogs understand a lot but do not reason much,

01:58:09.920 --> 01:58:16.240
and we can see how they could be implemented in something like neurons and brains. The split

01:58:16.240 --> 01:58:22.960
in our brains between reasoning and understanding was examined at length in Thinking Fast and Slow

01:58:22.960 --> 01:58:29.600
by Daniel Kahneman. The absolute majority of the brain's effort is spent processing low-level

01:58:29.600 --> 01:58:37.760
sensory input, mostly from the eyes. He calls this System 1. It provides understanding.

01:58:38.400 --> 01:58:43.920
Reasoning is done by System 2 based on the understanding from System 1.

01:58:43.920 --> 01:58:50.080
What most problems we deal with on a daily basis do not require System 2 at all.

01:58:50.720 --> 01:58:56.320
Artificial intelligence and machine learning computers can solve any suitable problem when

01:58:56.320 --> 01:59:03.040
given sufficient human help, such as a complete plan for the solution in the form of a computer

01:59:03.040 --> 01:59:11.440
program and valid input data. But since the AIML Revolution of 2012, we now know how to make

01:59:11.440 --> 01:59:18.240
computers understand certain problem domains through machine learning. The acquired understanding

01:59:18.240 --> 01:59:25.920
allows the machine to just do it for many different problems in the domain, without any human planning,

01:59:25.920 --> 01:59:34.320
reasoning, or programming, and using incomplete, unreliable, and noisy input data. This is

01:59:34.320 --> 01:59:42.000
changing how we are building systems with cognitive capabilities. Everyone working in ML or AI needs

01:59:42.000 --> 01:59:48.880
to understand the trade-offs we must make at the most fundamental, epistemological levels.

01:59:49.520 --> 01:59:55.760
Modern ML requires examining and seriously rethinking many things we were taught to vigilantly

01:59:55.760 --> 02:00:05.040
strive for in our science, technology, engineering, and mathematics STEM educations. Things like

02:00:05.040 --> 02:00:12.400
correlation is bad, but causality is good and do not jump to conclusions on scant evidence

02:00:12.400 --> 02:00:19.200
are still solid advice everywhere inside science. But when building understanding systems,

02:00:19.200 --> 02:00:26.000
these established strategies and modes of thinking no longer work, because correlation discovery and

02:00:26.000 --> 02:00:33.520
handling of sparse, unreliable, and inconsistent input data are exactly the kinds of tasks we will

02:00:33.520 --> 02:00:42.240
have to perform and perform well at these pre-scientific levels. In order to understand how to do this,

02:00:42.240 --> 02:00:49.600
we must switch to a holistic stance. A motivating example, beginning machine learning students

02:00:49.600 --> 02:00:56.720
are given exercises like this. They are given a large spreadsheet, which lists data about houses

02:00:56.720 --> 02:01:03.600
sold a certain year in the US. This information includes among other things, the zip code of the

02:01:03.600 --> 02:01:10.720
house, the living area and square feet, lot size, the number of bedrooms and bathrooms,

02:01:10.720 --> 02:01:17.600
the year the house was built, and the final sale price of the house. We would like to be able to

02:01:17.600 --> 02:01:24.560
predict this final sale price, given the corresponding data for current house we are about to list for

02:01:24.560 --> 02:01:30.960
sale. The given spreadsheet is the data the student will use to train a deep neural network.

02:01:31.520 --> 02:01:37.200
It is the entire learning corpus. It contains everything the system will ever know.

02:01:37.200 --> 02:01:45.440
These students can download deep neural network libraries like Keras and TensorFlow and runnable

02:01:45.440 --> 02:01:53.280
examples for many kinds of problems, including useful training data from places like Hugging Face

02:01:53.280 --> 02:02:01.600
and GitHub. Next the student trains, learns their network using the given data. This may take a while,

02:02:01.600 --> 02:02:08.400
but when learning finishes, they can give the system data for a house it has never seen and

02:02:08.400 --> 02:02:15.040
it will quite reliably predict what the house might sell for. This was the goal of the exercise.

02:02:15.680 --> 02:02:22.240
The student has created a system that understands how to estimate real estate prices from listings,

02:02:22.880 --> 02:02:29.680
but the student still does not understand anything about real estate. The predictive capability

02:02:29.680 --> 02:02:36.160
that many people working in real estate would be willing to pay money for is 100% based on

02:02:36.160 --> 02:02:42.880
understanding in the deep neural network, in the computer, and because all the libraries in many

02:02:42.880 --> 02:02:49.520
pre-sold examples of this nature were freely available, the student did not have to do much

02:02:49.520 --> 02:02:58.880
programming either. The vision. This is desirable. This is what AI should mean. The computer understands

02:02:58.880 --> 02:03:05.840
the problem so that we don't have to. Programming in the future will be like having a conversation

02:03:05.840 --> 02:03:12.400
with a competent coworker, and when the machine understands exactly what we want done, it will

02:03:12.400 --> 02:03:20.080
simply do it. No programming required on our part or on part of the machine, once a suitable,

02:03:20.080 --> 02:03:27.360
partially reductionist framework exists. The rest is learning and it can be done in any human

02:03:27.360 --> 02:03:34.080
language with equal ease. We are on the right track towards something worthy of the name AI

02:03:34.080 --> 02:03:40.960
with current machine learning. Going forward, there are thousands of paths to choose from,

02:03:40.960 --> 02:03:47.360
and the ability to choose wisely will depend on our ability to understand and adopt a holistic

02:03:47.360 --> 02:03:54.480
stance. Reductionism and Holism. These are important terms of the art in epistemology.

02:03:54.480 --> 02:04:02.320
Both of them have numerous correct, useful, and compatible definitions. We will henceforth

02:04:02.320 --> 02:04:09.360
use the following definitions for reasons of usefulness and simplicity. Reductionism is the

02:04:09.360 --> 02:04:19.040
use of models. Holism is the avoidance of models. Models are scientific models, theories, hypotheses,

02:04:19.040 --> 02:04:27.040
formulas, equations, naive models based on personal experiences, superstitions, if you can

02:04:27.040 --> 02:04:34.640
believe that, and traditional computer programs. In the reductionist paradigm, these models are

02:04:34.640 --> 02:04:42.800
created by humans, ostensibly by scientists, and are then used, ostensibly by engineers,

02:04:42.800 --> 02:04:49.600
to solve real world problems. Model creation and model use both require that these humans

02:04:49.600 --> 02:04:57.120
understand the problem domain, the problem at hand, the previously known shared models available,

02:04:57.120 --> 02:05:04.400
and how to design and use models. A PhD degree could be seen as a formal license to create new

02:05:04.400 --> 02:05:12.720
models. Mathematics can be seen as a discipline for model manipulation. But now, by avoiding the

02:05:12.720 --> 02:05:20.560
use of human-made models and switching to holistic methods, data scientists, programmers, and others

02:05:20.560 --> 02:05:28.080
do not themselves have to understand the problems they are given. They are no longer asked to provide

02:05:28.080 --> 02:05:34.720
a computer program or to otherwise solve a problem in a traditional reductionist or scientific way.

02:05:35.440 --> 02:05:42.000
Holistic systems like DNNs can provide solutions to many problems by first learning about the

02:05:42.000 --> 02:05:49.360
domain from data-insult examples, and then, in production, to match new situations to this

02:05:49.360 --> 02:05:56.000
gathered experience. These matches are guesses, but with sufficient learning, the results can be

02:05:56.000 --> 02:06:03.440
highly reliable. We will initially use computer-based holistic methods to solve individual and specific

02:06:03.440 --> 02:06:11.520
problems, such as self-driving cars. Over time, increasing numbers of artificial understanders

02:06:11.520 --> 02:06:18.320
will be able to provide immediate answers, guesses, to wider and wider ranges of problems.

02:06:19.040 --> 02:06:24.720
We can expect to see cell phone apps with such good command of language that it feels like

02:06:24.720 --> 02:06:31.120
talking to a competent co-worker. Voice will become the preferred way to interact with our

02:06:31.120 --> 02:06:38.880
personal AIs. Early and low-level but useful AI will manifest as computers that can solve problems

02:06:38.880 --> 02:06:45.520
we ourselves cannot or cannot be bothered to solve. They need not be superhuman.

02:06:46.080 --> 02:06:52.400
All they need to have in order to be extremely useful is exactly the ability to autonomously

02:06:52.400 --> 02:06:59.040
discover higher-level abstractions in some given problem domain, starting from low-level sensory

02:06:59.040 --> 02:07:06.320
input, for example, by learning from images or reading books. Such systems now exist.

02:07:06.320 --> 02:07:12.560
If we want to understand machine learning, then we need to understand all the strategies in the

02:07:12.560 --> 02:07:19.040
right most column in the tables that follow. They are all part of a holistic stance, and if we are

02:07:19.040 --> 02:07:26.000
working in machine learning, we need to adopt as many of them as possible. Differences at the level

02:07:26.000 --> 02:07:35.600
of epistemology. Reductionism in Science versus Holism in Machine Learning. The use of

02:07:35.600 --> 02:07:46.320
models versus the avoidance of models. Raising versus understanding requires human understanding

02:07:46.320 --> 02:07:54.800
versus provides human-like understanding. Problems are solved in an abstract model space versus

02:07:54.800 --> 02:08:01.520
problems are solved directly in the problem domain. Unbeatable strategy for dealing with

02:08:01.520 --> 02:08:08.080
a wide range of suitable problems faced by humans. Versus may handle some problems in

02:08:08.080 --> 02:08:14.560
domains where reductionist models cannot be created or used, known as bizarre domains.

02:08:15.840 --> 02:08:21.760
Handles many important complicated problems such as going to the moon or a highway system.

02:08:22.400 --> 02:08:28.400
Versus handles many important complex problems such as protein folding and playing go.

02:08:28.400 --> 02:08:37.840
Handles problems requiring planning or cooperation. Versus handles simple mundane problems such as

02:08:37.840 --> 02:08:45.360
understanding language or vision or making breakfast. Money rows in these tables discuss

02:08:45.360 --> 02:08:52.560
hard trade-offs where compromises are impossible or prohibitively expensive. These are identified

02:08:52.560 --> 02:08:59.120
by bold face numbers in the first column. The meaning rows may not be clear trade-offs or even

02:08:59.120 --> 02:09:06.640
disjoint alternatives. Mixed systems are described in a separate chapter. These form the core of

02:09:06.640 --> 02:09:12.240
these dichotomies and are discussed in most of what follows, but also in detail at the

02:09:12.240 --> 02:09:19.120
chapter on introducing AI epistemology and in videos of talks. A leather report is based on

02:09:19.120 --> 02:09:26.240
models in meteorology. To solve the problem directly in the problem domain, open a window

02:09:26.240 --> 02:09:34.000
to check if it smells like rain. Reductionism is the greatest invention our species has ever made.

02:09:34.640 --> 02:09:40.480
But reductionist models cannot be created or used when any one of the multitudes of blocking

02:09:40.480 --> 02:09:48.560
issues are present. Models work, in theory or in a laboratory where we can isolate a device,

02:09:48.560 --> 02:09:57.120
organism or phenomenon from a changing environment. However, complex situations may involve tracking

02:09:57.120 --> 02:10:03.360
and responding to a large number of conflicting and unreliable signals from a constantly changing

02:10:03.360 --> 02:10:10.000
world or environment. Reductionism is here at a severe disadvantage and can rarely perform

02:10:10.000 --> 02:10:16.560
above the level of statistical models. In contrast, holistic machine learning methods

02:10:16.560 --> 02:10:22.400
learning from unfiltered inputs can discover correlations that humans might miss and can

02:10:22.400 --> 02:10:28.800
construct internal pattern-based structures to provide recognition, epistemic reduction,

02:10:28.800 --> 02:10:36.880
abstraction, prediction, noise rejection and other cognitive capabilities. Humans generally

02:10:36.880 --> 02:10:44.080
use holistic methods for seemingly simple, but in reality, complex mundane problems

02:10:44.080 --> 02:10:50.720
like understanding vision, human language, learning to walk, or making breakfast.

02:10:51.360 --> 02:10:58.960
Computers use them for very complex problems and mel-based AI in general, such as protein folding

02:10:58.960 --> 02:11:06.000
and playing go, but also simpler ones, such as real estate pricing. Main trade-offs.

02:11:06.000 --> 02:11:15.600
Reductionism in science versus realism in machine learning. Optimality, the best answer,

02:11:15.600 --> 02:11:24.880
versus economy, reuse no useful answers. Completeness, all answers, versus promptness,

02:11:24.880 --> 02:11:33.600
except first use for a answer. Repeatability, same answer every time, versus learning,

02:11:33.600 --> 02:11:42.960
versus learning, results improve with practice. Extrapolation, in low-dimensionality domains,

02:11:42.960 --> 02:11:52.160
versus interpolation, even in high-dimensionality domains. Transparency, understand the process

02:11:52.160 --> 02:11:59.280
to get the answer, versus intuition, accept useful answers even if achieved by unknown or

02:11:59.280 --> 02:12:08.880
subconscious means. Explainability, understand the answer, versus positive ignorance, no need to

02:12:08.880 --> 02:12:16.800
even understand the problem or problem domain. Shareability, abstract models are taught in

02:12:16.800 --> 02:12:24.960
communicated using language or software, versus copyability. ML understanding, a competence

02:12:24.960 --> 02:12:33.200
can be copied as a memory image. Optimality, completeness, and repeatability are only available

02:12:33.200 --> 02:12:40.320
in theoretical model spaces and sometimes under laboratory conditions. Economy and promptness

02:12:40.320 --> 02:12:46.160
had much higher survival value in evolutionary history than optimality and completeness.

02:12:46.800 --> 02:12:52.960
The strongest hint that a system is holistic is that the results improve with practice because

02:12:52.960 --> 02:12:59.840
the system learns from its mistakes. In machine learning, a larger learning corpus is in general

02:12:59.840 --> 02:13:06.480
better than the smaller one because it provides more opportunities for making mistakes to learn from,

02:13:06.480 --> 02:13:13.680
such as corner cases. Models created by humans have manageable numbers of parameters because

02:13:13.680 --> 02:13:19.280
the scientist or engineer working on the problem has done a, hopefully correct,

02:13:19.280 --> 02:13:26.880
epistemic reduction from a complex and messy world to a computable model. This allows experimentation

02:13:26.880 --> 02:13:33.600
with what if scenarios by varying model parameters. It is up to the model user to determine which

02:13:33.600 --> 02:13:40.800
extrapolations are reasonable. In holistic ML systems, we are getting used to systems with

02:13:40.800 --> 02:13:47.200
millions or billions of parameters. These structures are very difficult to analyze,

02:13:47.200 --> 02:13:53.280
and just like with human intelligences, the best way to estimate their competence is through

02:13:53.280 --> 02:14:01.120
testing. Extrapolation is typically out of scope for holistic systems. The majority of end users

02:14:01.120 --> 02:14:07.440
will have no interest in how some machine came up with some obviously correct answer. They will

02:14:07.440 --> 02:14:14.800
just accept it the way we accept our own understanding of language, even though we do not know how we

02:14:14.800 --> 02:14:22.720
do it. We now find ourselves asking our machines to solve problems we either don't know how to solve,

02:14:22.720 --> 02:14:30.320
or can't be bothered to figure out how to solve. We have reached a major benefit of AI. We can be

02:14:30.320 --> 02:14:36.240
positively ignorant of many mundane things and will be happy to delegate such matters to our

02:14:36.240 --> 02:14:43.680
machines so that we may play or focus on more important things. Some schools of thought tend to

02:14:43.680 --> 02:14:51.920
overvalue explainability. To them, ML is a serious step down from results obtained scientifically

02:14:51.920 --> 02:14:58.560
where we can all inspect the causality, for instance in a reductionist production, expert

02:14:58.560 --> 02:15:06.000
systems. But the bottom line is that today we can often choose between one, understanding the

02:15:06.000 --> 02:15:14.640
problem domain, problem, the use of science and relevant models, and the answer. Or two, just

02:15:14.640 --> 02:15:20.480
getting a useful answer without even bothering to understand the problem or the problem domain.

02:15:21.120 --> 02:15:28.480
The latter, positive ignorance, is a lot closer to AI than the first, and we can expect the use

02:15:28.480 --> 02:15:35.760
of holistic methods to continue to increase. Science strives towards a consensus world model in order

02:15:35.760 --> 02:15:41.840
to facilitate communication and minimize costly engineering mistakes caused by ignorance and

02:15:41.840 --> 02:15:49.840
misunderstandings. Scientific communication requires a high-level context, a world model,

02:15:49.840 --> 02:15:57.840
shared by participants, and agreed upon signals such as words, math, and software. But direct

02:15:57.840 --> 02:16:04.960
understanding, such as the skills to become a just grandmaster or a downhill skier, cannot be

02:16:04.960 --> 02:16:12.800
shared using words. The experience must be acquired using individual practice. Computer-based systems

02:16:12.800 --> 02:16:18.720
that learn a skill through practice can share the entire understanding so acquired by copying the

02:16:18.720 --> 02:16:24.000
memory content to another machine. Advantages of Holistic Methods

02:16:25.200 --> 02:16:30.000
Reductionism in Science versus Holism in Machine Learning

02:16:30.000 --> 02:16:38.080
N.P. Hard Problems cannot be solved, versus fines-valid solutions by guessing well-based

02:16:38.080 --> 02:16:46.160
on a lifetime of experience. Geigo, garbage in, garbage out is a recognized problem,

02:16:46.800 --> 02:16:51.440
versus copes with missing, erroneous, and misleading inputs.

02:16:51.440 --> 02:16:59.200
Brightness. Experience catastrophic failures at edges of competence, versus anti-fragile.

02:16:59.200 --> 02:17:07.040
Learns from mistakes, especially almost correct guesses in small, correctable failures.

02:17:08.240 --> 02:17:13.600
The models of a constantly changing world are obsolete the moment they are created,

02:17:14.160 --> 02:17:20.080
versus incremental learning provides continuous adaptation to a constantly changing world.

02:17:20.080 --> 02:17:25.360
Algorithms may be incorrect or may be incorrectly implemented,

02:17:26.000 --> 02:17:32.640
versus self-repairing systems can tolerate or correct internal errors. It is because we desire

02:17:32.640 --> 02:17:41.520
certainty, optimality, completeness, etc. L.N.P. Hardness becomes a problem. There are many

02:17:41.520 --> 02:17:47.440
problems where it is relatively easy to find a provably valid solution, but where finding

02:17:47.440 --> 02:17:54.320
all solutions can be very expensive. Real-world traveling salesmen merrily travel long reasonable

02:17:54.320 --> 02:18:01.280
routes. If a reductionist system does not have complete and correct input data, it either cannot

02:18:01.280 --> 02:18:08.000
get started or produces questionable output. But it is an important requirement of real-world

02:18:08.000 --> 02:18:14.160
understanding machines that they be able to detect what is salient, important, in their input in

02:18:14.160 --> 02:18:22.080
order to avoid paying attention to, and learning from, noise. And they have to deal with incomplete,

02:18:22.080 --> 02:18:27.840
erroneous, and misleading input generated by millions of other intelligent agents with

02:18:27.840 --> 02:18:35.360
goals at odds with their own. They need to be able to detect omissions, duplications, errors,

02:18:35.360 --> 02:18:42.800
noise, lies, etc. And the only epistemologically plausible way to do this is to relate the input

02:18:42.800 --> 02:18:50.160
to similar input they have understood in the past, what they already know. They need to understand

02:18:50.160 --> 02:18:57.360
what matters but if they can also understand some of the noise. This is advertising, they can exploit

02:18:57.360 --> 02:19:03.920
that. There are many image and video apps available featuring image understanding based on deep

02:19:03.920 --> 02:19:11.440
learning. These apps can remove backgrounds, sharpen details like eyelashes, restore damaged

02:19:11.440 --> 02:19:19.120
photographs, etc. We need to keep in mind that the ability of holistic systems to fill in data

02:19:19.120 --> 02:19:25.920
and detect noise depends on them having learned from similar data in the past. We note that all

02:19:25.920 --> 02:19:31.920
the image improvements are confabulations based on prior experience from their learning corpora.

02:19:32.560 --> 02:19:39.280
But we can also note that image composition using these methods yields totally seamless images,

02:19:39.280 --> 02:19:47.040
very far from cut and paste of pixels. And quite similarly, we find language confabulation by

02:19:47.040 --> 02:19:54.880
systems like GPT-3 to flow seamlessly between sentences and topics. They have nothing to say,

02:19:54.880 --> 02:20:01.760
but they say it well. However, they bring us closer to meaningful language generation and

02:20:01.760 --> 02:20:08.000
when we achieve that, the public perception of what computers are capable of will totally change.

02:20:08.000 --> 02:20:15.040
Most of cognition is recognition. Being able to recognize that something has occurred before

02:20:15.040 --> 02:20:22.560
and knowing what might happen next has enormous survival value for any animal species. A mature

02:20:22.560 --> 02:20:29.280
human has used their eyes and other senses for decades. This represents an enormous learning

02:20:29.280 --> 02:20:36.880
corpus and they can understand anything they have prior experience of. The mistakes made by humans,

02:20:36.880 --> 02:20:43.840
animals and by holistic ML systems are very often of a near-miss variety which provides an

02:20:43.840 --> 02:20:50.640
opportunity to learn to do a better next time. Contrast is to reductionist software systems

02:20:50.640 --> 02:20:57.120
created for similar goals. Rule-based systems have long been infamous for their brittleness.

02:20:57.760 --> 02:21:03.360
As long as the rules and the rules that match the current input and reality perfectly,

02:21:03.360 --> 02:21:10.320
the results will be useful, repeatable and reliable. But at the edges of their competence,

02:21:10.320 --> 02:21:17.280
where the matches become more tenuous, the quality rapidly drops. Minor mistakes in the

02:21:17.280 --> 02:21:24.160
rule sets in the world modeling may lead such systems to return spectacularly incorrect results.

02:21:24.880 --> 02:21:30.400
Sometimes repeatability is important and sometimes tracking a changing world by

02:21:30.400 --> 02:21:37.600
continuously learning more about it is important. In ML, continuous incremental learning makes

02:21:37.600 --> 02:21:44.480
it possible to stay up to date. If we want repeatability, we can emit a condensed,

02:21:44.480 --> 02:21:50.800
cleaned and frozen competence file from a learner that can be loaded into non-learning,

02:21:50.800 --> 02:21:56.880
read-only, cloud-based understanding machines that serve the world and provide repeatability

02:21:56.880 --> 02:22:04.160
between scheduled software and competence releases. Three, in the case of reductionist systems,

02:22:04.160 --> 02:22:10.480
such as cell phone OS releases, we are used to getting well-tested new versions with minor bug

02:22:10.480 --> 02:22:18.080
fixes and occasional major features at regular intervals. Such systems learn only in the sense

02:22:18.080 --> 02:22:23.520
that the people who created them have learned more and put these insights into the new release.

02:22:23.520 --> 02:22:29.760
Reductionist systems working with complete incorrect input data are expected to provide

02:22:29.760 --> 02:22:34.720
correct and repeatable results according to the implementation of the algorithm.

02:22:35.360 --> 02:22:42.080
But both the algorithm and the implementation may have errors. If the algorithm does not adequately

02:22:42.080 --> 02:22:49.360
model its reality, then we have reduction errors. In the implementation, we may have bugs.

02:22:49.360 --> 02:22:54.960
Holistic software systems can be designed to a different standard of correctness.

02:22:55.680 --> 02:23:02.320
Since input data is normally incomplete and noisy, and results are based on emergent effects,

02:23:02.320 --> 02:23:08.080
we can expect similar enough results even if parts of the system have been damaged,

02:23:08.080 --> 02:23:14.000
for instance by catastrophic forgetting. Holistic systems can be made capable of

02:23:14.000 --> 02:23:20.160
self-repair using incremental learning. This has been observed in the deep learning community.

02:23:20.800 --> 02:23:25.600
Another technique is that when using multiple parallel threads in learning,

02:23:25.600 --> 02:23:30.080
there may be conflicts that would normally require locking of some values.

02:23:30.800 --> 02:23:36.240
But if the operations are simple enough, such as just incrementing a value,

02:23:36.240 --> 02:23:42.240
we can forego thread safety in the locking since the worst outcome is the loss of a single increment

02:23:42.240 --> 02:23:48.560
in a system that uses emergent results from millions of such values. And the mistake would,

02:23:48.560 --> 02:23:55.600
in a well-designed system, be self-correcting in the long run. At the cloud level, absolute

02:23:55.600 --> 02:24:02.640
consistency may not be as hard a requirement as it is for reductionist systems. Much larger

02:24:02.640 --> 02:24:08.880
mistakes can be expected to be attributable to misunderstandings of the corpus or poor corpus

02:24:08.880 --> 02:24:17.280
coverage. General strategies, decomposition into smaller problems, versus generalization

02:24:17.280 --> 02:24:24.720
may lead to an easier problem. Assuming discards everything irrelevant based on how new information

02:24:24.720 --> 02:24:30.880
matches existing experience, versus a machine discards everything irrelevant based on how

02:24:30.880 --> 02:24:41.200
new information matches existing experience, modularity, versus composability, gather valid,

02:24:41.200 --> 02:24:48.640
correct, and complete input data, versus use whatever information is available, and use all

02:24:48.640 --> 02:25:01.600
of it. Formal, rigorous methods, versus informal ad hoc methods, absolute control, versus creativity,

02:25:01.600 --> 02:25:10.160
intelligent design, versus evolution. The reductionist battle cries, the whole is equal to the sum

02:25:10.160 --> 02:25:17.120
of its parts, which gives us a license to split a large complicated problem into smaller problems

02:25:17.120 --> 02:25:23.600
to solve each of those using some suitable model, and then to combine all the sub-solutions

02:25:23.600 --> 02:25:31.120
into a model-based solution for the original, larger problem, such as in moonshots, highway

02:25:31.120 --> 02:25:38.640
systems, international banking, and generally in industrial intelligent design. This works

02:25:38.640 --> 02:25:45.520
in simple and some complicated domains, but cannot be done in complex domains, where everything

02:25:45.520 --> 02:25:52.240
potentially affects everything else. Spreading a complex system may cause any emergent effects

02:25:52.240 --> 02:26:00.960
to disappear, confounding analysis. Examples of complex problem domains are politics, neuroscience,

02:26:00.960 --> 02:26:09.200
ecology, economy, including stock markets, and cellular biology. Our life sciences operate

02:26:09.200 --> 02:26:17.520
in a complex problem domain because life itself is complex. Some say biology has physics envy,

02:26:17.520 --> 02:26:23.280
because in the life sciences, reductionist models are difficult to create and justify.

02:26:24.000 --> 02:26:31.520
On the other hand, physics is for simple problems. Problems with many complex interdependencies and

02:26:31.520 --> 02:26:38.640
unknown webs of causality can now be attacked using deep neural networks. These systems discover

02:26:38.640 --> 02:26:44.720
useful correlations and may often find solutions using mere hints in the input which match their

02:26:44.720 --> 02:26:50.400
prior experience. Reductionist strategies with correctness requirements outlaw this.

02:26:51.040 --> 02:26:57.520
It is notable that one of the larger triumphs of holistic methods is protein folding, which is a

02:26:57.520 --> 02:27:04.400
problem at the very core of the life sciences. So holistic understanding of a complex system

02:27:04.400 --> 02:27:11.120
can be acquired by observing it over time and learning from its behavior. There is no need to

02:27:11.120 --> 02:27:18.480
split the problem into pieces. Part of the holistic stance is that we give the machine everything.

02:27:18.480 --> 02:27:27.680
Holism comes from the Greek word, holos, amicron, lambda, amicron, sigma, in the written text.

02:27:27.680 --> 02:27:35.840
The whole, that is to say, all the information we have. If we start filtering the input data,

02:27:35.840 --> 02:27:42.400
by cleaning it up, then the system will effectively learn from a polyana version of the world,

02:27:42.400 --> 02:27:48.400
which will be confusing once it has to deal with real life inputs in a production environment.

02:27:48.960 --> 02:27:54.320
If we want our machines to learn to understand the world all by themselves,

02:27:54.320 --> 02:28:00.720
then we should not start by applying heavy-handed heuristic cleanup operations of our own design

02:28:00.720 --> 02:28:08.080
on their input data. Sometimes, reductionist strategies are clearly inferior. The natural

02:28:08.080 --> 02:28:14.080
language understanding is such a domain. Language understanding in a fluent speaker

02:28:14.080 --> 02:28:21.840
is almost 100% holistic because it is almost entirely based on prior exposure. We are now

02:28:21.840 --> 02:28:27.760
finding out that it is much easier to build a machine to learn any language on the planet from

02:28:27.760 --> 02:28:34.560
scratch than it is to build a good old-fashioned artificial intelligence, 20th century reductionist

02:28:34.560 --> 02:28:41.520
AI-based style machine that understands a single language such as English. The process where a

02:28:41.520 --> 02:28:48.080
human, by using their understanding, discards everything irrelevant to arrive at what matters

02:28:48.080 --> 02:28:54.240
is called the epistemic reduction and is discussed in the first five chapters in this book.

02:28:54.960 --> 02:29:01.760
This is the most important operation in reductionism, but for some reason discussions of reductionism

02:29:01.760 --> 02:29:09.920
in the past have tended to focus on other aspects. Perhaps this is a new result. ML systems discard

02:29:09.920 --> 02:29:16.560
with little fanfare anything that was expected and that has been seen before as boring, harmless,

02:29:16.560 --> 02:29:23.520
or otherwise ignorable. They may also discard things significantly outside of their experience

02:29:23.520 --> 02:29:30.160
as noise. Things can only be reduced away at the semantic level. They can be recognized that

02:29:30.880 --> 02:29:37.200
operations capable of epistemic reduction at multiple layers discard anything that's understood

02:29:37.200 --> 02:29:43.760
at that layer, and they may pass on upward to the next higher semantic layer, a summary of what

02:29:43.760 --> 02:29:50.400
they discarded plus everything they did not understand at their level. Empire levels do the

02:29:50.400 --> 02:29:57.520
same. This is why deep learning is deep. Intelligently designed systems are often made up out of

02:29:57.520 --> 02:30:04.160
interchangeable modules, which allow for easy replacement in case of failure, and in some

02:30:04.160 --> 02:30:10.960
cases, and especially in software, allow for customization of functionality by replacing

02:30:10.960 --> 02:30:17.280
or adding modules. These modules have well specified interfaces that allow for such

02:30:17.280 --> 02:30:24.240
interconnections. In the holistic case we can consider a human cell with thousands of proteins

02:30:24.240 --> 02:30:31.120
interact on contact or as required with many substances floating around in the cellular fluid.

02:30:31.680 --> 02:30:35.680
It is not the result of intelligent design, and it shows

02:30:35.680 --> 02:30:42.000
there are overlaps and redundancies that may contribute to more reliable operation, and there

02:30:42.000 --> 02:30:49.680
are multiple potentially complex mechanisms keeping each other in check, or we can consider music,

02:30:49.680 --> 02:30:56.000
or multiple notes in accord in different timbres in a symphony orchestra in a composition will

02:30:56.000 --> 02:31:02.640
conjure an emerging harmonic whole that sounds different than the sum of its parts, or consider

02:31:02.640 --> 02:31:10.560
spices in a soup, or opinions in a meeting that leads to a consensus. The word, composability,

02:31:10.560 --> 02:31:17.440
fits this capability in the holistic case. Unfortunately, in much literature it is merely

02:31:17.440 --> 02:31:25.040
used as a synonym for its reductionist counterpart, modularity. As discussed in the Geico case,

02:31:25.040 --> 02:31:30.960
in the section above, holistic ML systems can fill in missing details starting from

02:31:30.960 --> 02:31:38.240
various scant evidence. Compare for example confabulations of systems like GPT-3 and image

02:31:38.240 --> 02:31:45.360
enhancement apps. They supply the missing details by jumping to conclusions based on few clues

02:31:45.360 --> 02:31:51.840
and lots of experience. Since we are not omniscient and don't even know what is happening behind our

02:31:51.840 --> 02:31:58.480
backs, scant evidence is all we will ever have, but it is amazing how effective scant evidence

02:31:58.480 --> 02:32:05.920
can be in a familiar context. We can drive a car through fog or find an alarm clock in absolute

02:32:05.920 --> 02:32:12.720
darkness. The more the system has learned, the less input is needed to arrive at a reasonable

02:32:12.720 --> 02:32:18.560
identification of the problem and hence retrieve a previously discovered working solution.

02:32:19.440 --> 02:32:25.760
Formal methods and experimental rigorousness make for good science. On the other hand,

02:32:25.760 --> 02:32:32.480
holistic methods can follow tenuous threads, hoping for stronger threads or some solution,

02:32:32.480 --> 02:32:38.800
with little effort spent on backtracking or documentation because once a solution is found,

02:32:38.800 --> 02:32:45.200
it is the only thing that matters. Tracking has little value in non-repeating situations

02:32:45.200 --> 02:32:53.120
or when using holistic methods at massive scales, such as in deep learning. Absolute control requires

02:32:53.120 --> 02:32:59.120
that we know exactly what the problems and solutions are and all we need to do is implement them.

02:32:59.840 --> 02:33:06.320
Once deployed, systems frozen in this manner, which are exactly implementing the models of

02:33:06.320 --> 02:33:13.200
their creators, cannot improve by learning since there is no room for variation in the existing

02:33:13.200 --> 02:33:20.560
process and hence no experimentation and no way to discover further improvements. Only

02:33:20.560 --> 02:33:28.640
holistic systems can provide creativity and useful novelty. We also observe that, learning itself

02:33:28.640 --> 02:33:34.640
is a creative act, since it must fit new information into an existing network of prior

02:33:34.640 --> 02:33:42.880
experience. Just like the term, holism has been abused, so has intelligent design,

02:33:42.880 --> 02:33:48.400
which is a perfectly reasonable term for reductionist industrial end-to-end practice

02:33:48.400 --> 02:33:55.600
that consistently provides excellent results. On the holistic side, evolution in nature has

02:33:55.600 --> 02:34:01.840
created wonderful solutions to all kinds of problems that plants and animals need to handle.

02:34:02.560 --> 02:34:07.760
But we can put evolution, also known in the general sense as selectionism,

02:34:07.760 --> 02:34:15.440
to work for us in our holistic machines. They can create new, wonderful designs with a biological

02:34:15.440 --> 02:34:22.080
flavor to them that sometimes, depending on the problem, cannot perform intelligently designed

02:34:22.080 --> 02:34:30.480
alternatives. Evolution is the most holistic phenomenon we know. No goal functions, no models,

02:34:30.480 --> 02:34:38.560
no equations. Evolution is not a scientific theory. Science cannot contain it. It must be

02:34:38.560 --> 02:34:46.320
discussed in epistemology. Mixed systems, deep neural networks can perform autonomous epistemic

02:34:46.320 --> 02:34:53.040
reduction to find high-level representations for low-level input, such as pixels in an image

02:34:53.040 --> 02:35:00.000
or characters in text. Current vision understanding systems can reliably identify thousands of

02:35:00.000 --> 02:35:05.520
different kinds of objects from many different angles in a variety of lighting conditions

02:35:05.520 --> 02:35:13.280
and weather. They can classify what they see, but do not necessarily understand much beyond that,

02:35:13.280 --> 02:35:20.160
such as the expected behaviors of other, intentional Asians like cars, pedestrians,

02:35:20.160 --> 02:35:28.960
or cats. Therefore, at the moment in 2022, most deployments of machine learning use a mixture

02:35:28.960 --> 02:35:36.000
of reductionist and holistic methods, equations and formulas devised by humans implemented as

02:35:36.000 --> 02:35:43.040
computer code, and some inputs from a deep neural network solving a sub-problem that requires it,

02:35:43.040 --> 02:35:51.200
such as vision understanding. Self-driving cars use DNNs for understanding vision, radar,

02:35:51.200 --> 02:35:58.320
and lidar images, discovering high-level information like a pedestrian on the side of the road

02:35:58.320 --> 02:36:05.040
from pixel-based images and this understanding has, until recently, been fed to logic and

02:36:05.040 --> 02:36:12.880
role-based programs that implement the decision-making. Avoid driving into anything, period, that is used

02:36:12.880 --> 02:36:19.280
to control the car. The trend here is to move more and more responsibilities into the deep

02:36:19.280 --> 02:36:26.560
neural network, and over time to remove the hand-coded parts. In essence, the network learns

02:36:26.560 --> 02:36:33.760
not only to see, but learns to understand traffic. We are delegating more and more of our

02:36:33.760 --> 02:36:42.480
understanding of how to drive to the vehicle itself. This is desirable. Experimental Epistemology

02:36:43.680 --> 02:36:50.240
Epistemology is the theory of knowledge. It is concerned with the mind's relation to reality.

02:36:50.240 --> 02:36:57.680
This includes artificial minds. An introduction to epistemology should benefit anyone working in

02:36:57.680 --> 02:37:06.400
the IML field. Scientific statements look like F equals MA, Newton's second law, or E equals MC

02:37:06.400 --> 02:37:13.360
squared, Einstein's famous equation, and can all be proven and or derived from other accepted

02:37:13.360 --> 02:37:21.200
results or verified experimentally. Algebra is built on lemurs that are not part of algebra.

02:37:21.200 --> 02:37:27.840
They cannot be proven inside of algebra. Similarly, epistemological statements are

02:37:27.840 --> 02:37:34.960
not provable in science because science is built on top of epistemology. But when science is not

02:37:34.960 --> 02:37:42.880
helping, such as in bizarre domains, then setting scientific methodology aside and dropping down

02:37:42.880 --> 02:37:50.880
to the level of epistemology sometimes works. Epistemology is, just like philosophy in general,

02:37:50.880 --> 02:37:57.600
an armchair thinking exercise, and the results are judged on internal coherence and consistency

02:37:57.600 --> 02:38:04.720
with other accepted theory rather than by proofs or experiments. However, the availability of

02:38:04.720 --> 02:38:12.160
understanding machines, such as DNNs now suddenly provides the opportunity for actual experiments

02:38:12.160 --> 02:38:18.720
in epistemology. Consider the following statements from the domain of epistemology,

02:38:18.720 --> 02:38:25.040
and how each of them can be viewed as an implementation hint for AI designers. We are

02:38:25.040 --> 02:38:32.080
already able to measure their effects on system competence. You can only learn that which you

02:38:32.080 --> 02:38:42.720
already almost know. Patrick Winston, MIT. Our intelligences are fallible. Monica Anderson.

02:38:42.720 --> 02:38:49.360
In order to detect that something is new, you need to recognize everything old. Monica Anderson.

02:38:50.640 --> 02:38:58.320
You cannot reason about that which you do not understand. Monica Anderson. You are known by

02:38:58.320 --> 02:39:05.360
the company you keep, simple version of the yanni dilemma from category theory and the justification

02:39:05.360 --> 02:39:12.240
for embeddings in deep learning. All useful novelty in the universe is due to processes

02:39:12.240 --> 02:39:20.160
of variation and selection. The selectionist manifesto. Selectionism is the generalization

02:39:20.160 --> 02:39:28.880
of Darwinism. This is right genetic algorithms work. Science has no equations for concepts like

02:39:28.880 --> 02:39:36.240
understanding, reasoning, learning, abstraction, or modeling since they are all epistemology level

02:39:36.240 --> 02:39:44.160
concepts. We cannot even start using science until we have decided what model to use. We must use

02:39:44.160 --> 02:39:50.640
our experience to perform epistemic reductions, discarding the irrelevant, starting from the

02:39:50.640 --> 02:39:57.120
messy real world problem situation until we are left with a scientific model we can use,

02:39:57.120 --> 02:40:04.480
such as an equation. The focus in AI research should be on exactly how we can get our machines

02:40:04.480 --> 02:40:10.960
to perform this pre-scientific epistemic reduction by themselves and the answer to that

02:40:10.960 --> 02:40:18.400
cannot be found inside of science. Artificial General Intelligence Artificial General Intelligence,

02:40:18.400 --> 02:40:26.240
AGI, was a theoretical 20th century reductionist AI attempt to go beyond the narrow AIA of domain

02:40:26.240 --> 02:40:33.840
specific expert systems closer to a general intelligence they thought humans had. The

02:40:33.840 --> 02:40:42.240
term was mostly used by independent researchers, amateurs and enthusiasts. But the AGI term was

02:40:42.240 --> 02:40:48.400
not well enough defined and was not backed by sufficient theory to provide any AI implementation

02:40:48.400 --> 02:40:55.200
guidance and what little progress had been made by these groups was overtaken by holistic methods

02:40:55.200 --> 02:41:03.600
after 2012. Today we know that the entire premise of 20th century reductionist AGI was wrong.

02:41:04.160 --> 02:41:12.320
Humans are not general intelligences at birth. Instead, we are general learners capable of

02:41:12.320 --> 02:41:18.880
learning almost any skill or knowledge required in a wide range of problem domains. If we want

02:41:18.880 --> 02:41:25.760
human compatible cognitive systems, then we should build them in our image in this respect to build

02:41:25.760 --> 02:41:33.760
machines that learn and jump to conclusions on scant evidence. Decades ago, AGI implied a human

02:41:33.760 --> 02:41:40.560
programmed reductionist hand coded program based on logic and reasoning that can solve any problem

02:41:40.560 --> 02:41:47.040
because the programmers anticipated it. To argue against claims that this was impossible,

02:41:47.040 --> 02:41:53.840
the AGI community came up with a promise or threat of self-improving AI. But the amount

02:41:53.840 --> 02:42:01.040
of code in our cognitive systems has shrunk from 6 million propositions in sake around 1990

02:42:01.040 --> 02:42:09.840
to 600 lines of code to play video games around 2017 to about 13 lines of cares code in some

02:42:09.840 --> 02:42:16.560
research reports. And now there's AutoML and other efforts at eliminating all remaining programming

02:42:16.560 --> 02:42:23.760
from ML. The problems are not in the code. There's almost no code left to improve in

02:42:23.760 --> 02:42:32.160
modern machine learning systems. All that matters is the corpus. We can now, after 2012,

02:42:32.160 --> 02:42:37.840
see that machine learning is an absolute requirement for anything worthy of the name AI,

02:42:38.640 --> 02:42:44.240
which makes recursive self-improvement leading to evil superhuman omniscience logic based

02:42:44.240 --> 02:42:51.520
godlike artificial general intelligence a 20th century reductionist AI myth. We must focus on

02:42:51.520 --> 02:42:59.840
artificial general learners. Afterward, science was created to stop people from overrating correlations

02:42:59.840 --> 02:43:05.440
and jumping to erroneous conclusions on scant evidence and then sharing those conclusions

02:43:05.440 --> 02:43:12.960
with others, leading to compounded mistakes and much wasted effort. Consequently, promoting a

02:43:12.960 --> 02:43:19.840
holistic stance has long been a career-ending move in academia, and especially in computer science.

02:43:20.560 --> 02:43:27.440
But now we suddenly have machine learning that performs cognitive tasks such as protein folding,

02:43:27.440 --> 02:43:34.240
playing go, and estimating house prices at useful levels using exactly a holistic stance.

02:43:34.880 --> 02:43:42.080
So now science itself has a cognitive dissonance. This is a conflict about what science is or

02:43:42.080 --> 02:43:49.120
should be. Inherence of these stances leads people to develop significant personal cognitive

02:43:49.120 --> 02:43:54.560
dissonances, which is why discussions about these issues are very unpopular among people

02:43:54.560 --> 02:44:03.360
with solid STEM educations. But the dichotomy is real. We need to deal with it. Our choices so far

02:44:03.360 --> 02:44:12.960
seem to have been too. Claim that dichotomy doesn't exist. But Schrodinger and Pursig also discuss it.

02:44:12.960 --> 02:44:20.480
Claim that the holistic stance doesn't work. But deep learning works. Claim that reductionist

02:44:20.480 --> 02:44:27.840
methods are requirement, hobbling our toolkits for a principle. The reductionist stance also

02:44:27.840 --> 02:44:35.200
makes it difficult to imagine and accept things like systems capable of autonomous epistemic

02:44:35.200 --> 02:44:43.360
reduction, systems that do not have a goal function, systems that improve with practice,

02:44:44.720 --> 02:44:51.840
systems that exploit emergent effects, systems that by themselves make decisions about what

02:44:51.840 --> 02:44:58.880
matters most, systems that occasionally give a wrong answer but are nevertheless very useful.

02:44:59.600 --> 02:45:06.320
So after a serious education in machine learning we don't actually need to do almost any programming

02:45:06.320 --> 02:45:13.280
at all. And we don't need to understand anybody else's problem domains. Because we don't have

02:45:13.280 --> 02:45:21.040
to perform any epistemic reduction ourselves. We should recognize this for what it is. AI was

02:45:21.040 --> 02:45:27.200
supposed to solve our problems for us so we would not have to learn or understand any new

02:45:27.200 --> 02:45:34.880
problem domains. To not have to think. And that's what we have today, in machine learning,

02:45:34.880 --> 02:45:41.680
and with holistic methods in general. Why are some people surprised or unhappy about this?

02:45:42.400 --> 02:45:48.880
In my opinion, this is AI, this is what we have been trying to accomplish for decades.

02:45:48.880 --> 02:45:56.160
People who claim machine understanding is not AI are asking for human level human-centric reasoning

02:45:56.160 --> 02:46:02.880
and are, at their peril, blind to the nascent ML-based understanding we can achieve today.

02:46:03.520 --> 02:46:09.840
With expected reasonable improvements in machine understanding capabilities, familiarity and

02:46:09.840 --> 02:46:16.720
acceptance of the holistic stance will become a requirement for ML and AI-based work. It will

02:46:16.720 --> 02:46:25.040
likely take years for our educational system to adjust. This has been Monica's Little Pills,

02:46:25.040 --> 02:46:47.760
read to you by a computer. Thank you for listening.

