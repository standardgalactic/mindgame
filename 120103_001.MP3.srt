1
00:00:00,000 --> 00:00:10,920
The Blue Pill. Self-improving AI. Self-improving AI is a meme that has been circulating since

2
00:00:10,920 --> 00:00:18,760
the 1980s. Current proponents of the idea include Wastram and Omihandro. My own summary goes

3
00:00:18,760 --> 00:00:25,920
something like this. If we get any kind of AGI going, no matter how slow it is and how

4
00:00:25,920 --> 00:00:31,480
buggy it is, we can give it access to its own source code and let it analyze it and

5
00:00:31,480 --> 00:00:37,280
clean up and fix the bugs and then rewrite its code to be as good as it can make it.

6
00:00:37,280 --> 00:00:43,760
We then start up the slightly smarter AGI and repeat the process until the AGI's get

7
00:00:43,760 --> 00:00:51,560
super intelligent. On the surface, this is irrefutable. We already have examples of systems

8
00:00:51,560 --> 00:00:58,240
improving themselves. We can buy a cheap 3D printer and then quite cheaply print out parts

9
00:00:58,240 --> 00:01:05,000
for a much better 3D printer. Or to make computer chips that go into computers that design better

10
00:01:05,000 --> 00:01:12,160
computer chips. Not to mention evolution of all species in nature. I look at it from an

11
00:01:12,160 --> 00:01:17,880
epistemologist point of view and say, that's a hard line reductionist idea that should

12
00:01:17,880 --> 00:01:25,640
not have made it out of the 20th century. The idea, as its inception, imagined an AGI

13
00:01:25,640 --> 00:01:31,480
as something that was written by teams of human programmers using software development

14
00:01:31,480 --> 00:01:37,800
tools and mathematical equations. What I think the only thing that even approximates this

15
00:01:37,800 --> 00:01:43,760
outcome is that the code is perfect, and humans as well as machines all agree there are no

16
00:01:43,760 --> 00:01:51,120
more improvements to be made. And the resulting AGI's are still not super intelligent. The

17
00:01:51,120 --> 00:01:56,480
most likely outcome is that we all realize the folly in this argument and won't even

18
00:01:56,480 --> 00:02:03,800
try. It's not about the code. The number of lines of code in AI related projects has been

19
00:02:03,800 --> 00:02:26,000
declining rapidly. 2012. 34,000 lines.py.kudukrzebski et al. for ImageNet. 2013. 1571 lines of

20
00:02:26,000 --> 00:02:37,400
Lua to Play Atari games. 2017. 196 lines of Keras to Implement Deep Dream. 2018. Less

21
00:02:37,400 --> 00:02:45,520
than 100 lines of Keras for research paper-level results. And all of these, except Saig, included

22
00:02:45,520 --> 00:02:52,000
as the most famous example of a 20th century reductionist AI system, demonstrates new levels

23
00:02:52,000 --> 00:02:58,680
of power of machine learning. The limits to intelligence are not in the code. In fact,

24
00:02:58,680 --> 00:03:05,320
they are not even technological. The limit of intelligence is the complexity of the

25
00:03:05,320 --> 00:03:12,080
world. Admission is unavailable. The main purpose of intelligence is to guess, to jump

26
00:03:12,080 --> 00:03:18,440
to conclusions on scant evidence, and to do it well, based on a large set of historical

27
00:03:18,440 --> 00:03:24,840
patterns of problems and their solutions or events and their consequences. Because scant

28
00:03:24,840 --> 00:03:31,600
evidence is all we will ever have, we don't even know what goes on behind our back. And

29
00:03:31,600 --> 00:03:37,240
because our intelligence is guessing, I have repeatedly claimed that, all intelligences

30
00:03:37,240 --> 00:03:43,480
are fallible. We are already making machines that are better than humans in some aspect

31
00:03:43,480 --> 00:03:49,960
of guessing. Protein folding and playing go are examples of this. And these machines

32
00:03:49,960 --> 00:03:55,120
will get bigger and better at what they do and will be superhuman in various ways and

33
00:03:55,120 --> 00:04:02,520
in many problem domains, simply based on larger capacity to hold, look up, or search useful

34
00:04:02,520 --> 00:04:09,080
patterns. The code doing that can be hand optimized to the point where any AI improvement

35
00:04:09,080 --> 00:04:15,280
would be insignificant. My own code in the inner loop for understanding any language

36
00:04:15,280 --> 00:04:22,200
on the planet, once it has learned it, in inference mode is about 90 lines of Java.

37
00:04:22,200 --> 00:04:29,040
We can expect a best minor improvements to efficiency and speed. It comes down to the

38
00:04:29,040 --> 00:04:38,240
corpus. In my domain, NLU, simple tests can be scored at 100% after a few minutes of learning

39
00:04:38,240 --> 00:04:44,840
on a laptop. Continue learning for days and weeks would provide a larger sample set of

40
00:04:44,840 --> 00:04:50,160
vocabulary in appropriate contexts, which would mainly correct misunderstandings in

41
00:04:50,160 --> 00:04:58,240
corner cases. But these corporal are not comparable by several orders of magnitude, to the gathered

42
00:04:58,240 --> 00:05:05,280
life experience of a human at age 25. The main limit of intelligence is corpus size

43
00:05:05,280 --> 00:05:12,440
in ML situation. Future artificial intelligences will be nothing like what AGI fans have been

44
00:05:12,440 --> 00:05:19,520
fearmongering about. These are 20th century reductionist AI ideas. The components are

45
00:05:19,520 --> 00:05:26,800
blind to the most fundamental basics of epistemology. Reductionist good old fashioned AI has been

46
00:05:26,800 --> 00:05:32,480
demonstrated to being inferior in their own domains to even semi-trivial machine learning

47
00:05:32,480 --> 00:05:42,200
methods. We need AGL, not AGI. Machines learning to code. As of this writing, there are a handful

48
00:05:42,200 --> 00:05:47,480
of available code writing systems based on ML technology that has learned from large

49
00:05:47,480 --> 00:05:55,360
quantities of open source code. For example GitHub Copilot, OpenAI Codex, and Amazon Code

50
00:05:55,360 --> 00:06:02,800
Whisperer. They have not yet surpassed human programmers. But it's not about writing code

51
00:06:02,800 --> 00:06:09,040
either. AI's writing code is about as silly as AI magazine covers with pictures of robots

52
00:06:09,040 --> 00:06:16,840
typing, wink wink. In the future, if we want the computer to do something, we will have

53
00:06:16,840 --> 00:06:23,800
a conversation, speaking and listening, with the computer. The conversation is at the level

54
00:06:23,800 --> 00:06:30,560
of discussing a problem with a competent coworker or professional. It may spontaneously ask

55
00:06:30,560 --> 00:06:39,080
clarifying questions. I call this, contiguously rolling topic, mixed initiative dialogue, others

56
00:06:39,080 --> 00:06:46,080
talk of these bots as dialogue Asians. But this will go beyond Siri or Alexa, and when

57
00:06:46,080 --> 00:06:53,080
the computer understands exactly what you want done. It just does it. Why would reductionist

58
00:06:53,080 --> 00:07:00,000
style programming be a necessary step? Yes, there will still be lots of places where we

59
00:07:00,000 --> 00:07:05,960
want to use code. But whether that code is written by humans or AI's will make much

60
00:07:05,960 --> 00:07:12,120
less of a difference than we might expect based on today's use of computers.

61
00:07:12,120 --> 00:07:21,160
The Pink Pill. The Wisdom Salon. Wisdom Salon is an online world cafe. The World Cafe protocol

62
00:07:21,160 --> 00:07:28,720
is a recipe for organizing conversations that matter on a large scale. Thousands of people

63
00:07:28,720 --> 00:07:36,040
can cooperate in order to bring clarity to complex issues. This is a post-mortem summary

64
00:07:36,040 --> 00:07:43,280
for my interrupted wisdom salon project. I have all the code in an archive, but it requires

65
00:07:43,280 --> 00:07:49,680
a complete rewrite in order to fix the two biggest problems. The switch from flash,

66
00:07:49,680 --> 00:07:57,400
hack, to HTML5 for video and the cost of video connections. I know how to fix these but I'm

67
00:07:57,400 --> 00:08:04,040
busy working on understanding machines. At the moment, I am looking for someone to take

68
00:08:04,040 --> 00:08:10,960
this over. I also observe that there is a need for something like this. I see things discussed

69
00:08:10,960 --> 00:08:17,360
on Quora that would make good topics for a wisdom salon. I happen to believe video in

70
00:08:17,360 --> 00:08:21,720
spoken words are an important component for many reasons.

71
00:08:21,720 --> 00:08:30,720
Wisdom. Knowledge and information can easily be found on the web. But what about wisdom?

72
00:08:30,720 --> 00:08:36,440
Intelligence is based on gathered knowledge. Wisdom is based on gathered experience. To

73
00:08:36,440 --> 00:08:46,280
get wiser, seek out more experiences. Engage yourself. Do more stuff. Travel. Talk to people

74
00:08:46,280 --> 00:08:53,440
to share their experiences. Conversation with others is the easiest way to gain wisdom.

75
00:08:53,440 --> 00:09:00,400
But not all conversations are equal. We want conversations that matter. The World Café

76
00:09:00,400 --> 00:09:07,360
Protocol. The World Café Protocol is a recipe for organizing such conversations that matter

77
00:09:07,360 --> 00:09:14,160
on a large scale. Thousands of people can cooperate in order to bring clarity to complex

78
00:09:14,160 --> 00:09:21,440
issues. To find out more, buy the book or study the World Café website. But this is

79
00:09:21,440 --> 00:09:28,320
how it typically works. In some conference facilities or gymnasium, the organizers provide

80
00:09:28,320 --> 00:09:35,800
dozens to hundreds of square tables. Each has four chairs, a box of crayons, and a piece

81
00:09:35,800 --> 00:09:42,600
of butcher paper as a tablecloth. Stakeholders from all walks of life get invited and sit

82
00:09:42,600 --> 00:09:50,240
down at the tables. This could be a mixture of farmers, teachers, politicians, in corporate

83
00:09:50,240 --> 00:09:57,800
environments. Sometimes this is everybody in the company. Organizers now unveil a carefully

84
00:09:57,800 --> 00:10:03,960
phrased focusing question as the topic of the conversations. It is important that the

85
00:10:03,960 --> 00:10:11,160
question is positive and focusing. For education reform, don't ask, what is wrong with our

86
00:10:11,160 --> 00:10:18,480
education system? Instead, ask, what could a great school also be? The four people at

87
00:10:18,480 --> 00:10:24,600
each table now start a conversation around the question. Everyone takes notes on the

88
00:10:24,600 --> 00:10:33,400
butcher paper, using the crayons. After 20 minutes, a gong rings. Three people. Everyone

89
00:10:33,400 --> 00:10:40,040
except south in duplicate bridge terms. At each table get up and move to other tables

90
00:10:40,040 --> 00:10:47,280
at random. Through fresh random people sit down at each table. South now first explains

91
00:10:47,280 --> 00:10:53,360
to the newcomers what the notes on the tablecloth mean. This provides a kind of lightweight

92
00:10:53,360 --> 00:10:59,480
continuity from the previous conversation at this table. The three newcomers comment

93
00:10:59,480 --> 00:11:05,160
on these notes and add fresh comments. The best parts of what was said at their previous

94
00:11:05,160 --> 00:11:12,920
tables. These conversations unfold very naturally. Four strangers can easily have a friendly

95
00:11:12,920 --> 00:11:19,880
conversation about complex things that matter. They don't even have to introduce themselves.

96
00:11:19,880 --> 00:11:27,680
They contribute their wisdom and experiences. Not their resumes. Conversations now continue

97
00:11:27,680 --> 00:11:35,440
for another 20 minutes. The gong rings again, and the shuffling repeats. After two to three

98
00:11:35,440 --> 00:11:40,880
hours, the session is over and the butcher papers are gathered by the organizers into

99
00:11:40,880 --> 00:11:48,920
what is called the harvest. They are summarized in some time later. Perhaps, after lunch,

100
00:11:48,920 --> 00:11:56,560
the results are shared with all the stakeholders. Why this works so well? Someone pushing a

101
00:11:56,560 --> 00:12:03,760
bad idea of theirs at every table can spam at worst 27 people in three hours. A good

102
00:12:03,760 --> 00:12:10,680
idea. Introduced at the first table and repeated by all participants at subsequent tables will

103
00:12:10,680 --> 00:12:17,840
reach over 100,000 people or the majority of the audience, whichever is smaller. This

104
00:12:17,840 --> 00:12:24,000
is the filtering power of the World Café protocol. Wisdom Salon is an online World

105
00:12:24,000 --> 00:12:31,400
Café. Sadly, the Wisdom Salon project has been suspended because of changing infrastructure

106
00:12:31,400 --> 00:12:38,440
and cost structure for online video transmissions, and because of lack of time on my part. It

107
00:12:38,440 --> 00:12:44,680
is possible to restart the project using current video technology and with funding and a larger

108
00:12:44,680 --> 00:12:52,280
team. If interested in contributing to this, please get in touch. What follows is the original

109
00:12:52,280 --> 00:13:00,280
high-level design specification, written in the present tense, design specification.

110
00:13:00,280 --> 00:13:08,840
The Wisdom Salon is a 24-7 online World Café implemented as a video chat site. Conversations

111
00:13:08,840 --> 00:13:14,640
have four participants, but each conversation can also have a passive and quiet audience

112
00:13:14,640 --> 00:13:22,800
of any size. All conversations are always public. All conversation participants are

113
00:13:22,800 --> 00:13:29,960
known by their login identities. Why would anyone want to participate? The main purpose

114
00:13:29,960 --> 00:13:36,640
of Wisdom Salon is increased wisdom and improved clarity and complex issues for the participants.

115
00:13:36,640 --> 00:13:43,360
This is your main benefit. This is why you would want to participate. You will not get

116
00:13:43,360 --> 00:13:49,920
lags, but you might earn a local currency, called, Influence, that you can selectively

117
00:13:49,920 --> 00:13:52,600
use to extend your influence.

118
00:13:52,600 --> 00:13:59,720
Goal. The goal is specifically not to find the best grains of wisdom in the harvest.

119
00:13:59,720 --> 00:14:05,920
The grains are there mainly to provide continuity and shorten the time to get to talking about

120
00:14:05,920 --> 00:14:12,200
things that matter. The system is there to provide the users a chance to analyze large

121
00:14:12,200 --> 00:14:20,720
and complex issues with others in conversation and in exchange of experiences. Do not underestimate

122
00:14:20,720 --> 00:14:27,520
how different an interactive conversation is from a web search or reading a book. Have

123
00:14:27,520 --> 00:14:34,040
you ever spent days studying something without getting it only to have someone set you straight

124
00:14:34,040 --> 00:14:40,280
in two minutes of conversation? Have you ever been in a meeting where the resolution is

125
00:14:40,280 --> 00:14:46,000
something none of the participants even understood when the meeting started?

126
00:14:46,000 --> 00:14:53,040
Sample questions. What kinds of questions demonstrate the power of the Wisdom Salon?

127
00:14:53,040 --> 00:15:01,080
Consider these samples. I am considering a midlife career change. What matters? Where

128
00:15:01,080 --> 00:15:09,560
should I retire, and why there? Should I pursue a career in engineering or medicine?

129
00:15:09,560 --> 00:15:16,400
Lifestyle design in interesting times. What is the true promise of genetics research and

130
00:15:16,400 --> 00:15:23,800
why should I care? What movies should I let my children watch, and why?

131
00:15:23,800 --> 00:15:32,720
Musical education for my child. What matters? What instruments, and why? What is it really

132
00:15:32,720 --> 00:15:41,640
like to be a soldier in places like Afghanistan and Iraq? Should I retire in Costa Rica? User

133
00:15:41,640 --> 00:15:49,000
experience. People arrive when they want and leave when they want. They can engage in multiple

134
00:15:49,000 --> 00:15:58,000
ways. Upon entering the site, users are presented with the, at the moment, most popular conversation,

135
00:15:58,000 --> 00:16:04,640
the one with the largest audience. Below the conversation, there will be a list of other

136
00:16:04,640 --> 00:16:11,800
popular conversations, headed by conversations and topics the user may have watched or previously

137
00:16:11,800 --> 00:16:18,800
participated in. They can browse all ongoing conversations much like watching talk shows

138
00:16:18,800 --> 00:16:25,640
on television. They can select from hundreds of questions to find something that interests

139
00:16:25,640 --> 00:16:33,000
them, or add their own. Instead of a butcher paper, they can leave notes on each question

140
00:16:33,000 --> 00:16:39,680
known as, grains of wisdom, to provide the lightweight continuity from table to table.

141
00:16:39,680 --> 00:16:47,120
They can vote on these grains of wisdom so that they better result rise to the top. Results

142
00:16:47,120 --> 00:16:54,080
are immediately visible to all. They can observe what other people say and how they behave

143
00:16:54,080 --> 00:16:59,880
and modify their own social graph to improve their chances of interaction with the best

144
00:16:59,880 --> 00:17:06,880
people. A local currency is earned by passive engagement per hour, more of it is earned

145
00:17:06,880 --> 00:17:12,600
by participating in conversations, and the currency is used to pay for the privilege

146
00:17:12,600 --> 00:17:19,400
of posting a comment, because posting cost currency, spelling the grains of wisdom will

147
00:17:19,400 --> 00:17:26,400
be limited. A topic without currently active conversations still allows you to browse the

148
00:17:26,400 --> 00:17:33,800
grains of wisdom on the topic, and if you have influence, you can vote on the grains or notes

149
00:17:33,800 --> 00:17:40,000
that you like or otherwise agree with, and you can restart the topic by creating a table

150
00:17:40,000 --> 00:17:47,640
and hope others will join. Four main uses of wisdom salon. The site enables, but doesn't

151
00:17:47,640 --> 00:17:55,200
enforce the World Cafe protocol. You can use the site for several different purposes.

152
00:17:55,200 --> 00:18:01,520
As entertainment and education, passively watching conversations among your peers,

153
00:18:01,520 --> 00:18:08,080
much like flipping channels on television. To get both factual information and broad

154
00:18:08,080 --> 00:18:16,080
ranging personalized advice from experts. To share your expertise in fields you understand.

155
00:18:16,080 --> 00:18:24,080
To do micromantering. To find an audience for storytelling and sharing personal experiences

156
00:18:24,080 --> 00:18:32,640
from your life. To gain wisdom and personal clarity in complex issues. To debate the major

157
00:18:32,640 --> 00:18:39,680
issues of the day in person and productively selected and well behaved groups. To find

158
00:18:39,680 --> 00:18:45,600
new interesting and competent friends by observing their behavior and then befriending them,

159
00:18:45,600 --> 00:18:54,320
much like other social media. Any active conversation starts a 20 minute clock bar moving. You

160
00:18:54,320 --> 00:19:01,560
can leave anytime. System provides some incentive to stay the full 20 minutes. On the other

161
00:19:01,560 --> 00:19:09,000
hand, you don't have to leave after 20 minutes. If you like, you can continue conversation

162
00:19:09,000 --> 00:19:16,920
along as you want. But we expect a large fraction of people to adhere to the protocol. We believe

163
00:19:16,920 --> 00:19:24,240
this maximizes the wisdom gain per session. Without the right people, the system is worthless.

164
00:19:24,240 --> 00:19:32,000
Do not be discouraged. Facebook would be worthless with only 10 people on it. Wisdom salon really

165
00:19:32,000 --> 00:19:38,480
requires at least 50 people to be on the system before you are likely to find a conversation

166
00:19:38,480 --> 00:19:44,960
around a question you actually care about anytime you join. So nobody knows if this

167
00:19:44,960 --> 00:19:50,600
will work or not, and it may take a while before the system matures enough to attract

168
00:19:50,600 --> 00:19:56,400
a sufficient repeat audience to become what I designed it for. If you don't like it

169
00:19:56,400 --> 00:20:03,000
at first, please try again. It might well improve, and you might get lucky to get into

170
00:20:03,000 --> 00:20:10,360
an amazing conversation when you least expect it. Welcome to my experiment.

171
00:20:10,360 --> 00:20:19,360
The lavender pill. Model free AI. Don't model the world. Just model the mind. It's a lot

172
00:20:19,360 --> 00:20:29,120
easier. With some poetic freedom, I'd like to claim 1. Model the world. 10 billion lines

173
00:20:29,120 --> 00:20:40,960
of code. 2. Model the brain. 10 million lines of code. 3. Model the mind. 10,000 lines of

174
00:20:40,960 --> 00:20:48,640
code. Number one is regular programming. We make computers perform actions in a context

175
00:20:48,640 --> 00:20:54,480
that matches the programmer's mental model of some relevant parts of the world. Number

176
00:20:54,480 --> 00:21:01,160
two is neuroscience-based models of neurons, synapses and other biological structures and

177
00:21:01,160 --> 00:21:09,920
systems in brains. The number three is epistemology-based models of learning, understanding, reasoning,

178
00:21:09,920 --> 00:21:17,160
prediction, abstraction, and other holistic and emergent phenomena. Epistemology-based

179
00:21:17,160 --> 00:21:23,440
methods require a rather minimal infrastructure to support whatever operations these concepts

180
00:21:23,440 --> 00:21:30,680
require. I put models within irony quotes because they are strictly speaking metamodels

181
00:21:30,680 --> 00:21:37,360
because they are used in metascales. They are not about skills, such as English or folding

182
00:21:37,360 --> 00:21:44,280
proteins. They are about how to acquire such skills by learning from our mistakes.

183
00:21:44,280 --> 00:21:52,840
The purple pill. Corpus congruence. Understanding in brains and machines can be defined and

184
00:21:52,840 --> 00:22:01,960
measured as corpus congruence. Corpus congruence as a metric spans up almost all of NLP. Understanding

185
00:22:01,960 --> 00:22:08,440
in brains and machines can be defined and measured as corpus congruence. Let's consider

186
00:22:08,440 --> 00:22:16,040
this in the machine learning sense. If a machine is model-free, holistic, as all general understanders

187
00:22:16,040 --> 00:22:21,960
have to be in order to not get trapped into a limited model, then all it ever knows comes

188
00:22:21,960 --> 00:22:28,080
from the corpus it was trained on. And all it really can say is, this is more like my

189
00:22:28,080 --> 00:22:35,480
corpus than that. Or, this is more like these documents in my corpus than those corpus congruence

190
00:22:35,480 --> 00:22:44,160
as a metric spans up almost all of NLP. Because most of NLP is doxen in various guises. Given

191
00:22:44,160 --> 00:22:50,880
two documents A and B in some corpus, a classifier can say that an unknown document, which we

192
00:22:50,880 --> 00:22:59,360
can call U, is more like it than B given this capability we can build. Classification and

193
00:22:59,360 --> 00:23:09,640
clustering by using A, B, up to N as defining classes. Filtering by using A, wanted dox

194
00:23:09,640 --> 00:23:21,440
and B, unwanted dox. Summoned analysis by using A, negative dox and B, positive dox.

195
00:23:21,440 --> 00:23:29,440
Entity extraction by softly matching termed against lists of known entities. Doxen, find

196
00:23:29,440 --> 00:23:37,080
me more documents like this one. Reductionist and NLP uses all of these at the bag of words

197
00:23:37,080 --> 00:23:45,320
or word count levels for things like web search, span filtering, and clustering. Holistic

198
00:23:45,320 --> 00:23:52,600
NLU aims to do the same based on the meanings expressed in sentences and paragraphs. But

199
00:23:52,600 --> 00:23:59,720
semantic corpus congruence is still corpus congruence. Common sense now becomes, is the

200
00:23:59,720 --> 00:24:06,000
proposition before me congruent with my entire world model, as required by learning things

201
00:24:06,000 --> 00:24:13,440
from my training corpus. If it is well known, then we can likely ignore it this time, and

202
00:24:13,440 --> 00:24:18,960
if it is not, then the next question will be, is it close enough that it might be worth

203
00:24:18,960 --> 00:24:25,320
while extending the world model with this information? If the answer is no, then the

204
00:24:25,320 --> 00:24:32,160
input is by its definition nonsense. Otherwise it is either a new fact or a lie, but since

205
00:24:32,160 --> 00:24:39,760
we cannot tell, we have to accept it, possibly with a note that this is fresh, untested knowledge

206
00:24:39,760 --> 00:24:47,360
that may turn out to be irrelevant, false, counterproductive, or noise. Next we can note

207
00:24:47,360 --> 00:24:54,040
that it doesn't matter whether documents are text or images, or input from a point cloud

208
00:24:54,040 --> 00:25:00,280
of sensors for robots or autonomous vehicle sensors. And finally we can note that this

209
00:25:00,280 --> 00:25:07,640
definition also holds for humans if we take our corpus to be everything we've experienced

210
00:25:07,640 --> 00:25:09,640
since birth.

211
00:25:09,640 --> 00:25:12,640
Monika's Little Pills

212
00:25:12,640 --> 00:25:14,160
Chapter 1

213
00:25:14,160 --> 00:25:17,760
Why I Works

214
00:25:17,760 --> 00:25:24,760
Intelligence equals understanding plus reasoning. Interest in artificial intelligence is exploding,

215
00:25:24,760 --> 00:25:31,760
and for good reasons, computers and cars, phone apps, and on the web can do amazing

216
00:25:31,760 --> 00:25:39,280
things that we simply could not do before 2012. What's going on? This is an attempt

217
00:25:39,280 --> 00:25:45,840
to explain the current state of AI to a general audience without using mathematics, computer

218
00:25:45,840 --> 00:25:53,480
science, or neuroscience, discussions at these levels with focus on how AI works. Here I

219
00:25:53,480 --> 00:26:01,040
will discuss this at the level of epistemology and will try to explain why it works. Epistemology

220
00:26:01,040 --> 00:26:08,440
sounds scary, but it really isn't. It's mostly scary because it is unknown, it is not taught

221
00:26:08,440 --> 00:26:15,840
in schools anymore, which is a problem, because we now desperately need this branch of philosophy

222
00:26:15,840 --> 00:26:24,200
to guide our AI development. Epistemology discusses things like reasoning, understanding, learning,

223
00:26:24,200 --> 00:26:31,960
novelty, problem solving in the abstract, how to create models of the world, etc. These

224
00:26:31,960 --> 00:26:38,080
are all concepts one would think would be useful when working with artificial intelligences,

225
00:26:38,080 --> 00:26:44,600
but most practitioners enter the field of AI without any exposure to epistemology which

226
00:26:44,600 --> 00:26:51,320
makes their work more mysterious and frustrating than it has to be. I think of it epistemology

227
00:26:51,320 --> 00:26:57,640
as the general base for everything related to knowledge and problem solving. Science forms

228
00:26:57,640 --> 00:27:03,320
a small special case subset domain where we solve well-formed problems of the kind that

229
00:27:03,320 --> 00:27:10,040
science is best at. In the epistemology outside of science we are free to productively also

230
00:27:10,040 --> 00:27:16,400
discuss pre-scientific problem solving strategies, which is what brains are using most of the

231
00:27:16,400 --> 00:27:24,840
time. More later, intelligence equals understanding plus reasoning. In his book, Thinking Fast

232
00:27:24,840 --> 00:27:31,800
and Slow, Daniel Kahneman discusses the idea that human minds use two different and complementary

233
00:27:31,800 --> 00:27:38,440
processes, two different modes of thinking, which we call understanding and reasoning.

234
00:27:38,440 --> 00:27:44,600
The idea has been discussed for decades and has been verified using psychological studies

235
00:27:44,600 --> 00:27:52,200
and by neuroscience. Subconscious intuitive understanding is the full name of the fast

236
00:27:52,200 --> 00:27:59,280
thinking or system one thinking. It is fast because the brain can perform many parts of

237
00:27:59,280 --> 00:28:06,240
this task in parallel. The brain spends a lot of effort on this task. Conscious logical

238
00:28:06,240 --> 00:28:13,840
reasoning is the full name of slow thinking or system two thinking. To many people's

239
00:28:13,840 --> 00:28:21,440
surprise, this is very rarely used in practice. By soundbite for this is, you can make breakfast

240
00:28:21,440 --> 00:28:27,840
without reasoning. Almost everything we do on a daily basis in our rich mundane reality

241
00:28:27,840 --> 00:28:33,800
is done without a need to reason about it. We just repeat whatever worked last time we

242
00:28:33,800 --> 00:28:41,960
performed this task. Real experience driven. Intuitive means that the system can very quickly

243
00:28:41,960 --> 00:28:48,400
provide solutions to very complex problems but those solutions may not be correct every

244
00:28:48,400 --> 00:28:54,960
time. Logical means that answers are always correct as long as input data is correct and

245
00:28:54,960 --> 00:29:02,240
sufficient, which is not true in our rich mundane reality. It can only be true in a mathematically

246
00:29:02,240 --> 00:29:11,120
pure model space. If you like logic, you must also like models. Subconscious means we have

247
00:29:11,120 --> 00:29:18,020
no conscious, introspective access to these processes. You are reading this sentence

248
00:29:18,020 --> 00:29:24,320
and you understand it fully but you cannot explain to anyone, including yourself, how

249
00:29:24,320 --> 00:29:30,840
or why you understand it. Conscious means we are aware of the thought, we can access

250
00:29:30,840 --> 00:29:37,960
it through introspection and we may find reasons to why we believe a certain idea. Expensive

251
00:29:37,960 --> 00:29:44,160
is on the list because brains spend most of their effort on this understanding part. We

252
00:29:44,160 --> 00:29:51,800
really shouldn't be surprised that AI now requires very powerful computers. More later.

253
00:29:51,800 --> 00:29:58,240
In contrast, reasoning is efficient. It is most useful when you are stuck in a novel

254
00:29:58,240 --> 00:30:05,480
situation or experience and understanding doesn't help you. Or perhaps you need to plan ahead

255
00:30:05,480 --> 00:30:12,600
or need to find reasons for why something happened after the fact. It is used at a formal level

256
00:30:12,600 --> 00:30:20,960
in the sciences. Reasoning is important but just rarely needed or used. Finally, understanding

257
00:30:20,960 --> 00:30:28,480
is model-free and reasoning is model-based. This is likely the most important distinction

258
00:30:28,480 --> 00:30:33,880
to people who are implementing intelligent systems since it provides a way to keep the

259
00:30:33,880 --> 00:30:40,080
implementation on the correct path when the going gets rough. We cannot discuss these

260
00:30:40,080 --> 00:30:47,440
issues quite yet but if you are curious you can watch the videos at Vimeo.com which discuss

261
00:30:47,440 --> 00:30:53,920
this distinction at length. Think of the appearance in this table as a kind of foreshadowing.

262
00:30:53,920 --> 00:31:00,680
All of this groundwork allows me to state the main point of this section. We have known

263
00:31:00,680 --> 00:31:07,880
for a long time that brains use these two modes. But the AI research community has been spending

264
00:31:07,880 --> 00:31:13,560
over much effort on the reasoning part and has been ignoring the understanding part for

265
00:31:13,560 --> 00:31:21,400
60 years. We had several good reasons for this. Until quite recently, our machines were too

266
00:31:21,400 --> 00:31:28,320
small to run any useful sized neural network. And also, we didn't have a clue about how

267
00:31:28,320 --> 00:31:35,440
to implement this understanding. But that is exactly what changed in 2012 when a group

268
00:31:35,440 --> 00:31:40,960
of AI researchers from Toronto effectively demonstrated that deep neural networks could

269
00:31:40,960 --> 00:31:47,320
provide a simple kind of shallow and hollow proto-understanding. Well, they didn't call

270
00:31:47,320 --> 00:31:54,480
it that, but I do. I will look just a little into the future and overstate this just a

271
00:31:54,480 --> 00:32:02,440
little in order to make it more memorable. Deep neural networks can provide understanding.

272
00:32:02,440 --> 00:32:08,480
This new phase of AI took decades to develop, but it would never have happened without people

273
00:32:08,480 --> 00:32:15,600
like the group led by Jeffrey Hinton at the University of Toronto, who spent 34 plus years

274
00:32:15,600 --> 00:32:22,800
to develop the deep neural network technology we now call, deep learning. A number of breakthroughs

275
00:32:22,800 --> 00:32:31,160
from 1997 to 2006 led to a number of successful demonstrations, including first prizes in

276
00:32:31,160 --> 00:32:38,160
AI competitions in 2012. And we therefore count this year as the birth year of machine

277
00:32:38,160 --> 00:32:45,160
understanding. To an outsider, it may look like an older program or phone app might be

278
00:32:45,160 --> 00:32:51,200
understanding whatever the app is doing, but that understanding really only happened in

279
00:32:51,200 --> 00:32:57,480
the mind of the programmer creating the app. The programmer first simplified the problem

280
00:32:57,480 --> 00:33:04,640
in their own head by discarding a lot of irrelevant detail using programmer's understanding.

281
00:33:04,640 --> 00:33:10,640
The simplified mental model of the problem domain could then be explained to a computer

282
00:33:10,640 --> 00:33:16,720
in the form of a computer program. What is changing is that computers are now making

283
00:33:16,720 --> 00:33:24,520
these models themselves. The first bullet point describes regular programming, including

284
00:33:24,520 --> 00:33:34,120
old style AI programs. AI has, since 1955, provided many novel and brilliant algorithms

285
00:33:34,120 --> 00:33:40,800
that we now use in programs everywhere. But when you contrast old style AI to understanding

286
00:33:40,800 --> 00:33:47,960
systems, the old kind of AI is basically indistinguishable from any other kind of programming we do

287
00:33:47,960 --> 00:33:54,960
nowadays. The second bullet point describes the recent developments. Deep neural networks

288
00:33:54,960 --> 00:34:00,160
are so different from regular programs that we have to acknowledge them as a different

289
00:34:00,160 --> 00:34:07,160
computational paradigm. This is why they took almost four decades to develop. And the

290
00:34:07,160 --> 00:34:14,440
paradigm, being pre-scientific and model-free, is difficult to grasp if you receive a solid

291
00:34:14,440 --> 00:34:21,440
reductionist and model-based education. It takes a long time for an established AI practitioners

292
00:34:21,440 --> 00:34:27,840
or experienced programmer to switch. People who are just starting out in AI have an easier

293
00:34:27,840 --> 00:34:33,000
time assimilating this new paradigm since they haven't had a full career's worth of

294
00:34:33,000 --> 00:34:39,320
experience and success using old style AI techniques. The amount of work we have to

295
00:34:39,320 --> 00:34:45,720
do to get a deep neural network to understand is surprisingly small, and companies like

296
00:34:45,720 --> 00:34:51,000
Google and Cintiens are working on eliminating the remaining effort of programming neural

297
00:34:51,000 --> 00:34:58,680
networks. This is where things will get really weird. When the deep neural network, DNN,

298
00:34:58,680 --> 00:35:04,040
understands enough about the world and about the problem it is faced with, then we no longer

299
00:35:04,040 --> 00:35:11,560
need a programmer to acquire this understanding. Let me elaborate. Programmers are employed

300
00:35:11,560 --> 00:35:17,760
to bridge two different domains. They first have to study whatever application domain

301
00:35:17,760 --> 00:35:23,720
they are working on. For instance, if they are writing an airline ticket reservation

302
00:35:23,720 --> 00:35:30,520
system they will have to learn a lot of detailed information about airlines, airline tickets,

303
00:35:30,520 --> 00:35:38,400
flights, luggage, etc. and then know to provide features for unusual cases such as cancelled

304
00:35:38,400 --> 00:35:44,720
flights. And then the programmer uses their understanding of the problem domain to explain

305
00:35:44,720 --> 00:35:50,440
to a computer how it can reason about these things, but the programmer cannot make the

306
00:35:50,440 --> 00:35:56,640
system understand, it can only put in the hollow and fragile kind of reasoning, as a

307
00:35:56,640 --> 00:36:04,080
program with many of thin cases, and any misunderstandings the programmer has about the problem domain

308
00:36:04,080 --> 00:36:12,040
will become bugs in the computer program. Notice the shift in terminology. More later.

309
00:36:12,040 --> 00:36:20,040
But today, for certain classes of moderately complex problems, we can use a DNN to automatically

310
00:36:20,040 --> 00:36:26,760
learn for itself how to understand the problem, which means we no longer need a programmer

311
00:36:26,760 --> 00:36:33,520
to understand the problem. We have delegated our understanding to a machine, and if you

312
00:36:33,520 --> 00:36:39,880
think about that for a minute you will see that that's exactly what an AI should be doing.

313
00:36:39,880 --> 00:36:46,000
It should understand all kinds of things, so that we humans won't have to. And there

314
00:36:46,000 --> 00:36:52,000
are two common situations where this will be a really good idea. One is when we have

315
00:36:52,000 --> 00:36:58,960
a problem we cannot understand ourselves. We know a lot of those, starting with cellular

316
00:36:58,960 --> 00:37:05,440
biology. The other common case will be when we understand the problem well, but making

317
00:37:05,440 --> 00:37:10,920
the machine understand it well enough to get the job done is cheaper and easier than any

318
00:37:10,920 --> 00:37:17,800
alternative. MoonBoss accomplish this level of using old style AI methods, but I predict

319
00:37:17,800 --> 00:37:23,840
we will one day be flooded with similar, but DNN based devices that understand several

320
00:37:23,840 --> 00:37:31,400
aspects of domestic maintenance, as well as we do. Do machines really understand? If we

321
00:37:31,400 --> 00:37:37,840
give a picture like this to a DNN trained on images it will identify the important objects

322
00:37:37,840 --> 00:37:44,680
in the image and provide the rectangles, called, owning boxes as approximations to where the

323
00:37:44,680 --> 00:37:51,320
objects are. The text on the right says, woman in white dress standing with tennis racket

324
00:37:51,320 --> 00:37:58,280
to people in green behind her, which is not a bad description of the image. It could be

325
00:37:58,280 --> 00:38:05,160
used as the basis for a test for English skill level for adult education placement, for all

326
00:38:05,160 --> 00:38:12,480
practical purposes. This is understanding. We had no idea how to make our computers do

327
00:38:12,480 --> 00:38:21,400
this before 2012. This is a really big deal. This feat requires not only a new algorithm,

328
00:38:21,400 --> 00:38:28,840
it requires a new computational paradigm and images to a computer, a single long sequence

329
00:38:28,840 --> 00:38:37,560
of numbers denoting values for red, blue and green colors and values from 0 to 255. It

330
00:38:37,560 --> 00:38:44,480
also knows how wide the image is. How does it get from this very low level representation

331
00:38:44,480 --> 00:38:49,800
to knowing that there is a woman with a tennis racket in the image? This is what William

332
00:38:49,800 --> 00:38:56,240
Calvin has called, a river that flows uphill. There are very few mechanisms that can go

333
00:38:56,240 --> 00:39:03,680
in this direction, from low levels to high levels. Calvin used the term to describe evolution,

334
00:39:03,680 --> 00:39:11,120
and I can use this quote to describe understanding. I like to think of evolution as, nature's

335
00:39:11,120 --> 00:39:17,880
understanding because the phenomena are very similar at several levels. Evolution of species

336
00:39:17,880 --> 00:39:23,480
can bring forth advanced species starting from simpler species in the same manner that

337
00:39:23,480 --> 00:39:29,480
understanding is the discovery and reuse of high level concepts and low level input.

338
00:39:29,480 --> 00:39:36,640
In contrast, reasoning proceeds by breaking problems into sub-problems and solving those,

339
00:39:36,640 --> 00:39:44,600
which is a, flowing downhill, kind of strategy. In mathematics we accept, and many mathematicians

340
00:39:44,600 --> 00:39:51,040
only accept this reluctantly, that we need to use induction to move uphill in abstractions,

341
00:39:51,040 --> 00:39:58,240
and that's a very limited uphill movement at that. Epistemology allows for much stronger

342
00:39:58,240 --> 00:40:05,000
uphill moves. This is known as, jumping to conclusions on scant evidence and it's allowed

343
00:40:05,000 --> 00:40:11,800
in epistemology based pre-scientific systems. As an aside, here's a pretty deep related

344
00:40:11,800 --> 00:40:19,400
thought. In nature, evolution reuses anything that works. I like to think that understanding

345
00:40:19,400 --> 00:40:26,560
is a spandrel of evolution itself. Neural Darwinism certainly straddles this gap. Could

346
00:40:26,560 --> 00:40:33,880
be coincidence, or the only answer that will work at all. More later, we doubled our AI

347
00:40:33,880 --> 00:40:41,080
toolkit in 2012. We can now use these deep neural networks as components in our systems

348
00:40:41,080 --> 00:40:47,160
to provide understanding of certain things like vision, speech, and other problems that

349
00:40:47,160 --> 00:40:54,240
require that we discover high level concepts and low level data. The technical, epistemology

350
00:40:54,240 --> 00:41:00,840
level name for this uphill flow in processes, reduction, and we'll be using that term later

351
00:41:00,840 --> 00:41:06,360
after we explain what it means. Let's look at what the industry is doing with their new

352
00:41:06,360 --> 00:41:14,080
found toys. This is my view of what I think Tesla is doing, based on public sources in

353
00:41:14,080 --> 00:41:21,040
their self-driving, autopilot, cars, cameras feed vision understanding components based

354
00:41:21,040 --> 00:41:27,480
on deep learning, and radar feeds to radar understanding components. These supply bounding

355
00:41:27,480 --> 00:41:33,880
boxes in 2D or 3D with additional information like, there's a woman with a tennis racket

356
00:41:33,880 --> 00:41:39,800
ahead to a traffic reasoning component that uses regular programming, or some old style

357
00:41:39,800 --> 00:41:46,640
AI like a rule based system to actually control the car based on the vision and radar inputs,

358
00:41:46,640 --> 00:41:54,120
and the driver's desires. But this is not the only possible configuration. George Hopps

359
00:41:54,120 --> 00:42:01,480
at Comma.ai, a team at NVIDIA Corporation, and the deep Tesla class at MIT are using

360
00:42:01,480 --> 00:42:06,840
a simpler architecture with just a neural network that implements lane following and

361
00:42:06,840 --> 00:42:12,840
other simple driving behaviors directly in one single deep neural network. There's room

362
00:42:12,840 --> 00:42:20,360
for improvement, but there a big step in the direction we want to move in. Future automotive

363
00:42:20,360 --> 00:42:26,400
systems will likely integrate everything about driving into one single neural network, or

364
00:42:26,400 --> 00:42:33,240
something that effectively behaves as one. Vision, traffic, the car itself including

365
00:42:33,240 --> 00:42:40,080
various functionality like windscreen wipers, lights, and entertainment, how to drive in

366
00:42:40,080 --> 00:42:47,720
a safe and polite manner, and to understand also the drivers or car owners desires. And

367
00:42:47,720 --> 00:42:53,760
if we've gotten that far, then it is a given that we will have speech input and output

368
00:42:53,760 --> 00:42:59,320
so that the driver can have a conversation with the car while driving, and can just

369
00:42:59,320 --> 00:43:05,920
advise it in case it does something wrong. We are nowhere close to this today. But after

370
00:43:05,920 --> 00:43:13,240
a DNM breakthrough or two, who knows how quickly these kinds of systems become available. We

371
00:43:13,240 --> 00:43:20,520
can already see an increasing stream of new features built using understanding components.

372
00:43:20,520 --> 00:43:27,520
This article, and the next, are expansions of a talk given on June 10, 2017 at the San

373
00:43:27,520 --> 00:43:35,280
Francisco Bill Conference. A decade ago I created artificialintuition.com. I now have

374
00:43:35,280 --> 00:43:42,120
a lot more to say, but I need to split this meme package into digestible chunks. This

375
00:43:42,120 --> 00:43:48,120
takes a lot of effort to get right. If you liked this article and would like to see more

376
00:43:48,120 --> 00:43:55,120
like it then you can support my writing and my research in many ways, small to large,

377
00:43:55,120 --> 00:44:00,760
like and share these ideas with someone who might want to invest in sentience incorporated

378
00:44:00,760 --> 00:44:06,200
or might be otherwise interested in my research on a novel language understanding technology

379
00:44:06,200 --> 00:44:13,720
called organic learning. More on that later. I do not receive external funding from any

380
00:44:13,720 --> 00:44:20,220
investors for this research. You can support my research and writing directly at the donation

381
00:44:20,220 --> 00:44:29,400
section at artificialintuition.com. Chapter 2. Our Greatest Invention, Model Based Problem

382
00:44:29,400 --> 00:44:37,560
Solving. The first chapter, why AI works, provided the big picture of AI and understanding

383
00:44:37,560 --> 00:44:44,640
machines. Next we will focus on how to actually implement understanding in a computer. But

384
00:44:44,640 --> 00:44:50,600
before we can attack that core issue, we need to simplify the journey a bit by defining

385
00:44:50,600 --> 00:44:58,120
four important words and concepts. I'll define one in this section, two in the next, and

386
00:44:58,120 --> 00:45:04,400
the concept of reduction after that. We can then discuss the epistemology level algorithm

387
00:45:04,400 --> 00:45:11,040
for understanding itself. If you are already familiar with these concepts, just check the

388
00:45:11,040 --> 00:45:17,120
headings and definitions that follow in order to ensure we are using these words roughly

389
00:45:17,120 --> 00:45:24,400
the way you use them. You may have noticed I write certain, sometimes common words,

390
00:45:24,400 --> 00:45:31,600
such as model, with an uppercase first letter. This means I am using the word in a technical,

391
00:45:31,600 --> 00:45:38,920
well-defined, unchanging sense. I will define all such technical terms over time and I will

392
00:45:38,920 --> 00:45:46,040
try not to use these terms until I have defined them. We define 11 such terms in the first

393
00:45:46,040 --> 00:45:54,560
chapter, starting with understanding and reasoning. A dictionary of defined terms is in the works.

394
00:45:54,560 --> 00:46:01,000
Models are simplifications of reality in epistemology and science. Models are simplifications

395
00:46:01,000 --> 00:46:08,640
of reality. A rich mundane reality is too complex to land itself directly to computation.

396
00:46:08,640 --> 00:46:15,920
In OTB science fiction shows, we would sometimes hear. And then we fed all the information

397
00:46:15,920 --> 00:46:23,400
into the computer and this is what came out. Well, not anymore. Audiences now know that's

398
00:46:23,400 --> 00:46:32,200
not how regular computers work. Consider an automobile. It consists of thousands of parts,

399
00:46:32,200 --> 00:46:39,800
each with properties like materials, size, color, function, and sometimes complex interactions

400
00:46:39,800 --> 00:46:47,360
with other parts. What's all the information here? We can just feed all of those properties

401
00:46:47,360 --> 00:46:54,240
and measurements and facts into a computer and expect to get an answer. We need to ask

402
00:46:54,240 --> 00:47:01,200
a question and we also need to simplify the problem so that we can feed in just the facts

403
00:47:01,200 --> 00:47:07,720
or numbers that matter so that our question can be answered with minimum effort. How do

404
00:47:07,720 --> 00:47:15,500
we do that? We must identify or create, first in our minds, a very simple model of some

405
00:47:15,500 --> 00:47:22,240
sort of a generic automobile, and use that model for our computation. After we get the

406
00:47:22,240 --> 00:47:29,160
answer for the pure and simple model case, we apply the answer, with some care, back

407
00:47:29,160 --> 00:47:36,400
to our complex reality where the real automobile and the problem exists. What kind of model

408
00:47:36,400 --> 00:47:43,520
we choose depends on our goals. As an example of a model, Newton's second law states that

409
00:47:43,520 --> 00:47:51,560
force equals mass times acceleration, f equals ma. This equation is a classical scientific

410
00:47:51,560 --> 00:47:58,400
model. If we measure mass and acceleration of a car, then we can estimate how many horsepower

411
00:47:58,400 --> 00:48:06,360
the engine has. To use this equation, we engineers would model, in our minds, the car as a single

412
00:48:06,360 --> 00:48:13,200
small point mass with all the mass of the car in that point. Because if we don't, then

413
00:48:13,200 --> 00:48:19,440
we'd have to worry about the car rotating and other problems. This is how model-based

414
00:48:19,440 --> 00:48:27,080
science works. One or more scientists somehow derive a model for some phenomenon. The model

415
00:48:27,080 --> 00:48:34,600
is published as an equation, a formula, or a computer program. Scientists and engineers

416
00:48:34,600 --> 00:48:40,600
anywhere can now use this equation program model, treating it as a quick shortcut that

417
00:48:40,600 --> 00:48:46,720
works every time, as long as they have correct input data and are confidently applying the

418
00:48:46,720 --> 00:48:54,280
formula to a suitable problem in their reality. Our greatest invention, model-based problem

419
00:48:54,280 --> 00:49:01,200
solving, aka reductionism, is the greatest invention our species has ever made. The

420
00:49:01,200 --> 00:49:07,080
general strategy of simplifying problems before solving them must be tens of thousands of

421
00:49:07,080 --> 00:49:14,280
years old. In some sense, it is a prerequisite for all other inventions, including the use

422
00:49:14,280 --> 00:49:21,520
of fire. If you see a forest fire then you need to first imagine the utility of fire.

423
00:49:21,520 --> 00:49:27,320
As a model, before you can figure out that it might be useful to carry home a burning

424
00:49:27,320 --> 00:49:33,840
branch, we don't think of this problem solving strategy as an invention because it is already

425
00:49:33,840 --> 00:49:40,080
ubiquitous in our lives. We are all taught how to use model-based problem solving in

426
00:49:40,080 --> 00:49:46,320
school when we start solving story problems in math class, but most people never learn

427
00:49:46,320 --> 00:49:52,640
the names of these strategies and are missing the big epistemology level picture. This rarely

428
00:49:52,640 --> 00:49:59,440
matters until you start working with AI, where lack of an epistemological drowning may lead

429
00:49:59,440 --> 00:50:06,360
you astray into failing strategies. These little pills are an attempt to remedy that.

430
00:50:06,360 --> 00:50:13,360
Model-based methods were examined and refined into scientific methods over the past 450

431
00:50:13,360 --> 00:50:19,920
years. Science is now a collection of thousands of models that taken together allow science

432
00:50:19,920 --> 00:50:25,840
competent people to solve problems quickly and efficiently without having to redo all

433
00:50:25,840 --> 00:50:31,840
the work that scientists, like Newton, put into creating these models in the first place.

434
00:50:31,840 --> 00:50:38,640
And the sum total of those models covers many problems we want to solve scientifically,

435
00:50:38,640 --> 00:50:44,840
such as how to build a bridge or travel to the moon. This reuse is what makes science

436
00:50:44,840 --> 00:50:52,040
so effective, but not all sciences can benefit equally from this model-making. It works well

437
00:50:52,040 --> 00:51:00,440
for physics, chemistry, and most of biochemistry. As I'm fond of saying, physics is for simple

438
00:51:00,440 --> 00:51:06,960
problems, but as you get to more and more complex sciences, as you get further away

439
00:51:06,960 --> 00:51:14,320
from physics and closer to life, it gets harder to make decent models. The models used by

440
00:51:14,320 --> 00:51:21,560
for instance psychology, ecology, physiology, and medicine are generally more complex but

441
00:51:21,560 --> 00:51:28,960
also less powerful than models in physics. Given some solid data, a physicist can compute

442
00:51:28,960 --> 00:51:34,920
the mass of the proton to six decimal places, but we would have a harder time predicting

443
00:51:34,920 --> 00:51:40,480
the number of muskrats in New England next summer because that outcome depends on millions

444
00:51:40,480 --> 00:51:47,400
of parameters. The life sciences base many of their models on statistics. Statistical

445
00:51:47,400 --> 00:51:53,680
models are among the weakest models used in science. These statistical models when more

446
00:51:53,680 --> 00:52:00,880
powerful models with better predictive capabilities cannot be used for complexity reasons. Models

447
00:52:00,880 --> 00:52:11,960
are apothesis, unverified models, scientific theories, models verified by peer review,

448
00:52:11,960 --> 00:52:24,600
equations, formulas, complex scientific models, simulations of climate, weather, etc. Naive

449
00:52:24,600 --> 00:52:33,400
models that we create to simplify our own lives. Computer programs, and what is mathematics?

450
00:52:33,400 --> 00:52:40,720
It is a system that allows us to manipulate our models to cover more cases. Mathematics

451
00:52:40,720 --> 00:52:48,720
is the purest, most context free of all scientific disciplines. As such, its greatest value to

452
00:52:48,720 --> 00:52:55,080
humanity is in its role as a help discipline to all other disciplines. Einstein's famous

453
00:52:55,080 --> 00:53:01,440
equals MC squared model was derived using mathematical manipulation of other models

454
00:53:01,440 --> 00:53:08,240
known to Einstein at the time. But perhaps mathematics isn't as much a scientific discipline

455
00:53:08,240 --> 00:53:16,240
as an epistemological one. I may explore this aside later. Model use requires understanding.

456
00:53:16,240 --> 00:53:24,000
A good model is context free, since it maximizes the number of contexts it can be applied in.

457
00:53:24,000 --> 00:53:32,240
Newton's second law, F equals MA, works pretty much everywhere. We have forces, masses, and

458
00:53:32,240 --> 00:53:39,200
accelerations. The trade-off for this flexibility is that we ourselves need to understand the

459
00:53:39,200 --> 00:53:47,840
problem domain. In rocket science, when maneuvering in space, F equals MA will often work perfectly,

460
00:53:47,840 --> 00:53:53,680
but when you are applying it to the acceleration of your car, you need to account for lots of

461
00:53:53,680 --> 00:54:00,320
effects like friction between the road and the wheels, wind resistance, and the like.

462
00:54:00,320 --> 00:54:08,080
So, F equals MA, applied naively would give you the wrong answer if friction is involved.

463
00:54:08,080 --> 00:54:14,960
This demonstrates the main disadvantage with models. They require that both the model maker,

464
00:54:14,960 --> 00:54:21,680
scientists like Newton and the model users, STEM competent people everywhere, understand

465
00:54:21,680 --> 00:54:28,720
enough about the problem domain to know whether the model is applicable or not, and how to use it.

466
00:54:28,720 --> 00:54:34,560
This understanding is the expensive part of science, since using science requires first

467
00:54:34,560 --> 00:54:39,760
getting a solid science education in order to avoid mistakes when using models.

468
00:54:40,400 --> 00:54:45,920
And since models require understanding, they cannot be used to create understanding.

469
00:54:46,640 --> 00:54:52,080
This is a major problem for AI implementers. Chapter 3

470
00:54:52,080 --> 00:54:57,120
2 Dirty Words Reductionism is the use of models.

471
00:54:57,840 --> 00:55:04,240
Holism is the avoidance of models. Matters are scientific models, theories,

472
00:55:04,240 --> 00:55:11,760
hypotheses, formulas, equations, superstitions, and most computer programs.

473
00:55:13,040 --> 00:55:20,000
Reductionism and Holism. After having sorted out what models are, we can now discuss two

474
00:55:20,000 --> 00:55:27,280
complementary problem-solving strategies, or perhaps meta-strategies. There are in many ways

475
00:55:27,280 --> 00:55:33,360
each other's opposites, but the classification can become an argument about novel levels and

476
00:55:33,360 --> 00:55:40,560
definitions. I will initially pretend the division is clear and obvious, and will elaborate later.

477
00:55:41,200 --> 00:55:48,320
Reductionism is the use of models. In this series we will use exactly the above definition of the

478
00:55:48,320 --> 00:55:55,600
word, reductionism. If you look up the definition elsewhere you may find that some sources divide

479
00:55:55,600 --> 00:56:02,640
the strategy into sub-strategies. They also seem to miss the most important sub-strategy,

480
00:56:02,640 --> 00:56:08,720
which we'll discuss later. But what all these sub-strategies have in common is that they all

481
00:56:08,720 --> 00:56:15,440
provide ways to simplify observations of fragments of our rich mundane reality into much simpler

482
00:56:15,440 --> 00:56:23,840
models, which we use for reasoning, computation, and sharing. Reductionism is so central to how

483
00:56:23,840 --> 00:56:31,440
we do science, the heavy reliance on models, such as theories, equations and formulas,

484
00:56:31,440 --> 00:56:41,040
and physics, chemistry, etc. That we can speak of model-based sciences or reductionist sciences

485
00:56:41,040 --> 00:56:48,320
where such model-making is easy and effective, and this classification excludes those sciences,

486
00:56:48,320 --> 00:56:54,480
like psychology, where such model-making is difficult and less often rewarded with reliable

487
00:56:54,480 --> 00:57:01,760
results. After considering all the advantages of models we might wonder why we even bother

488
00:57:01,760 --> 00:57:09,360
discussing it. Too many people, especially those with a solid stem, science, technology,

489
00:57:09,360 --> 00:57:15,360
engineering, and mathematics education, it may well look like the only choice,

490
00:57:16,080 --> 00:57:23,040
but there's also the other strategy. Homism is the avoidance of models. This is where the

491
00:57:23,040 --> 00:57:30,000
questions start. This is where the paradox is surface. This is where your worldview may get

492
00:57:30,000 --> 00:57:38,000
shaken up. Seriously, especially if you are a scientist or engineer with a solid stem education

493
00:57:38,000 --> 00:57:46,240
and decades of professional success using science and models. In some sense, the goal of this entire

494
00:57:46,240 --> 00:57:52,080
series is to demonstrate that we need to use both problem-solving strategies when creating

495
00:57:52,080 --> 00:57:58,640
our artificial intelligences, because that is what it is going to take. We need holistic

496
00:57:58,640 --> 00:58:05,760
understanding. We established that in the first chapter, as a sample of the new ideas that we

497
00:58:05,760 --> 00:58:14,480
will have to deal with I will just mention, reasoning is reductionist. Understanding is holistic.

498
00:58:15,760 --> 00:58:23,120
Newer networks are holistic. Holistic systems can jump to conclusions on scant evidence.

499
00:58:24,400 --> 00:58:28,960
Holistic systems can themselves know what is important and what isn't.

500
00:58:28,960 --> 00:58:36,720
Holistic systems can solve problems we ourselves cannot or don't care to understand.

501
00:58:36,720 --> 00:58:45,360
Holistic systems are model-free. We do not use any a priori models of any problem domain.

502
00:58:46,560 --> 00:58:51,200
Reasoning systems inherit all problems and benefits of reductionism.

503
00:58:52,480 --> 00:58:57,120
Understanding systems inherit all problems and benefits of holism.

504
00:58:57,120 --> 00:59:04,000
Humans are born holistic. Humans each solve thousands of little

505
00:59:04,000 --> 00:59:11,840
problems every day, and we are solving almost all these problems holistically, using understanding,

506
00:59:11,840 --> 00:59:16,960
and without a need to reason at all. This includes fluent language use.

507
00:59:18,160 --> 00:59:24,000
A stem education instills a strict reductionist discipline in order to mitigate problems

508
00:59:24,000 --> 00:59:30,000
with fallibility of holistic human minds. Our intelligences are fallible.

509
00:59:30,800 --> 00:59:36,880
These claims all deserve individual treatments, and we'll get to all of them in later sections.

510
00:59:37,600 --> 00:59:43,360
But the major theme is clear. Humans are mainly holistic problem solvers.

511
00:59:44,000 --> 00:59:51,840
This must be true for our artificial intelligences. We had several reasons for focusing on reductionist

512
00:59:51,840 --> 00:59:59,280
methods, models, and reasoning during the first 60 years of AI. Our computers were too small to

513
00:59:59,280 --> 01:00:07,200
make neural networks work at all. But there were also ideological reasons. AI was born out of the

514
01:00:07,200 --> 01:00:13,520
math and computer science departments of our universities, and therefore most of the people

515
01:00:13,520 --> 01:00:19,440
working on AI were solidly oriented towards the goal of creating a logic-based reductionist

516
01:00:19,440 --> 01:00:26,640
infallible artificial mind. To build early AIs, like expert systems, we entered rules

517
01:00:26,640 --> 01:00:33,520
or programmed in lots of facts to reason about. But this was budding reductionist castles in the

518
01:00:33,520 --> 01:00:40,720
air, comprised of unanchored facts that didn't tie to any understanding whatsoever. The troubles

519
01:00:40,720 --> 01:00:47,280
with classical AI, such as bitterness, the tendency to make spectacular and expensive

520
01:00:47,280 --> 01:00:53,520
mistakes at the edges of their competence, can be directly traced to the lack of foundational

521
01:00:53,520 --> 01:00:59,360
understanding to support these attempts at reasoning. Understanding machines will not

522
01:00:59,360 --> 01:01:05,120
suffer from this brittleness, but will fail gracefully at the edges of their competence,

523
01:01:05,120 --> 01:01:12,240
much like humans. Most of the time they will know the answer beyond that they will guess,

524
01:01:12,240 --> 01:01:18,560
and the guesses they make are based on a lifetime of experience, gained through learning from a

525
01:01:18,560 --> 01:01:25,280
large corpus and so they have a good chance of being at least a workable choice, if not perfect.

526
01:01:25,920 --> 01:01:32,640
How can anyone solve problems without using models? A lot of people coming from a STEM

527
01:01:32,640 --> 01:01:38,960
background cannot even imagine how to solve problems without using models. But it's not

528
01:01:38,960 --> 01:01:45,760
hard, once you understand the difference, mostly it's a matter of doing what worked last time.

529
01:01:46,480 --> 01:01:52,400
The problem is now figuring out whether we are in a situation that's similar enough that it will

530
01:01:52,400 --> 01:02:00,400
work again. This is mostly a pattern matching problem. More later, what's the result? The

531
01:02:00,400 --> 01:02:07,120
holistic answer is a quick guess at the best action, based on experience with similar situations.

532
01:02:07,120 --> 01:02:13,360
Most of the time it's correct, sometimes it's a little wrong, and every now and then,

533
01:02:13,360 --> 01:02:20,080
there's a noticeable mistake. And if we get things a little wrong, we may notice the outcome

534
01:02:20,080 --> 01:02:26,560
and correct the action. We learn from our mistakes. If we practice something a lot,

535
01:02:26,560 --> 01:02:33,520
we will start doing it effectively and perfectly every time. Do we learn faster if we make more

536
01:02:33,520 --> 01:02:41,920
mistakes? Should we make mistakes on purpose? More later, in situations where you cannot use

537
01:02:41,920 --> 01:02:48,800
models, which are more common than many realize, the holistic guess may also be your only option.

538
01:02:49,600 --> 01:02:57,840
Conversely, if you have an adequately well-working model-based solution, just use that. My video,

539
01:02:57,840 --> 01:03:03,920
Model-Free Methods Workshop demonstrates how the group solves four different problems

540
01:03:03,920 --> 01:03:11,200
at a high level, using both reductionist and holistic methods. Why are these dirty words?

541
01:03:11,920 --> 01:03:19,040
Well, they are not dirty to epistemologists. Reductionism has been the default problem-solving

542
01:03:19,040 --> 01:03:25,600
paradigm because it's the one that has to be taught. We are born with a holistic problem-solving

543
01:03:25,600 --> 01:03:32,480
apparatus. But reductionist science doesn't come naturally. Therefore, it has to be taught in

544
01:03:32,480 --> 01:03:39,760
schools, practiced, and carried out according to certain rules. Perhaps that's why the sciences

545
01:03:39,760 --> 01:03:46,400
are called disciplines, because following the ideal scientific method requires practice and

546
01:03:46,400 --> 01:03:55,760
constant vigilance. J. C. Smutsbrook, Holism and Evolution, 1926 established the terminology in the

547
01:03:55,760 --> 01:04:04,640
epistemological literature. And no inchrodinger wrote, what is life, 1944, questioning the power

548
01:04:04,640 --> 01:04:11,840
of physics to provide useful explanations to the life sciences. Percy Grote, zen and the art of

549
01:04:11,840 --> 01:04:19,840
motorcycle maintenance, 1974, had contrast something very holistic, zen Buddhism, with

550
01:04:19,840 --> 01:04:27,920
something very reductionist, motorcycle maintenance. So the chasm between the strategies was identified

551
01:04:27,920 --> 01:04:36,240
a long time ago. The strategies are each other's opposites. H-O-L-E-L-I-S-M-based strategies for

552
01:04:36,240 --> 01:04:42,640
understanding can handle many important kinds of complexity and can quickly provide a guest

553
01:04:42,640 --> 01:04:49,840
answer. But these guesses are fallible, and often more expensive to compute. Reductionist

554
01:04:49,840 --> 01:04:55,840
education and strategies brought benefits of cheap model reuse and formal rigor to improve

555
01:04:55,840 --> 01:05:01,440
correctness, but cannot handle complexity and is therefore dependent on an external

556
01:05:01,440 --> 01:05:06,720
understander to determine applicability in real-world complexity rich situations.

557
01:05:07,440 --> 01:05:14,800
And as part of that education, we are told that holistic methods, such as jumping to conclusions

558
01:05:14,800 --> 01:05:21,600
unscanned evidence, are bad, in spite of the fact that our brains use holistic methods thousands

559
01:05:21,600 --> 01:05:28,240
of times each day to successfully understand the environment we live in. We can all use

560
01:05:28,240 --> 01:05:35,040
either strategy as appropriate. If we don't have a STEM education, we will still sometimes make

561
01:05:35,040 --> 01:05:41,520
naive models. But sometimes there is a choice and different people may prefer one or the other.

562
01:05:42,160 --> 01:05:48,640
When playing pool, some people estimate and compute bouncing angles and some people shoot

563
01:05:48,640 --> 01:05:55,920
by feel. But we have our preferences, and it may be tempting to label a person with an overly

564
01:05:55,920 --> 01:06:04,160
strong preference as a holistic or a reductionist. This is sometimes received badly, if perceived

565
01:06:04,160 --> 01:06:12,880
as a limitation. Some dictionaries even flag reductionist as derogatory. And yet, some people

566
01:06:12,880 --> 01:06:20,640
use it as a self-assigned label. I try to use these terms only as shorthand for a person with a

567
01:06:20,640 --> 01:06:28,240
stated strong preference for holistic or reductionist methods. The two terms were very useful in

568
01:06:28,240 --> 01:06:36,400
epistemology. But then someone invented the concept of holistic medicine. Instead of just shooting

569
01:06:36,400 --> 01:06:44,400
a single medical problem, you analyze the patient's entire situation, attempting to account for diet,

570
01:06:44,400 --> 01:06:53,680
exercise, sleep, work, habits, stress levels, allergies, family, friends, and environmental

571
01:06:53,680 --> 01:07:01,920
poisons. A good idea, in general. But the wide scope was unmanageable by the, traditionally

572
01:07:01,920 --> 01:07:09,760
reductionist medical establishment and the idea faded away. Instead, the whole idea of holism

573
01:07:09,760 --> 01:07:17,040
became tainted as woo-woo in the term, holistic medicine, became associated with woo-woo merchants

574
01:07:17,040 --> 01:07:24,560
selling crystals and aromatherapy. As explained above, holism is the avoidance of models,

575
01:07:24,560 --> 01:07:31,360
or better phrased, holism is the metastrategy of avoiding a priori models of the problem domain.

576
01:07:32,000 --> 01:07:38,720
That extra precision rarely matters. There's nothing woo-woo about it. It does say,

577
01:07:38,720 --> 01:07:45,360
science not required, but, you can make breakfast without reasoning. It is important to note that

578
01:07:45,360 --> 01:07:51,680
holistic methods are based on a lifetime of experience, in humans and a training corpus

579
01:07:51,680 --> 01:07:58,560
worth of experience, in neural networks. When you're making breakfast, you are relying on this

580
01:07:58,560 --> 01:08:05,520
experience, mostly repeating whatever worked yesterday. Some people claim they use reasoning

581
01:08:05,520 --> 01:08:11,360
while making breakfast, but they can make their breakfast while speaking to someone else on the

582
01:08:11,360 --> 01:08:17,440
phone. And as they hang up, they find themselves suddenly sitting at the breakfast table with

583
01:08:17,440 --> 01:08:24,800
their coffee and hot oatmeal. Same thing when driving to work. You may get lost in thought,

584
01:08:24,800 --> 01:08:31,520
and then you find yourself parked at work. You didn't need to reason, since all sub-problems

585
01:08:31,520 --> 01:08:36,480
that occur in driving had been solved multiple times, during years of driving.

586
01:08:37,120 --> 01:08:43,280
Sub-conscious understanding is used for simple things like sequencing our leg muscles as we

587
01:08:43,280 --> 01:08:52,000
walk. You have no idea how you are doing that, it just works. Same thing with vision. You understand

588
01:08:52,000 --> 01:08:58,640
that you are looking at a chair, but you do not have conscious access to the 15th rod cone pixel

589
01:08:58,640 --> 01:09:04,320
to the left of your center of vision, and have no idea how this understanding works.

590
01:09:05,040 --> 01:09:11,200
Same thing with understanding and generating language. You do not have any explanation for

591
01:09:11,200 --> 01:09:17,360
how you are able to understand the meaning of this sentence. Understanding is sub-conscious

592
01:09:17,360 --> 01:09:24,240
and holistic. So for the majority of things we do every day, we do not need reasoning or

593
01:09:24,240 --> 01:09:31,360
reductionist methods. Some people would like to think they are, logical thinkers, immune to

594
01:09:31,360 --> 01:09:38,560
most cognitive fallacies, but whether they are or not, at the lower levels, everyone is solving

595
01:09:38,560 --> 01:09:45,600
most of their problems holistically. I claim that reductionist reasoning requires holistic

596
01:09:45,600 --> 01:09:53,120
understanding. In other words, I need to understand the problem domain at hand before I can create

597
01:09:53,120 --> 01:10:00,160
and reuse models to enable me to reason about the domain. So holistic understanding is much

598
01:10:00,160 --> 01:10:06,560
more important than reductionist reasoning because it is the most used strategy, by far,

599
01:10:06,560 --> 01:10:13,600
and the former is also a prerequisite for the latter. But the fallibility of holistic understanding

600
01:10:13,600 --> 01:10:20,800
forced us to create reductionist science and to teach it in STEM education. It is as if the purpose

601
01:10:20,800 --> 01:10:27,920
of science is to keep holistic guessing in check, but this aversion to fallibility has a cost,

602
01:10:27,920 --> 01:10:35,440
because it means complexity bound and irreducible problems cannot be solved. Like language

603
01:10:35,440 --> 01:10:43,440
understanding, global resource allocation, and social interactions, reductionism and model-based

604
01:10:43,440 --> 01:10:51,760
science appeared around 1650 after a century of gestation. Excluding minor romantic interludes,

605
01:10:51,760 --> 01:10:58,800
it has held its position as the dominant paradigm for about 400 years. This is changing.

606
01:10:59,600 --> 01:11:06,000
The reductionist train is running out of track. The remaining hard problems facing humanity

607
01:11:06,000 --> 01:11:11,920
are problems of irreducible complexity in domains where reductionist model-based methods

608
01:11:11,920 --> 01:11:19,360
simply cannot work. Whether we like the idea or not, we need to accept these holistic methods

609
01:11:19,360 --> 01:11:26,480
into our AI toolkits. Starting now, we will use these methods either in their raw form,

610
01:11:26,480 --> 01:11:32,800
as model-free methods, or as understanding machines at any level from component to robot

611
01:11:32,800 --> 01:11:42,560
co-worker. Chapter 4. Reduction. Epistemic reduction is a process that discovers higher-level

612
01:11:42,560 --> 01:11:48,800
abstractions and lower-level data by discarding everything at the lower layer that it recognizes

613
01:11:48,800 --> 01:11:56,560
as irrelevant. We have seen the power of models. We have introduced the two problem-solving

614
01:11:56,560 --> 01:12:03,440
meta-strategies of reductionism and holism. We also noted that the creation and use of models

615
01:12:03,440 --> 01:12:10,320
requires an intelligent agent that understands the problem domain. Someone or something has to

616
01:12:10,320 --> 01:12:19,760
perform the reduction. I will now discuss reduction in some detail. Until 2012, only humans and other

617
01:12:19,760 --> 01:12:26,880
animals with brains could perform reduction. Now our deep neural networks, DNN, can perform

618
01:12:26,880 --> 01:12:34,640
limited reduction. How do brains and DNNs accomplish this? And how can we improve these algorithms?

619
01:12:35,360 --> 01:12:42,160
This may be, to some readers, the most rewarding part of this series, because it provides you

620
01:12:42,160 --> 01:12:48,960
the opportunity to learn a new and useful skill. Most people never think about the world at this

621
01:12:48,960 --> 01:12:55,760
level. Knowledge of reduction provides a new point of view that you can use to better understand

622
01:12:55,760 --> 01:13:01,440
your environment, other intelligent agents around you, and modern AI systems.

623
01:13:02,240 --> 01:13:08,880
Definition of reduction. Reduction is a process that discovers higher-level abstractions and

624
01:13:08,880 --> 01:13:14,800
lower-level data. We will initially note that reduction is exactly the same as abstraction.

625
01:13:14,800 --> 01:13:21,680
Why do we need a new word? Because the term abstraction is mostly used

626
01:13:21,680 --> 01:13:28,080
by scientists already operating in a pure model space, seeking a higher level of abstraction

627
01:13:28,080 --> 01:13:34,880
in that space. But to them, abstraction is something that just magically happens in their

628
01:13:34,880 --> 01:13:41,760
heads, since there are no scientific theories for how abstraction works. There cannot be,

629
01:13:41,760 --> 01:13:49,280
since abstraction is a concept in epistemology, not science. AI researchers are starting from

630
01:13:49,280 --> 01:13:55,600
something much closer to a rich mundane reality, where there is a lot of confounding context.

631
01:13:56,160 --> 01:14:01,920
We are solving the metal problem of how to move from there into a space that is sufficiently

632
01:14:01,920 --> 01:14:08,720
abstract to solve the problem at hand. Here, reduction is a much more appropriate term.

633
01:14:08,720 --> 01:14:15,600
We can abstract the red pixel or the letter B, but we can reduce a rich context containing

634
01:14:15,600 --> 01:14:21,600
that pixel or letter into a higher-level concept. We are swimming in reduction.

635
01:14:21,600 --> 01:14:27,600
Paradoxically, one of the hardest things about teaching reduction is that we don't see the

636
01:14:27,600 --> 01:14:33,920
need to learn about it because we all do it all the time, every millisecond, and the resulting

637
01:14:33,920 --> 01:14:42,240
reductions, models, become available to our conscious minds as if, by magic, brains reduce

638
01:14:42,240 --> 01:14:51,200
away 99.999% of their sensory input, but this process is subconscious and hence invisible to us.

639
01:14:51,920 --> 01:15:00,240
The situation is much like, supposedly, a fish swimming in water. We are all masters of reduction,

640
01:15:00,240 --> 01:15:06,160
but we don't know how we do it or that we even do it. We didn't know this would ever matter.

641
01:15:06,800 --> 01:15:14,560
And generally, it doesn't. Well, it matters in epistemology, and it matters in AI,

642
01:15:14,560 --> 01:15:21,840
since we need to actually implement that magic. We as epistemologists must know how abstraction

643
01:15:21,840 --> 01:15:28,560
is actually performed, and we give the epistemology-level equivalent of abstraction the name

644
01:15:28,560 --> 01:15:34,880
reduction, because that's the recipe for how to accomplish it. We reduce our rich mundane

645
01:15:34,880 --> 01:15:42,400
reality by discarding, reducing away, what's irrelevant. And by using the name reduction,

646
01:15:42,400 --> 01:15:48,480
we, as AI epistemologists, keep reminding ourselves how it is properly done.

647
01:15:49,120 --> 01:15:56,160
Consider the following descriptions of a car. The slide is meant to be read from the bottom up,

648
01:15:56,160 --> 01:16:05,360
to match abstraction levels from low to high. If I'm driving to work, I better be driving my car.

649
01:16:06,000 --> 01:16:12,640
If the police are looking for a stolen car, they would be looking for red 2010 Toyota Celica.

650
01:16:13,280 --> 01:16:18,640
If I'm buying a new car, then I might be looking for just a new Toyota Celica.

651
01:16:19,200 --> 01:16:25,600
And a self-driving car would likely only need to understand whether an obstacle is a vehicle or

652
01:16:25,600 --> 01:16:33,120
not, in order to model maximum speed for future movement. We see that we want to pick the appropriate

653
01:16:33,120 --> 01:16:39,120
level of abstraction to deal with the same object, or topic, in different situations.

654
01:16:39,840 --> 01:16:45,280
But more importantly, we see that we can get from a more detailed description,

655
01:16:45,280 --> 01:16:51,200
at the bottom, to a more generic one, higher up, by simply discarding some detail.

656
01:16:51,200 --> 01:16:57,520
I hasten to point out that reduction is more complicated than this simple example of decreasing

657
01:16:57,520 --> 01:17:04,480
specificity shows. What we need to start somewhere in this image allows us to form intuitions that

658
01:17:04,480 --> 01:17:11,680
will serve for a while. True reduction involves operations like shifting from syntax to semantics

659
01:17:11,680 --> 01:17:19,680
or from instance to type. The appearance of car as an abstraction of Toyota, and the step from

660
01:17:19,680 --> 01:17:27,840
my Toyota to a Toyota illustrates these steps. Algorithms for these things are known.

661
01:17:28,640 --> 01:17:36,000
Salience, part of the trick is to know what to discard. At each level of abstraction,

662
01:17:36,000 --> 01:17:43,760
something can typically be identified as the least important property. Red and Celica are more

663
01:17:43,760 --> 01:17:53,280
significant than 2010 for anyone looking for a car. If we had started from my red 2010 Toyota

664
01:17:53,280 --> 01:18:01,360
truck, then the word truck would not be discarded until the top level. Reduction requires understanding

665
01:18:01,360 --> 01:18:09,440
what's relevant. In reduction we keep that which is salient. More later, partial reductions.

666
01:18:09,440 --> 01:18:17,360
Most of the time we do not perform reduction all the way to models. I cannot stress this enough.

667
01:18:18,000 --> 01:18:25,120
We discuss reduction to models for pedagogical reasons. It is easy to initially see the context

668
01:18:25,120 --> 01:18:32,960
free model as the goal of reduction. In reality, in brains, we can stop reducing the moment we

669
01:18:32,960 --> 01:18:39,680
recognize that we have a working answer or response, such as a command to contract some muscle or

670
01:18:39,680 --> 01:18:46,000
having understood the meaning of a sentence subconsciously. At this point, there is still

671
01:18:46,000 --> 01:18:52,160
some residual context but we use that context productively rather than discard it to move

672
01:18:52,160 --> 01:19:00,160
to higher levels. Some people claim we use models for all our thinking, but I'm using capital M

673
01:19:00,160 --> 01:19:08,160
model only to describe a completely context free abstraction. F equals M A is an example of that.

674
01:19:08,880 --> 01:19:15,920
There is no need to check whether a car is a red car or a Toyota. The equation works not only for

675
01:19:15,920 --> 01:19:23,680
all cars but for all forces, masses and accelerations. We might come up with a special equation for

676
01:19:23,680 --> 01:19:29,760
acceleration of Tesla cars which would require different inputs like battery charge level

677
01:19:29,760 --> 01:19:36,480
and software settings. That would not be a context free model since it would not work on a Toyota.

678
01:19:37,120 --> 01:19:45,280
For almost all tasks, basically, in everything except science and even there, only rarely,

679
01:19:45,280 --> 01:19:52,800
we only perform as much reduction as is necessary to get the job done. When learning to ski,

680
01:19:52,800 --> 01:19:58,560
you only figure out how you yourself need to perform given your body and equipment.

681
01:19:58,560 --> 01:20:04,880
We do not need to parameterize our skiing skills for someone with twice the body mass

682
01:20:04,880 --> 01:20:11,040
because that would be useless to us for the purpose of our own skiing. But a scientist would

683
01:20:11,040 --> 01:20:17,840
have to go that far in order to parameterize away one more piece of context from the model

684
01:20:17,840 --> 01:20:24,720
they are creating. For instance, when creating a skiing video game or designing a new ski,

685
01:20:24,720 --> 01:20:30,960
if we consider the enormous amount of subconscious activity that happens in the brain,

686
01:20:30,960 --> 01:20:37,520
we can safely say that partial reductions are the most common reductions. For instance,

687
01:20:37,520 --> 01:20:44,320
when we take a step forward, our subconscious has analyzed our posture and velocity by using

688
01:20:44,320 --> 01:20:50,000
reduction based on low level nerve signals and is commanding leg muscles to contract an

689
01:20:50,000 --> 01:20:57,040
up precisely timed sequence. This activity is something we are unaware of. Most of us don't

690
01:20:57,040 --> 01:21:04,000
even know what leg muscles we have. And there would be no time to perform reduction all the way to

691
01:21:04,000 --> 01:21:10,560
models. That process takes a minimum of a half second and you don't have that kind of time

692
01:21:10,560 --> 01:21:17,040
available to respond to an imbalance when walking or skiing. Reduction in society.

693
01:21:17,040 --> 01:21:24,240
Most of us get paid to understand whatever we need to understand in order to perform our jobs.

694
01:21:24,880 --> 01:21:31,760
In other words, most of us get paid to do reduction. If you are approving building permits,

695
01:21:31,760 --> 01:21:40,160
you reduce a stack of forms to a one bit verdict of approved or rejected. We accelerate reduction,

696
01:21:40,160 --> 01:21:44,720
and this is the main reason most of us haven't been replaced by robots.

697
01:21:44,720 --> 01:21:51,440
But we see that when future understanding machines can perform reduction by themselves,

698
01:21:51,440 --> 01:21:56,160
then we are unlikely to get paid for it. Levels of reduction.

699
01:21:56,960 --> 01:22:03,040
Suppose a young man and a young woman fall in love, something happens to mess it all up,

700
01:22:03,040 --> 01:22:08,400
and then they sort this out and reunite. This is what happened in the man's,

701
01:22:08,400 --> 01:22:15,440
which mundane reality. Suppose the man wants to share this experience, because there was some

702
01:22:15,440 --> 01:22:20,800
moral to the story that he thinks would be interesting to others and possibly important.

703
01:22:21,440 --> 01:22:27,120
He could analyze what happened and figure out which were the key events in the saga and then

704
01:22:27,120 --> 01:22:34,160
have actors on a stage re-enact the story as a play. This is a reduction because the boring parts

705
01:22:34,160 --> 01:22:41,040
of the story would not be part of the play. They are discarded as irrelevant, but the story would

706
01:22:41,040 --> 01:22:48,080
be acted out by real people in front of a live audience. If you are in the audience, you can move

707
01:22:48,080 --> 01:22:54,480
your head to see behind any actor on the stage and you can clearly see everything on the stage,

708
01:22:54,480 --> 01:23:02,080
not just one actor speaking at a time. He can make a movie about it. Now your point of view

709
01:23:02,080 --> 01:23:09,520
is pre-defined by the camera angle and cropping. You can no longer see behind an actor, and you

710
01:23:09,520 --> 01:23:16,320
can often only see those actors that are involved in the main action. He could write a book about it.

711
01:23:16,960 --> 01:23:23,040
We no longer can see even the people described in the book, except in our imagination.

712
01:23:23,760 --> 01:23:31,280
A critic review in the theater play may reduce it to, Boy meets girl, Boy loses girl, Boy gets

713
01:23:31,280 --> 01:23:38,640
girl. A drama school graduate may summarize it as a double reversal plot. This is a description

714
01:23:38,640 --> 01:23:46,160
that is so free from context, doesn't even specify boys or girls that it could be argued it qualifies

715
01:23:46,160 --> 01:23:55,360
to be called a model. Plays, movies, books, stories, tropes, etc. are all partial reductions of

716
01:23:55,360 --> 01:24:03,920
reality, and some are more reduced than others. Just like in the red Toyota case, we need to find

717
01:24:03,920 --> 01:24:10,960
the appropriate level of abstraction to work with. The young man in the example, when writing a

718
01:24:10,960 --> 01:24:17,600
book or a screenplay, has much in common with a scientist trying to describe something in nature

719
01:24:17,600 --> 01:24:24,560
in a reusable context free manner by reducing it to a model. They are model makers, or are at

720
01:24:24,560 --> 01:24:31,360
least performing partial reduction. They are discarding the irrelevant bits. The opposite of

721
01:24:31,360 --> 01:24:38,640
reduction. We also need to be able to move in the opposite direction, from models to reality,

722
01:24:39,280 --> 01:24:46,240
or at least from more abstract partial models to partial models closer to reality. When an actor

723
01:24:46,240 --> 01:24:52,560
is given a screenplay, they know it only contains rough directions for what to do and what lines

724
01:24:52,560 --> 01:25:00,160
to say. The actor's job is to give a little of themselves to flesh out the screenplay to actual

725
01:25:00,160 --> 01:25:08,720
actions, including creating, synthesizing, the appropriate display of emotions, tone of voice,

726
01:25:08,720 --> 01:25:16,160
and body language. They use their experience as people and as actors. They use elements of their

727
01:25:16,160 --> 01:25:22,320
past lives and skills they have acquired by training to create something people in the audience

728
01:25:22,320 --> 01:25:30,080
might relate to. For example, they may repurpose a personal experience. He is sad as when my

729
01:25:30,080 --> 01:25:38,880
hamster died. Things they learned in drama school, such as speaking, singing, dancing, and swordplay,

730
01:25:38,880 --> 01:25:46,720
from other actors, what would bogart do, from fiction, from other movies and plays, etc.

731
01:25:46,720 --> 01:25:54,880
The actor's artist who convey whatever the script intends to convey, emotions, a morality cookie,

732
01:25:54,880 --> 01:26:02,800
a political position, titillation, surprise, and so on. Starting from the simple model,

733
01:26:02,800 --> 01:26:09,360
the screenplay, their job is similar to an engineer's when they are faced with a problem

734
01:26:09,360 --> 01:26:15,440
and use a model to solve it. The engineer would use their experience to decide that

735
01:26:15,440 --> 01:26:22,560
M is the mass of the car and not the tire pressure. The actor decides that sadness

736
01:26:22,560 --> 01:26:31,280
is more appropriate than grief for a certain scene, etc. I call this process, which is the

737
01:26:31,280 --> 01:26:39,280
opposite of reduction by the name it is used in problem solving application. We use a model to

738
01:26:39,280 --> 01:26:47,360
simplify a problem situation, moving it into an abstract and pure model space. We solve the

739
01:26:47,360 --> 01:26:55,040
problem there by performing math, perhaps, and then apply the answer to our rich reality

740
01:26:55,040 --> 01:27:02,080
to the problem we are trying to solve. Many of you may recognize the word application or

741
01:27:02,080 --> 01:27:09,840
its abbreviation, app. That's not as far-fetched as it might seem. Apps are software-based models.

742
01:27:10,560 --> 01:27:16,240
Reduction in application and brains. Back to the issue of partial reductions.

743
01:27:16,960 --> 01:27:24,400
Consider the actor reading a screenplay. They are using their eyes to gather pixels of color

744
01:27:24,400 --> 01:27:32,320
and orientation. The brain then performs pattern matching, reduction, from these low-level signals

745
01:27:32,320 --> 01:27:40,160
to letters, words, to language, to high-level concepts like love and separation, and eventually

746
01:27:40,160 --> 01:27:46,560
to a high-level understanding of the playwright's intents. The actor then takes this high-level

747
01:27:46,560 --> 01:27:52,640
understanding and by performing application, they add their own experience to the script

748
01:27:52,640 --> 01:27:59,360
to get closer to reality and their performance. Our brains are capable of moving up and down

749
01:27:59,360 --> 01:28:06,080
many levels of abstraction at once. Perhaps it tracks all of them simultaneously,

750
01:28:06,080 --> 01:28:12,080
keeping layers of abstraction separate. This is a clue for why deep neural networks

751
01:28:12,080 --> 01:28:16,960
perform better than shallow ones. Which is what we'll discuss next.

752
01:28:16,960 --> 01:28:25,120
Chapter 5. Why Deep Learning Works. Deep learning performs epistemic reduction.

753
01:28:26,320 --> 01:28:32,720
A math-free computer science-free description of why deep learning works. We have now built

754
01:28:32,720 --> 01:28:39,440
a base of theory for why AI works, what models are, and how to create them, what reductionism

755
01:28:39,440 --> 01:28:47,040
and holism are, and what the process of reduction is. These are the fundamentals of AI epistemology.

756
01:28:47,760 --> 01:28:53,760
This base allows us to discuss various strategies to move towards understanding machines in a

757
01:28:53,760 --> 01:29:00,240
well-understood and controlled manner. We are now ready to discuss why deep learning,

758
01:29:00,240 --> 01:29:07,920
DL, works. This is the fifth and last entry in the AI epistemology primer. Deep learning

759
01:29:07,920 --> 01:29:14,080
performs reduction. This is an unsurprising claim, considering the preceding chapters.

760
01:29:14,800 --> 01:29:21,680
There are several mutually compatible theories for how deep learning works. But just as in

761
01:29:21,680 --> 01:29:28,640
the first chapter, we will now discuss the epistemological aspects, why it works,

762
01:29:28,640 --> 01:29:35,520
from several viewpoints and levels, starting from the bottom. We would use examples from the

763
01:29:35,520 --> 01:29:42,720
TensorFlow system and API as a library, as a stand-in for all deep learning family algorithms

764
01:29:42,720 --> 01:29:49,680
and TF programs, because the available API functions heavily shape and constrain solutions

765
01:29:49,680 --> 01:29:55,040
that can be implemented in this space. And the generalization should be straightforward enough.

766
01:29:55,760 --> 01:30:01,280
Consider the following illustration of image understanding using Keras, an excellent

767
01:30:01,280 --> 01:30:08,400
abstraction layer on top of TensorFlow. I like to refer to the input layer as being

768
01:30:08,400 --> 01:30:14,880
on the bottom rather than at the far left as in this image. When viewing it my way,

769
01:30:14,880 --> 01:30:20,560
the low to high dimension we use in my rotated version of the image can be mentally mapped

770
01:30:20,560 --> 01:30:27,360
to a low to high stack of abstraction levels. I'm not the only one using this dimension this way.

771
01:30:27,360 --> 01:30:33,920
I hope this rotation isn't too confusing. We can see that there is an obvious data reduction

772
01:30:33,920 --> 01:30:40,400
and an obvious complexity reduction. Can we determine whether the system is also performing

773
01:30:40,400 --> 01:30:47,360
what I'd like to call the epistemic reduction? Is it reducing a way that which is unimportant?

774
01:30:47,360 --> 01:30:53,840
And if so, how does it accomplish this? How does an operator in a deep learning stack

775
01:30:53,840 --> 01:31:01,120
know what makes something important? Salient, up your data, reduction of sorts could be

776
01:31:01,120 --> 01:31:09,200
accomplished by compression schemes or even random deletion. This is undesirable. We need to discard

777
01:31:09,200 --> 01:31:16,640
the non-salient parts so that in the end, we are left with what is salient. Some people have not

778
01:31:16,640 --> 01:31:22,400
understood the importance of salient's based reduction and useless compression power of

779
01:31:22,400 --> 01:31:28,560
reversible algorithms as a measurement of intelligence, which is no more useful than

780
01:31:28,560 --> 01:31:36,080
believing a simple video camera can understand what it sees. So let me conjure up a bit like in

781
01:31:36,080 --> 01:31:43,440
the movie, Inside Out, a fairy tale of what goes on in a deep learning network, except we'll do it,

782
01:31:43,440 --> 01:31:50,560
bottom up. Suppose we have built a system for finding faces in an image with the intent of

783
01:31:50,560 --> 01:31:56,800
incorporating that as a feature in a camera. Many cameras have this feature already,

784
01:31:56,800 --> 01:32:03,600
so this is not a far-fetched example. We implement an image understanding neural network,

785
01:32:03,600 --> 01:32:10,720
show the system many kinds of images for a few days, perhaps using so-called supervised learning

786
01:32:10,720 --> 01:32:17,200
in order to improve this story, and then we show it an image of a family having a picnic in a park

787
01:32:17,200 --> 01:32:23,520
and ask the system to outline where the faces are so that the camera can focus sharply on them.

788
01:32:24,240 --> 01:32:30,800
The input image is converted from RGB color values to an input array and the data in this array is

789
01:32:30,800 --> 01:32:37,280
then shuffled through many layers of operators. And for many of these layers, there are fewer

790
01:32:37,280 --> 01:32:44,320
outputs than there are inputs, as you can see above, which means some things have to be discarded

791
01:32:44,320 --> 01:32:52,320
by the processing. Each layer receives initially signals, from below, that is, from the input,

792
01:32:52,320 --> 01:32:58,960
or from lower levels of abstraction, and produces some reduced output to send to the next layer

793
01:32:58,960 --> 01:33:06,080
operator above. To continue detail, at some early level, some operator is given a few

794
01:33:06,080 --> 01:33:12,000
adjacent pixels and determines that there is a vertical, slightly curved line dividing the

795
01:33:12,000 --> 01:33:19,440
darker green area from the lighter green area. So it tells the operator above the simpler line

796
01:33:19,440 --> 01:33:26,080
or color-based description using some encoding we don't really care about. The operator at the

797
01:33:26,080 --> 01:33:32,000
level above might have gotten another matching curve and says, these match what I saw a lot of

798
01:33:32,000 --> 01:33:39,040
when the label blade of grass was given as a ground truth label during supervised learning.

799
01:33:39,040 --> 01:33:44,640
If no label is known, then we again assume some other uninteresting representation.

800
01:33:44,640 --> 01:33:51,680
It is okay to propagate results without human-labeled signals because whatever signaling scheme is

801
01:33:51,680 --> 01:33:58,240
used will be learned by the level above. The operator above that says, when I get lots of

802
01:33:58,240 --> 01:34:04,320
blades of grass signals, I reduce all of that to a long signal as I send it upward.

803
01:34:04,320 --> 01:34:10,720
And eventually we reach the higher operator layers and someone there says, we are a face-finder

804
01:34:10,720 --> 01:34:17,440
application. We are completely uninterested in lawns and discards the lawn as non-cellient.

805
01:34:17,440 --> 01:34:24,320
What remains after you discard all non-faces are the faces. You cannot discard anything

806
01:34:24,320 --> 01:34:31,120
until you know what it is, or can at least estimate whether it's worth learning. Specifically,

807
01:34:31,120 --> 01:34:39,040
until you understand it at the level of abstraction you are operating at. The low-level blade of

808
01:34:39,040 --> 01:34:44,800
grass recognizers could not discard the grass because they had no clue about the high-level

809
01:34:44,800 --> 01:34:51,680
saliencies of lawn or not in face or not that the higher layers specialize in. You can only tell

810
01:34:51,680 --> 01:34:58,000
what salient or not, important or not at the level of understanding and abstraction you are

811
01:34:58,000 --> 01:35:05,280
operating at. Each layer receives lower-level descriptions from below, discards what it

812
01:35:05,280 --> 01:35:11,840
recognizes as irrelevant, and sends its own version of higher-level descriptions upward

813
01:35:11,840 --> 01:35:17,760
until we reach someone who knows what we are really looking for. This is of course why deep

814
01:35:17,760 --> 01:35:26,640
learning is deep. This idea itself is not new. It was discussed by Oliver Selfridge in 1959.

815
01:35:26,640 --> 01:35:33,920
He described an idea called, Pandemonium, which was largely ignored by the AI community because of

816
01:35:33,920 --> 01:35:40,480
its radical departure from the logic-based AI promoted by people like John McCarthy and Marvin

817
01:35:40,480 --> 01:35:48,400
Minsky. But Pandemonium presaged, by almost 60 years, the layer-by-layer architecture with

818
01:35:48,400 --> 01:35:55,120
signals passing up and down that is used today in all deep neural networks. This is the reason my

819
01:35:55,120 --> 01:36:02,720
online handle is at Pandemonica. So do any TensorFlow operators support this reduction?

820
01:36:03,360 --> 01:36:11,040
Let's start by examining the pooling operators. There are a few in the diagram. They are conceptually

821
01:36:11,040 --> 01:36:18,000
simple. There are over 50 pooling operators in TensorFlow. There is an operator named

822
01:36:18,000 --> 01:36:27,040
2x2 Max Pool operator. In the diagram, it is used four times. It is given four inputs with

823
01:36:27,040 --> 01:36:34,000
varying values and propagates the highest value of those as its only output. Close to the input

824
01:36:34,000 --> 01:36:39,680
layer of these four values may be four adjacent pixels where their values might be a brightness

825
01:36:39,680 --> 01:36:47,440
in some color channel, but higher up they mean whatever they mean. In effect, the Max Pool 2x2

826
01:36:47,440 --> 01:36:55,680
discards the least important 75% of its input data, preserving and propagating only one

827
01:36:55,680 --> 01:37:04,000
highest value. In the case of pixels, it might mean the brightest color value. In the case of blades

828
01:37:04,000 --> 01:37:11,200
of grass, it might mean there is at least one blade of grass here. The interpretation of what is

829
01:37:11,200 --> 01:37:19,200
discarded depends on the layer, because in a very real sense, layers represent levels of reduction,

830
01:37:19,200 --> 01:37:26,400
abstraction levels, if you prefer that term. And we should now be clearly seeing one of the most

831
01:37:26,400 --> 01:37:32,640
important ideas in deep neural networks, the reduction has to be done at multiple levels

832
01:37:32,640 --> 01:37:39,360
of abstraction. Each set of decisions about what is reduced away as irrelevant and what is kept as

833
01:37:39,360 --> 01:37:46,480
possibly relevant can only be made at an appropriate abstraction level. We cannot yet abstract away

834
01:37:46,480 --> 01:37:53,120
the lawn if all we know is there are dark and light green areas levels. This is a simplification.

835
01:37:53,680 --> 01:37:59,840
Decisions made in this manner will be heated only if they have contributed to positive outcomes in

836
01:37:59,840 --> 01:38:06,880
learning. Unreliable and useless decision makers will be ignored using any of several mechanisms

837
01:38:06,880 --> 01:38:15,040
that we may apply during learning. More later, for now, we continue by examining the most popular

838
01:38:15,040 --> 01:38:22,800
subset of all TensorFlow operators. The convolution family from the TensorFlow manual,

839
01:38:22,800 --> 01:38:28,960
note that although these ops are called convolution, they are strictly speaking cross

840
01:38:28,960 --> 01:38:35,840
correlation. Convolution layers discover cross correlations and co-occurrences of various kinds.

841
01:38:35,840 --> 01:38:42,640
Co-occurrences to known patterns in the image at various locations. Spatial relationships

842
01:38:42,640 --> 01:38:49,200
within an image itself, like Jeff Hinton's recent example of the mouth normally being found below

843
01:38:49,200 --> 01:38:56,560
the nose. And more obviously, in the supervised learning case, correlations between discovered

844
01:38:56,560 --> 01:39:02,800
patterns and the available meta-information, tags, labels that correlate with the patterns

845
01:39:02,800 --> 01:39:09,360
the system may discover. This is what allows an image-understander to tag the occurrence of a

846
01:39:09,360 --> 01:39:16,720
nose in an image with the text string nose. Beyond this, such systems may learn to understand

847
01:39:16,720 --> 01:39:24,320
concepts like behind and under. The information that is propagated to the higher levels in the

848
01:39:24,320 --> 01:39:31,440
network now describes these correlations. Uncorrelated information is viewed as non-salient

849
01:39:31,440 --> 01:39:38,880
and is discarded. In the Crescent diagram, this discarding is done by a max pooling layer after

850
01:39:38,880 --> 01:39:46,560
the convolution plus ReLU layers. ReLU is a kind of layer operator that discards negative values,

851
01:39:46,560 --> 01:39:53,120
introducing a non-linearity that is important for DL but not really important for our analysis.

852
01:39:53,120 --> 01:40:01,440
This pattern of three layers, convolution, then ReLU, then a pooling layer, is quite popular

853
01:40:01,440 --> 01:40:07,760
because this combination is performing one reliable reduction step. These three-layer types

854
01:40:07,760 --> 01:40:15,120
in this packaged sequence may appear many times in a DL computational graph. In each of these

855
01:40:15,120 --> 01:40:21,200
three-layer packages is reducing away things that levels below had no chance of evaluating

856
01:40:21,200 --> 01:40:28,160
for saliency because they didn't understand their input at the correct level. Again,

857
01:40:28,160 --> 01:40:34,560
this is why deep learning is deep because you can only do reduction by discarding the irrelevant

858
01:40:34,560 --> 01:40:41,280
if you understand what is relevant and irrelevant at each different level of abstraction. Is

859
01:40:41,280 --> 01:40:48,000
deep learning science or not? While the deep learning process can be described using mathematical

860
01:40:48,000 --> 01:40:56,480
notation, mostly using linear algebra, the process itself isn't scientific. We cannot explain how

861
01:40:56,480 --> 01:41:03,360
this system is capable of forming any kind of understanding by just staring at these equations,

862
01:41:03,360 --> 01:41:08,480
since understanding is an emergent effect of repeated reductions over many layers.

863
01:41:08,480 --> 01:41:17,680
Consider the convolution operators. As the TF manual quote clearly states, convolution layers discover

864
01:41:17,680 --> 01:41:25,920
correlations. Many blades of grass together typically means a lawn. In TF, a lot of cycles

865
01:41:25,920 --> 01:41:32,320
are spent on discovering these correlations. Once found, the correlation leads to some

866
01:41:32,320 --> 01:41:37,760
adjustments of some way to make the correct reduction more likely to be rediscovered

867
01:41:37,760 --> 01:41:43,280
the next round, because this reduction is done multiple times. But in essence,

868
01:41:43,280 --> 01:41:48,640
all correlations are forgotten and have to be rediscovered in every path through the deep

869
01:41:48,640 --> 01:41:54,160
learning loop of upward signaling and downward gradient descent with minute adjustments to

870
01:41:54,160 --> 01:42:01,200
erring variables. This system is in effect learning from its mistakes, which is a good sign,

871
01:42:01,200 --> 01:42:06,560
since that may well be the only way to learn anything. At least at these levels.

872
01:42:06,560 --> 01:42:13,840
This up and down may be repeated many times for each image in the learning set. This up and down

873
01:42:13,840 --> 01:42:20,400
makes some sense for image understanding. Some are using the same algorithms for text.

874
01:42:21,120 --> 01:42:27,680
Fortunately, in the text case, there are very efficient alternatives to this ridiculously

875
01:42:27,680 --> 01:42:35,200
expensive algorithm. For starters, we can represent the discovered correlations explicitly,

876
01:42:35,200 --> 01:42:41,520
using regular pointers or object references in our programming languages.

877
01:42:42,320 --> 01:42:50,320
Or, synapses in brains. This software neuron correlates with that software neuron says a

878
01:42:50,320 --> 01:42:56,800
synapse or reference connecting this to that. We shall discuss such systems in the section on

879
01:42:56,800 --> 01:43:03,760
organic learning, which is coming up next. Then either the deep learning family of algorithms,

880
01:43:03,760 --> 01:43:10,480
or organic learning, are scientific in any meaningful way. They jump to conclusions on

881
01:43:10,480 --> 01:43:17,600
scant evidence and trust correlations without insisting on provable causality. This is disallowed

882
01:43:17,600 --> 01:43:23,280
in scientific theory, where absolutely reliable causality is the coin of the realm.

883
01:43:24,000 --> 01:43:30,960
F equals m a or go home. The most deep neural network programming is uncomfortably close to

884
01:43:30,960 --> 01:43:37,840
trial and error, with only minor clues about how to improve the system when reaching mediocre results.

885
01:43:38,480 --> 01:43:45,120
Adding more layers doesn't always help. These kinds of problems are the everyday reality to

886
01:43:45,120 --> 01:43:52,080
most practitioners of deep neural networks. With no a priori models, there will be no a priori

887
01:43:52,080 --> 01:43:58,480
guarantees. The best estimate of the reliability and correctness of any deep neural network,

888
01:43:58,480 --> 01:44:04,560
or even any holistic system we can ever devise, is going to be extensive testing.

889
01:44:05,200 --> 01:44:11,440
We're on this later. Why would we ever use engineered systems that cannot be guaranteed

890
01:44:11,440 --> 01:44:18,880
to provide the correct answer? Because we have no choice. We only use holistic methods when the

891
01:44:18,880 --> 01:44:25,920
reliable reductionist methods are unavailable. As is the case when the task requires the ability

892
01:44:25,920 --> 01:44:32,320
to perform autonomous reduction of context rich slices of our rich complex reality as a whole.

893
01:44:33,040 --> 01:44:39,840
When the task requires understanding, don't we have an alternative to these under liable machines?

894
01:44:40,480 --> 01:44:47,520
Sure we do. There are billions of humans on the planet that are already masters of this complex

895
01:44:47,520 --> 01:44:54,800
task because they live in the rich world and need skills that are unavailable with reductionist methods,

896
01:44:54,800 --> 01:45:01,120
starting with low level things like object permanence. So you can replace a well performing

897
01:45:01,120 --> 01:45:07,680
but theoretically unproven contraption, a holistic understanding machine built out of deep neural

898
01:45:07,680 --> 01:45:14,080
networks, with a well performing human being using a deeply mystical kind of understanding

899
01:45:14,080 --> 01:45:20,800
hidden in their opaque heads. Who earns much more per hour. This doesn't look like much of an

900
01:45:20,800 --> 01:45:27,600
improvement. The machine cannot be proven correct because it doesn't function like normal computers.

901
01:45:28,160 --> 01:45:36,000
It is performing reduction, the skill formally restricted to animals. A holistic skill. My

902
01:45:36,000 --> 01:45:43,280
favorite soundbite is a mere corollary to the frame problem by McCarthy and Hayes. You have seen

903
01:45:43,280 --> 01:45:49,280
it and you will see it again, since it is one of the stronger results of AI epistemology.

904
01:45:49,280 --> 01:45:56,560
But we will, in but a few years, agree on a definition of intelligence that makes autonomous

905
01:45:56,560 --> 01:46:04,240
reduction a requirement. This once semi-heretic soundbite will then be obvious to all. If it

906
01:46:04,240 --> 01:46:13,600
isn't already, our intelligences are fallible. Chapter 6. Experimental Epistemology for AI

907
01:46:13,600 --> 01:46:21,040
We can now create computer based experimental implementations to epistemology level theories

908
01:46:21,040 --> 01:46:28,160
in order to test them and learn from the outcomes. Experimental epistemology is the use of the

909
01:46:28,160 --> 01:46:34,640
experimental methods of the cognitive sciences to shed light on debates within epistemology,

910
01:46:34,640 --> 01:46:42,080
the philosophical study of knowledge and rationally justified belief. Some skeptics contend that

911
01:46:42,080 --> 01:46:49,600
experimental epistemology or experimental philosophy more generally is an oxymoron.

912
01:46:50,320 --> 01:46:57,600
If you are doing experiments, they say, you are not doing philosophy. You are doing psychology

913
01:46:57,600 --> 01:47:04,080
or some other scientific activity. It is true that the part of experimental philosophy that is

914
01:47:04,080 --> 01:47:10,240
devoted to carrying out experiments and performing statistical analyses on the data obtained is

915
01:47:10,240 --> 01:47:17,600
primarily a scientific rather than a philosophical activity. However, because the experiments are

916
01:47:17,600 --> 01:47:24,560
designed to shed light on debates within philosophy, the experiments themselves grow out of mainstream

917
01:47:24,560 --> 01:47:30,480
philosophical debate and their results are injected back into the debate, with an item

918
01:47:30,480 --> 01:47:37,040
moving the debate forward. This part of experimental philosophy is indeed philosophy,

919
01:47:37,040 --> 01:47:44,960
not philosophy as usual perhaps, but philosophy nonetheless. Experimental epistemology by James

920
01:47:44,960 --> 01:47:51,680
R. B. B. Traditional experimental epistemology conducted experiments on interviews and psychological

921
01:47:51,680 --> 01:47:58,640
tests on human volunteers or relied on population statistics. As one of the newer branches of

922
01:47:58,640 --> 01:48:04,560
cognitive science, machine learning has now provided us with a very different approach

923
01:48:04,560 --> 01:48:11,440
to this domain. We can now create computer-based experimental implementations to epistemology

924
01:48:11,440 --> 01:48:17,840
level theories in order to test them and learn from the outcomes. In machine learning, the most

925
01:48:17,840 --> 01:48:24,400
important epistemology level concepts and hypotheses are about reasoning, understanding,

926
01:48:24,400 --> 01:48:32,480
learning, epistemic reduction, abstraction, creativity, prediction, attention, instincts,

927
01:48:32,480 --> 01:48:40,480
intuitions, concepts, resiliency, models, reductionism, wholism, and other things all

928
01:48:40,480 --> 01:48:49,040
sharing these features. One, science has no equations, formulas, or other models for how

929
01:48:49,040 --> 01:48:58,080
they work. They're epistemology level concepts, not science level concepts. Two, our theories

930
01:48:58,080 --> 01:49:04,720
about these concepts have to be sufficiently solid and detailed to allow for computer implementations.

931
01:49:05,440 --> 01:49:12,480
This is because science itself is built on top of epistemology level concepts, and practitioners

932
01:49:12,480 --> 01:49:18,480
need to be aware of this or they will experience cognitive dissonance-induced confusion and stress.

933
01:49:19,120 --> 01:49:24,720
The red pill of machine learning confronts the elephant in the room of machine learning.

934
01:49:24,720 --> 01:49:32,160
Machine learning is not scientific. What can we learn from AI epistemology? An excerpt from the

935
01:49:32,160 --> 01:49:39,040
red pill can say the following statements from the domain of epistemology and how each of them

936
01:49:39,040 --> 01:49:45,760
can be viewed as an implementation hint for AI designers. We are already able to measure

937
01:49:45,760 --> 01:49:52,000
their effects and system competence. You can only learn that which you already almost know.

938
01:49:52,000 --> 01:50:01,200
Patrick Winston, MIT. Our intelligences are fallible. Monica Anderson. In order to detect

939
01:50:01,200 --> 01:50:08,080
that something is new, you need to recognize everything old. Monica Anderson. You cannot

940
01:50:08,080 --> 01:50:15,120
reason about that which you do not understand. Monica Anderson. You are known by the company

941
01:50:15,120 --> 01:50:22,400
you keep, simple version of the Yanida Lemur from Category Theory and the justification for embeddings

942
01:50:22,400 --> 01:50:28,800
in deep learning. All useful novelty in the universe is due to processes of variation and

943
01:50:28,800 --> 01:50:37,200
selection. The selectionist manifesto. Selectionism is the generalization of Darwinism. This is

944
01:50:37,200 --> 01:50:45,920
right genetic algorithms work. Science has no equations for concepts like understanding, reasoning,

945
01:50:45,920 --> 01:50:51,760
learning, abstraction, or modeling since they are all epistemology level concepts.

946
01:50:52,400 --> 01:50:59,280
We cannot even start using science until we have decided what model to use. We must use our

947
01:50:59,280 --> 01:51:05,840
experience to perform epistemic reductions, discarding the irrelevant, starting from the messy

948
01:51:05,840 --> 01:51:12,800
real world problem situation until we are left with a scientific model we can use, such as an

949
01:51:12,800 --> 01:51:20,160
equation. The focus in AI research should be on exactly how we can get our machines to perform

950
01:51:20,160 --> 01:51:27,680
this pre-scientific epistemic reduction by themselves and the answer to that cannot be found inside

951
01:51:27,680 --> 01:51:36,000
of science. Chapter 7. The Red Pill of Machine Learning. Reductionism is the use of models.

952
01:51:36,640 --> 01:51:45,440
Holism is the avoidance of models. Models are scientific models, theories, hypotheses, formulas,

953
01:51:45,440 --> 01:51:52,480
equations, naive models based on personal experiences, superstitions, and traditional

954
01:51:52,480 --> 01:52:00,800
computer programs. The deep learning revolution of 2012 changed how we think about artificial

955
01:52:00,800 --> 01:52:08,080
intelligence, machine learning, and deep neural networks. What changed, and what does this mean

956
01:52:08,080 --> 01:52:14,800
going forward? The new cognitive capabilities in our machines are the result of a shift in the way

957
01:52:14,800 --> 01:52:21,600
we think about problem solving. It is the most significant change ever in artificial intelligence

958
01:52:21,600 --> 01:52:29,360
AI, if not in science as a whole. Machine learning, ML based systems are successfully

959
01:52:29,360 --> 01:52:36,000
attacking both simple and complex problems using novel methods that only became available after

960
01:52:36,000 --> 01:52:43,760
2012. We are experiencing a revolution at the level of epistemology which will affect much more

961
01:52:43,760 --> 01:52:50,080
than just the field of machine learning. We want to add more of these novel methods to our

962
01:52:50,080 --> 01:52:56,480
standard problem solving toolkit, but we need to understand the trade-offs and the conflict.

963
01:52:57,040 --> 01:53:04,240
I argue that understanding deep neural networks, DNNs, and other ML technologies requires that

964
01:53:04,240 --> 01:53:11,040
practitioners adopt a holistic stance which is, at important levels, blatantly incompatible with

965
01:53:11,040 --> 01:53:17,760
the reductionist stance of modern science. As ML practitioners we have to make hard choices

966
01:53:17,760 --> 01:53:24,160
that seemingly contradict many of our core scientific convictions. As a result we may get

967
01:53:24,160 --> 01:53:30,960
the feeling something is wrong. The conflict is real and important and the seemingly counter-intuitive

968
01:53:30,960 --> 01:53:38,160
choices make sense only when viewed in the light of epistemology. Improved clarity in these matters

969
01:53:38,160 --> 01:53:43,760
should alleviate the cognitive dissonance experienced by some ML practitioners and should

970
01:53:43,760 --> 01:53:49,680
accelerate progress in these fields. The title refers to the eye-opening clarity

971
01:53:49,680 --> 01:53:56,400
some machine learning practitioners achieve when adopting a holistic stance. Parallel dichotomies

972
01:53:56,400 --> 01:54:03,520
sentient sync research is natural language understanding, NLU. We are creating novel

973
01:54:03,520 --> 01:54:10,000
systems that allow computers to learn to understand human natural languages. Any one of them,

974
01:54:10,000 --> 01:54:17,440
we use deep neural networks of our own design. The goal is to achieve some kind of human-like

975
01:54:17,440 --> 01:54:24,000
but not necessarily human-level understanding. This is very different from traditional natural

976
01:54:24,000 --> 01:54:31,840
language processing, NLP, which relies on human-made models of some language, such as English,

977
01:54:31,840 --> 01:54:38,800
and perhaps models of fragments of the world. The NLP and NLU disciplines have chosen

978
01:54:38,800 --> 01:54:45,440
opposite answers to their difficult two-way choices. They are now defined by these choices,

979
01:54:45,440 --> 01:54:51,520
and we can use their stances to highlight the main conflict. The split is so deep

980
01:54:51,520 --> 01:54:58,080
that it cuts through many layers of our reality. The following dichotomies are all manifestations

981
01:54:58,080 --> 01:55:05,360
of this incompatibility at different levels, listed by impact, but discussed in no particular order.

982
01:55:05,360 --> 01:55:15,280
The main science, the complex, including the mundane, epistemology, reductionism,

983
01:55:16,000 --> 01:55:27,040
realism, meanings, reasoning, understanding, problem solving, plan it, then do it, just do it.

984
01:55:27,680 --> 01:55:34,960
Artificial intelligence, 20th century, good, old-fashioned AI machine learning,

985
01:55:34,960 --> 01:55:44,720
deep neural networks, natural language and computers, NLP, NLU. The problem-solving level

986
01:55:44,720 --> 01:55:52,080
provides many familiar examples of these issues. In our mundane lives, we solve many kinds of

987
01:55:52,080 --> 01:55:58,400
problems every day but our strategies for solving them fall into just those two categories.

988
01:55:58,400 --> 01:56:06,720
For any complicated problem, we had better have a plan before we start, but most problems

989
01:56:06,720 --> 01:56:13,920
the brain deals with every day are things we never have to think about because we do not need to plan

990
01:56:13,920 --> 01:56:20,000
a reason about them. These are the millions of low-level problems we encountered in our

991
01:56:20,000 --> 01:56:25,840
mundane life every day, and this is the world that our AIs will have to operate in.

992
01:56:25,840 --> 01:56:33,280
Consider someone walking across the floor. Their brain signals their leg muscles to contract in

993
01:56:33,280 --> 01:56:40,240
the correct cadence. Do they need to consciously plan each step? Do they reason about how to

994
01:56:40,240 --> 01:56:46,880
maintain their balance? No. They probably don't even know what leg muscles they have.

995
01:56:47,600 --> 01:56:54,160
Consider understanding this sentence. Did you use reasoning? Did you use grammar?

996
01:56:54,160 --> 01:57:00,640
If you are a fluent speaker, you do not need grammars to understand or produce language,

997
01:57:01,280 --> 01:57:08,400
and you do not have time to reason about language while hearing it spoken. Reasoning is slow,

998
01:57:08,400 --> 01:57:14,240
but understanding is instantaneous. Consider someone braking for a stoplight.

999
01:57:14,960 --> 01:57:21,440
How hard should they push on the brake pedal? Do they compute the required differential equation?

1000
01:57:21,440 --> 01:57:25,840
Should such equations be part of the driver's license test?

1001
01:57:26,560 --> 01:57:33,520
Consider someone making breakfast. Did they have to reason about anything or plan anything,

1002
01:57:33,520 --> 01:57:40,240
or did they just do what worked yesterday, without thinking about it? Without consciously planning

1003
01:57:40,240 --> 01:57:48,240
it? Walking and talking, braking and breakfasting, like almost everything we use our brains for,

1004
01:57:48,240 --> 01:57:54,480
rely on learning from our experiences in order to reuse anything that has worked in the past,

1005
01:57:55,120 --> 01:58:03,040
and, over time, we learn to correct our mistakes. These strategies are simple enough that we can

1006
01:58:03,040 --> 01:58:09,360
identify them in other life forms. Dogs understand a lot but do not reason much,

1007
01:58:09,920 --> 01:58:16,240
and we can see how they could be implemented in something like neurons and brains. The split

1008
01:58:16,240 --> 01:58:22,960
in our brains between reasoning and understanding was examined at length in Thinking Fast and Slow

1009
01:58:22,960 --> 01:58:29,600
by Daniel Kahneman. The absolute majority of the brain's effort is spent processing low-level

1010
01:58:29,600 --> 01:58:37,760
sensory input, mostly from the eyes. He calls this System 1. It provides understanding.

1011
01:58:38,400 --> 01:58:43,920
Reasoning is done by System 2 based on the understanding from System 1.

1012
01:58:43,920 --> 01:58:50,080
What most problems we deal with on a daily basis do not require System 2 at all.

1013
01:58:50,720 --> 01:58:56,320
Artificial intelligence and machine learning computers can solve any suitable problem when

1014
01:58:56,320 --> 01:59:03,040
given sufficient human help, such as a complete plan for the solution in the form of a computer

1015
01:59:03,040 --> 01:59:11,440
program and valid input data. But since the AIML Revolution of 2012, we now know how to make

1016
01:59:11,440 --> 01:59:18,240
computers understand certain problem domains through machine learning. The acquired understanding

1017
01:59:18,240 --> 01:59:25,920
allows the machine to just do it for many different problems in the domain, without any human planning,

1018
01:59:25,920 --> 01:59:34,320
reasoning, or programming, and using incomplete, unreliable, and noisy input data. This is

1019
01:59:34,320 --> 01:59:42,000
changing how we are building systems with cognitive capabilities. Everyone working in ML or AI needs

1020
01:59:42,000 --> 01:59:48,880
to understand the trade-offs we must make at the most fundamental, epistemological levels.

1021
01:59:49,520 --> 01:59:55,760
Modern ML requires examining and seriously rethinking many things we were taught to vigilantly

1022
01:59:55,760 --> 02:00:05,040
strive for in our science, technology, engineering, and mathematics STEM educations. Things like

1023
02:00:05,040 --> 02:00:12,400
correlation is bad, but causality is good and do not jump to conclusions on scant evidence

1024
02:00:12,400 --> 02:00:19,200
are still solid advice everywhere inside science. But when building understanding systems,

1025
02:00:19,200 --> 02:00:26,000
these established strategies and modes of thinking no longer work, because correlation discovery and

1026
02:00:26,000 --> 02:00:33,520
handling of sparse, unreliable, and inconsistent input data are exactly the kinds of tasks we will

1027
02:00:33,520 --> 02:00:42,240
have to perform and perform well at these pre-scientific levels. In order to understand how to do this,

1028
02:00:42,240 --> 02:00:49,600
we must switch to a holistic stance. A motivating example, beginning machine learning students

1029
02:00:49,600 --> 02:00:56,720
are given exercises like this. They are given a large spreadsheet, which lists data about houses

1030
02:00:56,720 --> 02:01:03,600
sold a certain year in the US. This information includes among other things, the zip code of the

1031
02:01:03,600 --> 02:01:10,720
house, the living area and square feet, lot size, the number of bedrooms and bathrooms,

1032
02:01:10,720 --> 02:01:17,600
the year the house was built, and the final sale price of the house. We would like to be able to

1033
02:01:17,600 --> 02:01:24,560
predict this final sale price, given the corresponding data for current house we are about to list for

1034
02:01:24,560 --> 02:01:30,960
sale. The given spreadsheet is the data the student will use to train a deep neural network.

1035
02:01:31,520 --> 02:01:37,200
It is the entire learning corpus. It contains everything the system will ever know.

1036
02:01:37,200 --> 02:01:45,440
These students can download deep neural network libraries like Keras and TensorFlow and runnable

1037
02:01:45,440 --> 02:01:53,280
examples for many kinds of problems, including useful training data from places like Hugging Face

1038
02:01:53,280 --> 02:02:01,600
and GitHub. Next the student trains, learns their network using the given data. This may take a while,

1039
02:02:01,600 --> 02:02:08,400
but when learning finishes, they can give the system data for a house it has never seen and

1040
02:02:08,400 --> 02:02:15,040
it will quite reliably predict what the house might sell for. This was the goal of the exercise.

1041
02:02:15,680 --> 02:02:22,240
The student has created a system that understands how to estimate real estate prices from listings,

1042
02:02:22,880 --> 02:02:29,680
but the student still does not understand anything about real estate. The predictive capability

1043
02:02:29,680 --> 02:02:36,160
that many people working in real estate would be willing to pay money for is 100% based on

1044
02:02:36,160 --> 02:02:42,880
understanding in the deep neural network, in the computer, and because all the libraries in many

1045
02:02:42,880 --> 02:02:49,520
pre-sold examples of this nature were freely available, the student did not have to do much

1046
02:02:49,520 --> 02:02:58,880
programming either. The vision. This is desirable. This is what AI should mean. The computer understands

1047
02:02:58,880 --> 02:03:05,840
the problem so that we don't have to. Programming in the future will be like having a conversation

1048
02:03:05,840 --> 02:03:12,400
with a competent coworker, and when the machine understands exactly what we want done, it will

1049
02:03:12,400 --> 02:03:20,080
simply do it. No programming required on our part or on part of the machine, once a suitable,

1050
02:03:20,080 --> 02:03:27,360
partially reductionist framework exists. The rest is learning and it can be done in any human

1051
02:03:27,360 --> 02:03:34,080
language with equal ease. We are on the right track towards something worthy of the name AI

1052
02:03:34,080 --> 02:03:40,960
with current machine learning. Going forward, there are thousands of paths to choose from,

1053
02:03:40,960 --> 02:03:47,360
and the ability to choose wisely will depend on our ability to understand and adopt a holistic

1054
02:03:47,360 --> 02:03:54,480
stance. Reductionism and Holism. These are important terms of the art in epistemology.

1055
02:03:54,480 --> 02:04:02,320
Both of them have numerous correct, useful, and compatible definitions. We will henceforth

1056
02:04:02,320 --> 02:04:09,360
use the following definitions for reasons of usefulness and simplicity. Reductionism is the

1057
02:04:09,360 --> 02:04:19,040
use of models. Holism is the avoidance of models. Models are scientific models, theories, hypotheses,

1058
02:04:19,040 --> 02:04:27,040
formulas, equations, naive models based on personal experiences, superstitions, if you can

1059
02:04:27,040 --> 02:04:34,640
believe that, and traditional computer programs. In the reductionist paradigm, these models are

1060
02:04:34,640 --> 02:04:42,800
created by humans, ostensibly by scientists, and are then used, ostensibly by engineers,

1061
02:04:42,800 --> 02:04:49,600
to solve real world problems. Model creation and model use both require that these humans

1062
02:04:49,600 --> 02:04:57,120
understand the problem domain, the problem at hand, the previously known shared models available,

1063
02:04:57,120 --> 02:05:04,400
and how to design and use models. A PhD degree could be seen as a formal license to create new

1064
02:05:04,400 --> 02:05:12,720
models. Mathematics can be seen as a discipline for model manipulation. But now, by avoiding the

1065
02:05:12,720 --> 02:05:20,560
use of human-made models and switching to holistic methods, data scientists, programmers, and others

1066
02:05:20,560 --> 02:05:28,080
do not themselves have to understand the problems they are given. They are no longer asked to provide

1067
02:05:28,080 --> 02:05:34,720
a computer program or to otherwise solve a problem in a traditional reductionist or scientific way.

1068
02:05:35,440 --> 02:05:42,000
Holistic systems like DNNs can provide solutions to many problems by first learning about the

1069
02:05:42,000 --> 02:05:49,360
domain from data-insult examples, and then, in production, to match new situations to this

1070
02:05:49,360 --> 02:05:56,000
gathered experience. These matches are guesses, but with sufficient learning, the results can be

1071
02:05:56,000 --> 02:06:03,440
highly reliable. We will initially use computer-based holistic methods to solve individual and specific

1072
02:06:03,440 --> 02:06:11,520
problems, such as self-driving cars. Over time, increasing numbers of artificial understanders

1073
02:06:11,520 --> 02:06:18,320
will be able to provide immediate answers, guesses, to wider and wider ranges of problems.

1074
02:06:19,040 --> 02:06:24,720
We can expect to see cell phone apps with such good command of language that it feels like

1075
02:06:24,720 --> 02:06:31,120
talking to a competent co-worker. Voice will become the preferred way to interact with our

1076
02:06:31,120 --> 02:06:38,880
personal AIs. Early and low-level but useful AI will manifest as computers that can solve problems

1077
02:06:38,880 --> 02:06:45,520
we ourselves cannot or cannot be bothered to solve. They need not be superhuman.

1078
02:06:46,080 --> 02:06:52,400
All they need to have in order to be extremely useful is exactly the ability to autonomously

1079
02:06:52,400 --> 02:06:59,040
discover higher-level abstractions in some given problem domain, starting from low-level sensory

1080
02:06:59,040 --> 02:07:06,320
input, for example, by learning from images or reading books. Such systems now exist.

1081
02:07:06,320 --> 02:07:12,560
If we want to understand machine learning, then we need to understand all the strategies in the

1082
02:07:12,560 --> 02:07:19,040
right most column in the tables that follow. They are all part of a holistic stance, and if we are

1083
02:07:19,040 --> 02:07:26,000
working in machine learning, we need to adopt as many of them as possible. Differences at the level

1084
02:07:26,000 --> 02:07:35,600
of epistemology. Reductionism in Science versus Holism in Machine Learning. The use of

1085
02:07:35,600 --> 02:07:46,320
models versus the avoidance of models. Raising versus understanding requires human understanding

1086
02:07:46,320 --> 02:07:54,800
versus provides human-like understanding. Problems are solved in an abstract model space versus

1087
02:07:54,800 --> 02:08:01,520
problems are solved directly in the problem domain. Unbeatable strategy for dealing with

1088
02:08:01,520 --> 02:08:08,080
a wide range of suitable problems faced by humans. Versus may handle some problems in

1089
02:08:08,080 --> 02:08:14,560
domains where reductionist models cannot be created or used, known as bizarre domains.

1090
02:08:15,840 --> 02:08:21,760
Handles many important complicated problems such as going to the moon or a highway system.

1091
02:08:22,400 --> 02:08:28,400
Versus handles many important complex problems such as protein folding and playing go.

1092
02:08:28,400 --> 02:08:37,840
Handles problems requiring planning or cooperation. Versus handles simple mundane problems such as

1093
02:08:37,840 --> 02:08:45,360
understanding language or vision or making breakfast. Money rows in these tables discuss

1094
02:08:45,360 --> 02:08:52,560
hard trade-offs where compromises are impossible or prohibitively expensive. These are identified

1095
02:08:52,560 --> 02:08:59,120
by bold face numbers in the first column. The meaning rows may not be clear trade-offs or even

1096
02:08:59,120 --> 02:09:06,640
disjoint alternatives. Mixed systems are described in a separate chapter. These form the core of

1097
02:09:06,640 --> 02:09:12,240
these dichotomies and are discussed in most of what follows, but also in detail at the

1098
02:09:12,240 --> 02:09:19,120
chapter on introducing AI epistemology and in videos of talks. A leather report is based on

1099
02:09:19,120 --> 02:09:26,240
models in meteorology. To solve the problem directly in the problem domain, open a window

1100
02:09:26,240 --> 02:09:34,000
to check if it smells like rain. Reductionism is the greatest invention our species has ever made.

1101
02:09:34,640 --> 02:09:40,480
But reductionist models cannot be created or used when any one of the multitudes of blocking

1102
02:09:40,480 --> 02:09:48,560
issues are present. Models work, in theory or in a laboratory where we can isolate a device,

1103
02:09:48,560 --> 02:09:57,120
organism or phenomenon from a changing environment. However, complex situations may involve tracking

1104
02:09:57,120 --> 02:10:03,360
and responding to a large number of conflicting and unreliable signals from a constantly changing

1105
02:10:03,360 --> 02:10:10,000
world or environment. Reductionism is here at a severe disadvantage and can rarely perform

1106
02:10:10,000 --> 02:10:16,560
above the level of statistical models. In contrast, holistic machine learning methods

1107
02:10:16,560 --> 02:10:22,400
learning from unfiltered inputs can discover correlations that humans might miss and can

1108
02:10:22,400 --> 02:10:28,800
construct internal pattern-based structures to provide recognition, epistemic reduction,

1109
02:10:28,800 --> 02:10:36,880
abstraction, prediction, noise rejection and other cognitive capabilities. Humans generally

1110
02:10:36,880 --> 02:10:44,080
use holistic methods for seemingly simple, but in reality, complex mundane problems

1111
02:10:44,080 --> 02:10:50,720
like understanding vision, human language, learning to walk, or making breakfast.

1112
02:10:51,360 --> 02:10:58,960
Computers use them for very complex problems and mel-based AI in general, such as protein folding

1113
02:10:58,960 --> 02:11:06,000
and playing go, but also simpler ones, such as real estate pricing. Main trade-offs.

1114
02:11:06,000 --> 02:11:15,600
Reductionism in science versus realism in machine learning. Optimality, the best answer,

1115
02:11:15,600 --> 02:11:24,880
versus economy, reuse no useful answers. Completeness, all answers, versus promptness,

1116
02:11:24,880 --> 02:11:33,600
except first use for a answer. Repeatability, same answer every time, versus learning,

1117
02:11:33,600 --> 02:11:42,960
versus learning, results improve with practice. Extrapolation, in low-dimensionality domains,

1118
02:11:42,960 --> 02:11:52,160
versus interpolation, even in high-dimensionality domains. Transparency, understand the process

1119
02:11:52,160 --> 02:11:59,280
to get the answer, versus intuition, accept useful answers even if achieved by unknown or

1120
02:11:59,280 --> 02:12:08,880
subconscious means. Explainability, understand the answer, versus positive ignorance, no need to

1121
02:12:08,880 --> 02:12:16,800
even understand the problem or problem domain. Shareability, abstract models are taught in

1122
02:12:16,800 --> 02:12:24,960
communicated using language or software, versus copyability. ML understanding, a competence

1123
02:12:24,960 --> 02:12:33,200
can be copied as a memory image. Optimality, completeness, and repeatability are only available

1124
02:12:33,200 --> 02:12:40,320
in theoretical model spaces and sometimes under laboratory conditions. Economy and promptness

1125
02:12:40,320 --> 02:12:46,160
had much higher survival value in evolutionary history than optimality and completeness.

1126
02:12:46,800 --> 02:12:52,960
The strongest hint that a system is holistic is that the results improve with practice because

1127
02:12:52,960 --> 02:12:59,840
the system learns from its mistakes. In machine learning, a larger learning corpus is in general

1128
02:12:59,840 --> 02:13:06,480
better than the smaller one because it provides more opportunities for making mistakes to learn from,

1129
02:13:06,480 --> 02:13:13,680
such as corner cases. Models created by humans have manageable numbers of parameters because

1130
02:13:13,680 --> 02:13:19,280
the scientist or engineer working on the problem has done a, hopefully correct,

1131
02:13:19,280 --> 02:13:26,880
epistemic reduction from a complex and messy world to a computable model. This allows experimentation

1132
02:13:26,880 --> 02:13:33,600
with what if scenarios by varying model parameters. It is up to the model user to determine which

1133
02:13:33,600 --> 02:13:40,800
extrapolations are reasonable. In holistic ML systems, we are getting used to systems with

1134
02:13:40,800 --> 02:13:47,200
millions or billions of parameters. These structures are very difficult to analyze,

1135
02:13:47,200 --> 02:13:53,280
and just like with human intelligences, the best way to estimate their competence is through

1136
02:13:53,280 --> 02:14:01,120
testing. Extrapolation is typically out of scope for holistic systems. The majority of end users

1137
02:14:01,120 --> 02:14:07,440
will have no interest in how some machine came up with some obviously correct answer. They will

1138
02:14:07,440 --> 02:14:14,800
just accept it the way we accept our own understanding of language, even though we do not know how we

1139
02:14:14,800 --> 02:14:22,720
do it. We now find ourselves asking our machines to solve problems we either don't know how to solve,

1140
02:14:22,720 --> 02:14:30,320
or can't be bothered to figure out how to solve. We have reached a major benefit of AI. We can be

1141
02:14:30,320 --> 02:14:36,240
positively ignorant of many mundane things and will be happy to delegate such matters to our

1142
02:14:36,240 --> 02:14:43,680
machines so that we may play or focus on more important things. Some schools of thought tend to

1143
02:14:43,680 --> 02:14:51,920
overvalue explainability. To them, ML is a serious step down from results obtained scientifically

1144
02:14:51,920 --> 02:14:58,560
where we can all inspect the causality, for instance in a reductionist production, expert

1145
02:14:58,560 --> 02:15:06,000
systems. But the bottom line is that today we can often choose between one, understanding the

1146
02:15:06,000 --> 02:15:14,640
problem domain, problem, the use of science and relevant models, and the answer. Or two, just

1147
02:15:14,640 --> 02:15:20,480
getting a useful answer without even bothering to understand the problem or the problem domain.

1148
02:15:21,120 --> 02:15:28,480
The latter, positive ignorance, is a lot closer to AI than the first, and we can expect the use

1149
02:15:28,480 --> 02:15:35,760
of holistic methods to continue to increase. Science strives towards a consensus world model in order

1150
02:15:35,760 --> 02:15:41,840
to facilitate communication and minimize costly engineering mistakes caused by ignorance and

1151
02:15:41,840 --> 02:15:49,840
misunderstandings. Scientific communication requires a high-level context, a world model,

1152
02:15:49,840 --> 02:15:57,840
shared by participants, and agreed upon signals such as words, math, and software. But direct

1153
02:15:57,840 --> 02:16:04,960
understanding, such as the skills to become a just grandmaster or a downhill skier, cannot be

1154
02:16:04,960 --> 02:16:12,800
shared using words. The experience must be acquired using individual practice. Computer-based systems

1155
02:16:12,800 --> 02:16:18,720
that learn a skill through practice can share the entire understanding so acquired by copying the

1156
02:16:18,720 --> 02:16:24,000
memory content to another machine. Advantages of Holistic Methods

1157
02:16:25,200 --> 02:16:30,000
Reductionism in Science versus Holism in Machine Learning

1158
02:16:30,000 --> 02:16:38,080
N.P. Hard Problems cannot be solved, versus fines-valid solutions by guessing well-based

1159
02:16:38,080 --> 02:16:46,160
on a lifetime of experience. Geigo, garbage in, garbage out is a recognized problem,

1160
02:16:46,800 --> 02:16:51,440
versus copes with missing, erroneous, and misleading inputs.

1161
02:16:51,440 --> 02:16:59,200
Brightness. Experience catastrophic failures at edges of competence, versus anti-fragile.

1162
02:16:59,200 --> 02:17:07,040
Learns from mistakes, especially almost correct guesses in small, correctable failures.

1163
02:17:08,240 --> 02:17:13,600
The models of a constantly changing world are obsolete the moment they are created,

1164
02:17:14,160 --> 02:17:20,080
versus incremental learning provides continuous adaptation to a constantly changing world.

1165
02:17:20,080 --> 02:17:25,360
Algorithms may be incorrect or may be incorrectly implemented,

1166
02:17:26,000 --> 02:17:32,640
versus self-repairing systems can tolerate or correct internal errors. It is because we desire

1167
02:17:32,640 --> 02:17:41,520
certainty, optimality, completeness, etc. L.N.P. Hardness becomes a problem. There are many

1168
02:17:41,520 --> 02:17:47,440
problems where it is relatively easy to find a provably valid solution, but where finding

1169
02:17:47,440 --> 02:17:54,320
all solutions can be very expensive. Real-world traveling salesmen merrily travel long reasonable

1170
02:17:54,320 --> 02:18:01,280
routes. If a reductionist system does not have complete and correct input data, it either cannot

1171
02:18:01,280 --> 02:18:08,000
get started or produces questionable output. But it is an important requirement of real-world

1172
02:18:08,000 --> 02:18:14,160
understanding machines that they be able to detect what is salient, important, in their input in

1173
02:18:14,160 --> 02:18:22,080
order to avoid paying attention to, and learning from, noise. And they have to deal with incomplete,

1174
02:18:22,080 --> 02:18:27,840
erroneous, and misleading input generated by millions of other intelligent agents with

1175
02:18:27,840 --> 02:18:35,360
goals at odds with their own. They need to be able to detect omissions, duplications, errors,

1176
02:18:35,360 --> 02:18:42,800
noise, lies, etc. And the only epistemologically plausible way to do this is to relate the input

1177
02:18:42,800 --> 02:18:50,160
to similar input they have understood in the past, what they already know. They need to understand

1178
02:18:50,160 --> 02:18:57,360
what matters but if they can also understand some of the noise. This is advertising, they can exploit

1179
02:18:57,360 --> 02:19:03,920
that. There are many image and video apps available featuring image understanding based on deep

1180
02:19:03,920 --> 02:19:11,440
learning. These apps can remove backgrounds, sharpen details like eyelashes, restore damaged

1181
02:19:11,440 --> 02:19:19,120
photographs, etc. We need to keep in mind that the ability of holistic systems to fill in data

1182
02:19:19,120 --> 02:19:25,920
and detect noise depends on them having learned from similar data in the past. We note that all

1183
02:19:25,920 --> 02:19:31,920
the image improvements are confabulations based on prior experience from their learning corpora.

1184
02:19:32,560 --> 02:19:39,280
But we can also note that image composition using these methods yields totally seamless images,

1185
02:19:39,280 --> 02:19:47,040
very far from cut and paste of pixels. And quite similarly, we find language confabulation by

1186
02:19:47,040 --> 02:19:54,880
systems like GPT-3 to flow seamlessly between sentences and topics. They have nothing to say,

1187
02:19:54,880 --> 02:20:01,760
but they say it well. However, they bring us closer to meaningful language generation and

1188
02:20:01,760 --> 02:20:08,000
when we achieve that, the public perception of what computers are capable of will totally change.

1189
02:20:08,000 --> 02:20:15,040
Most of cognition is recognition. Being able to recognize that something has occurred before

1190
02:20:15,040 --> 02:20:22,560
and knowing what might happen next has enormous survival value for any animal species. A mature

1191
02:20:22,560 --> 02:20:29,280
human has used their eyes and other senses for decades. This represents an enormous learning

1192
02:20:29,280 --> 02:20:36,880
corpus and they can understand anything they have prior experience of. The mistakes made by humans,

1193
02:20:36,880 --> 02:20:43,840
animals and by holistic ML systems are very often of a near-miss variety which provides an

1194
02:20:43,840 --> 02:20:50,640
opportunity to learn to do a better next time. Contrast is to reductionist software systems

1195
02:20:50,640 --> 02:20:57,120
created for similar goals. Rule-based systems have long been infamous for their brittleness.

1196
02:20:57,760 --> 02:21:03,360
As long as the rules and the rules that match the current input and reality perfectly,

1197
02:21:03,360 --> 02:21:10,320
the results will be useful, repeatable and reliable. But at the edges of their competence,

1198
02:21:10,320 --> 02:21:17,280
where the matches become more tenuous, the quality rapidly drops. Minor mistakes in the

1199
02:21:17,280 --> 02:21:24,160
rule sets in the world modeling may lead such systems to return spectacularly incorrect results.

1200
02:21:24,880 --> 02:21:30,400
Sometimes repeatability is important and sometimes tracking a changing world by

1201
02:21:30,400 --> 02:21:37,600
continuously learning more about it is important. In ML, continuous incremental learning makes

1202
02:21:37,600 --> 02:21:44,480
it possible to stay up to date. If we want repeatability, we can emit a condensed,

1203
02:21:44,480 --> 02:21:50,800
cleaned and frozen competence file from a learner that can be loaded into non-learning,

1204
02:21:50,800 --> 02:21:56,880
read-only, cloud-based understanding machines that serve the world and provide repeatability

1205
02:21:56,880 --> 02:22:04,160
between scheduled software and competence releases. Three, in the case of reductionist systems,

1206
02:22:04,160 --> 02:22:10,480
such as cell phone OS releases, we are used to getting well-tested new versions with minor bug

1207
02:22:10,480 --> 02:22:18,080
fixes and occasional major features at regular intervals. Such systems learn only in the sense

1208
02:22:18,080 --> 02:22:23,520
that the people who created them have learned more and put these insights into the new release.

1209
02:22:23,520 --> 02:22:29,760
Reductionist systems working with complete incorrect input data are expected to provide

1210
02:22:29,760 --> 02:22:34,720
correct and repeatable results according to the implementation of the algorithm.

1211
02:22:35,360 --> 02:22:42,080
But both the algorithm and the implementation may have errors. If the algorithm does not adequately

1212
02:22:42,080 --> 02:22:49,360
model its reality, then we have reduction errors. In the implementation, we may have bugs.

1213
02:22:49,360 --> 02:22:54,960
Holistic software systems can be designed to a different standard of correctness.

1214
02:22:55,680 --> 02:23:02,320
Since input data is normally incomplete and noisy, and results are based on emergent effects,

1215
02:23:02,320 --> 02:23:08,080
we can expect similar enough results even if parts of the system have been damaged,

1216
02:23:08,080 --> 02:23:14,000
for instance by catastrophic forgetting. Holistic systems can be made capable of

1217
02:23:14,000 --> 02:23:20,160
self-repair using incremental learning. This has been observed in the deep learning community.

1218
02:23:20,800 --> 02:23:25,600
Another technique is that when using multiple parallel threads in learning,

1219
02:23:25,600 --> 02:23:30,080
there may be conflicts that would normally require locking of some values.

1220
02:23:30,800 --> 02:23:36,240
But if the operations are simple enough, such as just incrementing a value,

1221
02:23:36,240 --> 02:23:42,240
we can forego thread safety in the locking since the worst outcome is the loss of a single increment

1222
02:23:42,240 --> 02:23:48,560
in a system that uses emergent results from millions of such values. And the mistake would,

1223
02:23:48,560 --> 02:23:55,600
in a well-designed system, be self-correcting in the long run. At the cloud level, absolute

1224
02:23:55,600 --> 02:24:02,640
consistency may not be as hard a requirement as it is for reductionist systems. Much larger

1225
02:24:02,640 --> 02:24:08,880
mistakes can be expected to be attributable to misunderstandings of the corpus or poor corpus

1226
02:24:08,880 --> 02:24:17,280
coverage. General strategies, decomposition into smaller problems, versus generalization

1227
02:24:17,280 --> 02:24:24,720
may lead to an easier problem. Assuming discards everything irrelevant based on how new information

1228
02:24:24,720 --> 02:24:30,880
matches existing experience, versus a machine discards everything irrelevant based on how

1229
02:24:30,880 --> 02:24:41,200
new information matches existing experience, modularity, versus composability, gather valid,

1230
02:24:41,200 --> 02:24:48,640
correct, and complete input data, versus use whatever information is available, and use all

1231
02:24:48,640 --> 02:25:01,600
of it. Formal, rigorous methods, versus informal ad hoc methods, absolute control, versus creativity,

1232
02:25:01,600 --> 02:25:10,160
intelligent design, versus evolution. The reductionist battle cries, the whole is equal to the sum

1233
02:25:10,160 --> 02:25:17,120
of its parts, which gives us a license to split a large complicated problem into smaller problems

1234
02:25:17,120 --> 02:25:23,600
to solve each of those using some suitable model, and then to combine all the sub-solutions

1235
02:25:23,600 --> 02:25:31,120
into a model-based solution for the original, larger problem, such as in moonshots, highway

1236
02:25:31,120 --> 02:25:38,640
systems, international banking, and generally in industrial intelligent design. This works

1237
02:25:38,640 --> 02:25:45,520
in simple and some complicated domains, but cannot be done in complex domains, where everything

1238
02:25:45,520 --> 02:25:52,240
potentially affects everything else. Spreading a complex system may cause any emergent effects

1239
02:25:52,240 --> 02:26:00,960
to disappear, confounding analysis. Examples of complex problem domains are politics, neuroscience,

1240
02:26:00,960 --> 02:26:09,200
ecology, economy, including stock markets, and cellular biology. Our life sciences operate

1241
02:26:09,200 --> 02:26:17,520
in a complex problem domain because life itself is complex. Some say biology has physics envy,

1242
02:26:17,520 --> 02:26:23,280
because in the life sciences, reductionist models are difficult to create and justify.

1243
02:26:24,000 --> 02:26:31,520
On the other hand, physics is for simple problems. Problems with many complex interdependencies and

1244
02:26:31,520 --> 02:26:38,640
unknown webs of causality can now be attacked using deep neural networks. These systems discover

1245
02:26:38,640 --> 02:26:44,720
useful correlations and may often find solutions using mere hints in the input which match their

1246
02:26:44,720 --> 02:26:50,400
prior experience. Reductionist strategies with correctness requirements outlaw this.

1247
02:26:51,040 --> 02:26:57,520
It is notable that one of the larger triumphs of holistic methods is protein folding, which is a

1248
02:26:57,520 --> 02:27:04,400
problem at the very core of the life sciences. So holistic understanding of a complex system

1249
02:27:04,400 --> 02:27:11,120
can be acquired by observing it over time and learning from its behavior. There is no need to

1250
02:27:11,120 --> 02:27:18,480
split the problem into pieces. Part of the holistic stance is that we give the machine everything.

1251
02:27:18,480 --> 02:27:27,680
Holism comes from the Greek word, holos, amicron, lambda, amicron, sigma, in the written text.

1252
02:27:27,680 --> 02:27:35,840
The whole, that is to say, all the information we have. If we start filtering the input data,

1253
02:27:35,840 --> 02:27:42,400
by cleaning it up, then the system will effectively learn from a polyana version of the world,

1254
02:27:42,400 --> 02:27:48,400
which will be confusing once it has to deal with real life inputs in a production environment.

1255
02:27:48,960 --> 02:27:54,320
If we want our machines to learn to understand the world all by themselves,

1256
02:27:54,320 --> 02:28:00,720
then we should not start by applying heavy-handed heuristic cleanup operations of our own design

1257
02:28:00,720 --> 02:28:08,080
on their input data. Sometimes, reductionist strategies are clearly inferior. The natural

1258
02:28:08,080 --> 02:28:14,080
language understanding is such a domain. Language understanding in a fluent speaker

1259
02:28:14,080 --> 02:28:21,840
is almost 100% holistic because it is almost entirely based on prior exposure. We are now

1260
02:28:21,840 --> 02:28:27,760
finding out that it is much easier to build a machine to learn any language on the planet from

1261
02:28:27,760 --> 02:28:34,560
scratch than it is to build a good old-fashioned artificial intelligence, 20th century reductionist

1262
02:28:34,560 --> 02:28:41,520
AI-based style machine that understands a single language such as English. The process where a

1263
02:28:41,520 --> 02:28:48,080
human, by using their understanding, discards everything irrelevant to arrive at what matters

1264
02:28:48,080 --> 02:28:54,240
is called the epistemic reduction and is discussed in the first five chapters in this book.

1265
02:28:54,960 --> 02:29:01,760
This is the most important operation in reductionism, but for some reason discussions of reductionism

1266
02:29:01,760 --> 02:29:09,920
in the past have tended to focus on other aspects. Perhaps this is a new result. ML systems discard

1267
02:29:09,920 --> 02:29:16,560
with little fanfare anything that was expected and that has been seen before as boring, harmless,

1268
02:29:16,560 --> 02:29:23,520
or otherwise ignorable. They may also discard things significantly outside of their experience

1269
02:29:23,520 --> 02:29:30,160
as noise. Things can only be reduced away at the semantic level. They can be recognized that

1270
02:29:30,880 --> 02:29:37,200
operations capable of epistemic reduction at multiple layers discard anything that's understood

1271
02:29:37,200 --> 02:29:43,760
at that layer, and they may pass on upward to the next higher semantic layer, a summary of what

1272
02:29:43,760 --> 02:29:50,400
they discarded plus everything they did not understand at their level. Empire levels do the

1273
02:29:50,400 --> 02:29:57,520
same. This is why deep learning is deep. Intelligently designed systems are often made up out of

1274
02:29:57,520 --> 02:30:04,160
interchangeable modules, which allow for easy replacement in case of failure, and in some

1275
02:30:04,160 --> 02:30:10,960
cases, and especially in software, allow for customization of functionality by replacing

1276
02:30:10,960 --> 02:30:17,280
or adding modules. These modules have well specified interfaces that allow for such

1277
02:30:17,280 --> 02:30:24,240
interconnections. In the holistic case we can consider a human cell with thousands of proteins

1278
02:30:24,240 --> 02:30:31,120
interact on contact or as required with many substances floating around in the cellular fluid.

1279
02:30:31,680 --> 02:30:35,680
It is not the result of intelligent design, and it shows

1280
02:30:35,680 --> 02:30:42,000
there are overlaps and redundancies that may contribute to more reliable operation, and there

1281
02:30:42,000 --> 02:30:49,680
are multiple potentially complex mechanisms keeping each other in check, or we can consider music,

1282
02:30:49,680 --> 02:30:56,000
or multiple notes in accord in different timbres in a symphony orchestra in a composition will

1283
02:30:56,000 --> 02:31:02,640
conjure an emerging harmonic whole that sounds different than the sum of its parts, or consider

1284
02:31:02,640 --> 02:31:10,560
spices in a soup, or opinions in a meeting that leads to a consensus. The word, composability,

1285
02:31:10,560 --> 02:31:17,440
fits this capability in the holistic case. Unfortunately, in much literature it is merely

1286
02:31:17,440 --> 02:31:25,040
used as a synonym for its reductionist counterpart, modularity. As discussed in the Geico case,

1287
02:31:25,040 --> 02:31:30,960
in the section above, holistic ML systems can fill in missing details starting from

1288
02:31:30,960 --> 02:31:38,240
various scant evidence. Compare for example confabulations of systems like GPT-3 and image

1289
02:31:38,240 --> 02:31:45,360
enhancement apps. They supply the missing details by jumping to conclusions based on few clues

1290
02:31:45,360 --> 02:31:51,840
and lots of experience. Since we are not omniscient and don't even know what is happening behind our

1291
02:31:51,840 --> 02:31:58,480
backs, scant evidence is all we will ever have, but it is amazing how effective scant evidence

1292
02:31:58,480 --> 02:32:05,920
can be in a familiar context. We can drive a car through fog or find an alarm clock in absolute

1293
02:32:05,920 --> 02:32:12,720
darkness. The more the system has learned, the less input is needed to arrive at a reasonable

1294
02:32:12,720 --> 02:32:18,560
identification of the problem and hence retrieve a previously discovered working solution.

1295
02:32:19,440 --> 02:32:25,760
Formal methods and experimental rigorousness make for good science. On the other hand,

1296
02:32:25,760 --> 02:32:32,480
holistic methods can follow tenuous threads, hoping for stronger threads or some solution,

1297
02:32:32,480 --> 02:32:38,800
with little effort spent on backtracking or documentation because once a solution is found,

1298
02:32:38,800 --> 02:32:45,200
it is the only thing that matters. Tracking has little value in non-repeating situations

1299
02:32:45,200 --> 02:32:53,120
or when using holistic methods at massive scales, such as in deep learning. Absolute control requires

1300
02:32:53,120 --> 02:32:59,120
that we know exactly what the problems and solutions are and all we need to do is implement them.

1301
02:32:59,840 --> 02:33:06,320
Once deployed, systems frozen in this manner, which are exactly implementing the models of

1302
02:33:06,320 --> 02:33:13,200
their creators, cannot improve by learning since there is no room for variation in the existing

1303
02:33:13,200 --> 02:33:20,560
process and hence no experimentation and no way to discover further improvements. Only

1304
02:33:20,560 --> 02:33:28,640
holistic systems can provide creativity and useful novelty. We also observe that, learning itself

1305
02:33:28,640 --> 02:33:34,640
is a creative act, since it must fit new information into an existing network of prior

1306
02:33:34,640 --> 02:33:42,880
experience. Just like the term, holism has been abused, so has intelligent design,

1307
02:33:42,880 --> 02:33:48,400
which is a perfectly reasonable term for reductionist industrial end-to-end practice

1308
02:33:48,400 --> 02:33:55,600
that consistently provides excellent results. On the holistic side, evolution in nature has

1309
02:33:55,600 --> 02:34:01,840
created wonderful solutions to all kinds of problems that plants and animals need to handle.

1310
02:34:02,560 --> 02:34:07,760
But we can put evolution, also known in the general sense as selectionism,

1311
02:34:07,760 --> 02:34:15,440
to work for us in our holistic machines. They can create new, wonderful designs with a biological

1312
02:34:15,440 --> 02:34:22,080
flavor to them that sometimes, depending on the problem, cannot perform intelligently designed

1313
02:34:22,080 --> 02:34:30,480
alternatives. Evolution is the most holistic phenomenon we know. No goal functions, no models,

1314
02:34:30,480 --> 02:34:38,560
no equations. Evolution is not a scientific theory. Science cannot contain it. It must be

1315
02:34:38,560 --> 02:34:46,320
discussed in epistemology. Mixed systems, deep neural networks can perform autonomous epistemic

1316
02:34:46,320 --> 02:34:53,040
reduction to find high-level representations for low-level input, such as pixels in an image

1317
02:34:53,040 --> 02:35:00,000
or characters in text. Current vision understanding systems can reliably identify thousands of

1318
02:35:00,000 --> 02:35:05,520
different kinds of objects from many different angles in a variety of lighting conditions

1319
02:35:05,520 --> 02:35:13,280
and weather. They can classify what they see, but do not necessarily understand much beyond that,

1320
02:35:13,280 --> 02:35:20,160
such as the expected behaviors of other, intentional Asians like cars, pedestrians,

1321
02:35:20,160 --> 02:35:28,960
or cats. Therefore, at the moment in 2022, most deployments of machine learning use a mixture

1322
02:35:28,960 --> 02:35:36,000
of reductionist and holistic methods, equations and formulas devised by humans implemented as

1323
02:35:36,000 --> 02:35:43,040
computer code, and some inputs from a deep neural network solving a sub-problem that requires it,

1324
02:35:43,040 --> 02:35:51,200
such as vision understanding. Self-driving cars use DNNs for understanding vision, radar,

1325
02:35:51,200 --> 02:35:58,320
and lidar images, discovering high-level information like a pedestrian on the side of the road

1326
02:35:58,320 --> 02:36:05,040
from pixel-based images and this understanding has, until recently, been fed to logic and

1327
02:36:05,040 --> 02:36:12,880
role-based programs that implement the decision-making. Avoid driving into anything, period, that is used

1328
02:36:12,880 --> 02:36:19,280
to control the car. The trend here is to move more and more responsibilities into the deep

1329
02:36:19,280 --> 02:36:26,560
neural network, and over time to remove the hand-coded parts. In essence, the network learns

1330
02:36:26,560 --> 02:36:33,760
not only to see, but learns to understand traffic. We are delegating more and more of our

1331
02:36:33,760 --> 02:36:42,480
understanding of how to drive to the vehicle itself. This is desirable. Experimental Epistemology

1332
02:36:43,680 --> 02:36:50,240
Epistemology is the theory of knowledge. It is concerned with the mind's relation to reality.

1333
02:36:50,240 --> 02:36:57,680
This includes artificial minds. An introduction to epistemology should benefit anyone working in

1334
02:36:57,680 --> 02:37:06,400
the IML field. Scientific statements look like F equals MA, Newton's second law, or E equals MC

1335
02:37:06,400 --> 02:37:13,360
squared, Einstein's famous equation, and can all be proven and or derived from other accepted

1336
02:37:13,360 --> 02:37:21,200
results or verified experimentally. Algebra is built on lemurs that are not part of algebra.

1337
02:37:21,200 --> 02:37:27,840
They cannot be proven inside of algebra. Similarly, epistemological statements are

1338
02:37:27,840 --> 02:37:34,960
not provable in science because science is built on top of epistemology. But when science is not

1339
02:37:34,960 --> 02:37:42,880
helping, such as in bizarre domains, then setting scientific methodology aside and dropping down

1340
02:37:42,880 --> 02:37:50,880
to the level of epistemology sometimes works. Epistemology is, just like philosophy in general,

1341
02:37:50,880 --> 02:37:57,600
an armchair thinking exercise, and the results are judged on internal coherence and consistency

1342
02:37:57,600 --> 02:38:04,720
with other accepted theory rather than by proofs or experiments. However, the availability of

1343
02:38:04,720 --> 02:38:12,160
understanding machines, such as DNNs now suddenly provides the opportunity for actual experiments

1344
02:38:12,160 --> 02:38:18,720
in epistemology. Consider the following statements from the domain of epistemology,

1345
02:38:18,720 --> 02:38:25,040
and how each of them can be viewed as an implementation hint for AI designers. We are

1346
02:38:25,040 --> 02:38:32,080
already able to measure their effects on system competence. You can only learn that which you

1347
02:38:32,080 --> 02:38:42,720
already almost know. Patrick Winston, MIT. Our intelligences are fallible. Monica Anderson.

1348
02:38:42,720 --> 02:38:49,360
In order to detect that something is new, you need to recognize everything old. Monica Anderson.

1349
02:38:50,640 --> 02:38:58,320
You cannot reason about that which you do not understand. Monica Anderson. You are known by

1350
02:38:58,320 --> 02:39:05,360
the company you keep, simple version of the yanni dilemma from category theory and the justification

1351
02:39:05,360 --> 02:39:12,240
for embeddings in deep learning. All useful novelty in the universe is due to processes

1352
02:39:12,240 --> 02:39:20,160
of variation and selection. The selectionist manifesto. Selectionism is the generalization

1353
02:39:20,160 --> 02:39:28,880
of Darwinism. This is right genetic algorithms work. Science has no equations for concepts like

1354
02:39:28,880 --> 02:39:36,240
understanding, reasoning, learning, abstraction, or modeling since they are all epistemology level

1355
02:39:36,240 --> 02:39:44,160
concepts. We cannot even start using science until we have decided what model to use. We must use

1356
02:39:44,160 --> 02:39:50,640
our experience to perform epistemic reductions, discarding the irrelevant, starting from the

1357
02:39:50,640 --> 02:39:57,120
messy real world problem situation until we are left with a scientific model we can use,

1358
02:39:57,120 --> 02:40:04,480
such as an equation. The focus in AI research should be on exactly how we can get our machines

1359
02:40:04,480 --> 02:40:10,960
to perform this pre-scientific epistemic reduction by themselves and the answer to that

1360
02:40:10,960 --> 02:40:18,400
cannot be found inside of science. Artificial General Intelligence Artificial General Intelligence,

1361
02:40:18,400 --> 02:40:26,240
AGI, was a theoretical 20th century reductionist AI attempt to go beyond the narrow AIA of domain

1362
02:40:26,240 --> 02:40:33,840
specific expert systems closer to a general intelligence they thought humans had. The

1363
02:40:33,840 --> 02:40:42,240
term was mostly used by independent researchers, amateurs and enthusiasts. But the AGI term was

1364
02:40:42,240 --> 02:40:48,400
not well enough defined and was not backed by sufficient theory to provide any AI implementation

1365
02:40:48,400 --> 02:40:55,200
guidance and what little progress had been made by these groups was overtaken by holistic methods

1366
02:40:55,200 --> 02:41:03,600
after 2012. Today we know that the entire premise of 20th century reductionist AGI was wrong.

1367
02:41:04,160 --> 02:41:12,320
Humans are not general intelligences at birth. Instead, we are general learners capable of

1368
02:41:12,320 --> 02:41:18,880
learning almost any skill or knowledge required in a wide range of problem domains. If we want

1369
02:41:18,880 --> 02:41:25,760
human compatible cognitive systems, then we should build them in our image in this respect to build

1370
02:41:25,760 --> 02:41:33,760
machines that learn and jump to conclusions on scant evidence. Decades ago, AGI implied a human

1371
02:41:33,760 --> 02:41:40,560
programmed reductionist hand coded program based on logic and reasoning that can solve any problem

1372
02:41:40,560 --> 02:41:47,040
because the programmers anticipated it. To argue against claims that this was impossible,

1373
02:41:47,040 --> 02:41:53,840
the AGI community came up with a promise or threat of self-improving AI. But the amount

1374
02:41:53,840 --> 02:42:01,040
of code in our cognitive systems has shrunk from 6 million propositions in sake around 1990

1375
02:42:01,040 --> 02:42:09,840
to 600 lines of code to play video games around 2017 to about 13 lines of cares code in some

1376
02:42:09,840 --> 02:42:16,560
research reports. And now there's AutoML and other efforts at eliminating all remaining programming

1377
02:42:16,560 --> 02:42:23,760
from ML. The problems are not in the code. There's almost no code left to improve in

1378
02:42:23,760 --> 02:42:32,160
modern machine learning systems. All that matters is the corpus. We can now, after 2012,

1379
02:42:32,160 --> 02:42:37,840
see that machine learning is an absolute requirement for anything worthy of the name AI,

1380
02:42:38,640 --> 02:42:44,240
which makes recursive self-improvement leading to evil superhuman omniscience logic based

1381
02:42:44,240 --> 02:42:51,520
godlike artificial general intelligence a 20th century reductionist AI myth. We must focus on

1382
02:42:51,520 --> 02:42:59,840
artificial general learners. Afterward, science was created to stop people from overrating correlations

1383
02:42:59,840 --> 02:43:05,440
and jumping to erroneous conclusions on scant evidence and then sharing those conclusions

1384
02:43:05,440 --> 02:43:12,960
with others, leading to compounded mistakes and much wasted effort. Consequently, promoting a

1385
02:43:12,960 --> 02:43:19,840
holistic stance has long been a career-ending move in academia, and especially in computer science.

1386
02:43:20,560 --> 02:43:27,440
But now we suddenly have machine learning that performs cognitive tasks such as protein folding,

1387
02:43:27,440 --> 02:43:34,240
playing go, and estimating house prices at useful levels using exactly a holistic stance.

1388
02:43:34,880 --> 02:43:42,080
So now science itself has a cognitive dissonance. This is a conflict about what science is or

1389
02:43:42,080 --> 02:43:49,120
should be. Inherence of these stances leads people to develop significant personal cognitive

1390
02:43:49,120 --> 02:43:54,560
dissonances, which is why discussions about these issues are very unpopular among people

1391
02:43:54,560 --> 02:44:03,360
with solid STEM educations. But the dichotomy is real. We need to deal with it. Our choices so far

1392
02:44:03,360 --> 02:44:12,960
seem to have been too. Claim that dichotomy doesn't exist. But Schrodinger and Pursig also discuss it.

1393
02:44:12,960 --> 02:44:20,480
Claim that the holistic stance doesn't work. But deep learning works. Claim that reductionist

1394
02:44:20,480 --> 02:44:27,840
methods are requirement, hobbling our toolkits for a principle. The reductionist stance also

1395
02:44:27,840 --> 02:44:35,200
makes it difficult to imagine and accept things like systems capable of autonomous epistemic

1396
02:44:35,200 --> 02:44:43,360
reduction, systems that do not have a goal function, systems that improve with practice,

1397
02:44:44,720 --> 02:44:51,840
systems that exploit emergent effects, systems that by themselves make decisions about what

1398
02:44:51,840 --> 02:44:58,880
matters most, systems that occasionally give a wrong answer but are nevertheless very useful.

1399
02:44:59,600 --> 02:45:06,320
So after a serious education in machine learning we don't actually need to do almost any programming

1400
02:45:06,320 --> 02:45:13,280
at all. And we don't need to understand anybody else's problem domains. Because we don't have

1401
02:45:13,280 --> 02:45:21,040
to perform any epistemic reduction ourselves. We should recognize this for what it is. AI was

1402
02:45:21,040 --> 02:45:27,200
supposed to solve our problems for us so we would not have to learn or understand any new

1403
02:45:27,200 --> 02:45:34,880
problem domains. To not have to think. And that's what we have today, in machine learning,

1404
02:45:34,880 --> 02:45:41,680
and with holistic methods in general. Why are some people surprised or unhappy about this?

1405
02:45:42,400 --> 02:45:48,880
In my opinion, this is AI, this is what we have been trying to accomplish for decades.

1406
02:45:48,880 --> 02:45:56,160
People who claim machine understanding is not AI are asking for human level human-centric reasoning

1407
02:45:56,160 --> 02:46:02,880
and are, at their peril, blind to the nascent ML-based understanding we can achieve today.

1408
02:46:03,520 --> 02:46:09,840
With expected reasonable improvements in machine understanding capabilities, familiarity and

1409
02:46:09,840 --> 02:46:16,720
acceptance of the holistic stance will become a requirement for ML and AI-based work. It will

1410
02:46:16,720 --> 02:46:25,040
likely take years for our educational system to adjust. This has been Monica's Little Pills,

1411
02:46:25,040 --> 02:46:47,760
read to you by a computer. Thank you for listening.

