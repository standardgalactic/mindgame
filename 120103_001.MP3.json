{"text": " The Blue Pill. Self-improving AI. Self-improving AI is a meme that has been circulating since the 1980s. Current proponents of the idea include Wastram and Omihandro. My own summary goes something like this. If we get any kind of AGI going, no matter how slow it is and how buggy it is, we can give it access to its own source code and let it analyze it and clean up and fix the bugs and then rewrite its code to be as good as it can make it. We then start up the slightly smarter AGI and repeat the process until the AGI's get super intelligent. On the surface, this is irrefutable. We already have examples of systems improving themselves. We can buy a cheap 3D printer and then quite cheaply print out parts for a much better 3D printer. Or to make computer chips that go into computers that design better computer chips. Not to mention evolution of all species in nature. I look at it from an epistemologist point of view and say, that's a hard line reductionist idea that should not have made it out of the 20th century. The idea, as its inception, imagined an AGI as something that was written by teams of human programmers using software development tools and mathematical equations. What I think the only thing that even approximates this outcome is that the code is perfect, and humans as well as machines all agree there are no more improvements to be made. And the resulting AGI's are still not super intelligent. The most likely outcome is that we all realize the folly in this argument and won't even try. It's not about the code. The number of lines of code in AI related projects has been declining rapidly. 2012. 34,000 lines.py.kudukrzebski et al. for ImageNet. 2013. 1571 lines of Lua to Play Atari games. 2017. 196 lines of Keras to Implement Deep Dream. 2018. Less than 100 lines of Keras for research paper-level results. And all of these, except Saig, included as the most famous example of a 20th century reductionist AI system, demonstrates new levels of power of machine learning. The limits to intelligence are not in the code. In fact, they are not even technological. The limit of intelligence is the complexity of the world. Admission is unavailable. The main purpose of intelligence is to guess, to jump to conclusions on scant evidence, and to do it well, based on a large set of historical patterns of problems and their solutions or events and their consequences. Because scant evidence is all we will ever have, we don't even know what goes on behind our back. And because our intelligence is guessing, I have repeatedly claimed that, all intelligences are fallible. We are already making machines that are better than humans in some aspect of guessing. Protein folding and playing go are examples of this. And these machines will get bigger and better at what they do and will be superhuman in various ways and in many problem domains, simply based on larger capacity to hold, look up, or search useful patterns. The code doing that can be hand optimized to the point where any AI improvement would be insignificant. My own code in the inner loop for understanding any language on the planet, once it has learned it, in inference mode is about 90 lines of Java. We can expect a best minor improvements to efficiency and speed. It comes down to the corpus. In my domain, NLU, simple tests can be scored at 100% after a few minutes of learning on a laptop. Continue learning for days and weeks would provide a larger sample set of vocabulary in appropriate contexts, which would mainly correct misunderstandings in corner cases. But these corporal are not comparable by several orders of magnitude, to the gathered life experience of a human at age 25. The main limit of intelligence is corpus size in ML situation. Future artificial intelligences will be nothing like what AGI fans have been fearmongering about. These are 20th century reductionist AI ideas. The components are blind to the most fundamental basics of epistemology. Reductionist good old fashioned AI has been demonstrated to being inferior in their own domains to even semi-trivial machine learning methods. We need AGL, not AGI. Machines learning to code. As of this writing, there are a handful of available code writing systems based on ML technology that has learned from large quantities of open source code. For example GitHub Copilot, OpenAI Codex, and Amazon Code Whisperer. They have not yet surpassed human programmers. But it's not about writing code either. AI's writing code is about as silly as AI magazine covers with pictures of robots typing, wink wink. In the future, if we want the computer to do something, we will have a conversation, speaking and listening, with the computer. The conversation is at the level of discussing a problem with a competent coworker or professional. It may spontaneously ask clarifying questions. I call this, contiguously rolling topic, mixed initiative dialogue, others talk of these bots as dialogue Asians. But this will go beyond Siri or Alexa, and when the computer understands exactly what you want done. It just does it. Why would reductionist style programming be a necessary step? Yes, there will still be lots of places where we want to use code. But whether that code is written by humans or AI's will make much less of a difference than we might expect based on today's use of computers. The Pink Pill. The Wisdom Salon. Wisdom Salon is an online world cafe. The World Cafe protocol is a recipe for organizing conversations that matter on a large scale. Thousands of people can cooperate in order to bring clarity to complex issues. This is a post-mortem summary for my interrupted wisdom salon project. I have all the code in an archive, but it requires a complete rewrite in order to fix the two biggest problems. The switch from flash, hack, to HTML5 for video and the cost of video connections. I know how to fix these but I'm busy working on understanding machines. At the moment, I am looking for someone to take this over. I also observe that there is a need for something like this. I see things discussed on Quora that would make good topics for a wisdom salon. I happen to believe video in spoken words are an important component for many reasons. Wisdom. Knowledge and information can easily be found on the web. But what about wisdom? Intelligence is based on gathered knowledge. Wisdom is based on gathered experience. To get wiser, seek out more experiences. Engage yourself. Do more stuff. Travel. Talk to people to share their experiences. Conversation with others is the easiest way to gain wisdom. But not all conversations are equal. We want conversations that matter. The World Caf\u00e9 Protocol. The World Caf\u00e9 Protocol is a recipe for organizing such conversations that matter on a large scale. Thousands of people can cooperate in order to bring clarity to complex issues. To find out more, buy the book or study the World Caf\u00e9 website. But this is how it typically works. In some conference facilities or gymnasium, the organizers provide dozens to hundreds of square tables. Each has four chairs, a box of crayons, and a piece of butcher paper as a tablecloth. Stakeholders from all walks of life get invited and sit down at the tables. This could be a mixture of farmers, teachers, politicians, in corporate environments. Sometimes this is everybody in the company. Organizers now unveil a carefully phrased focusing question as the topic of the conversations. It is important that the question is positive and focusing. For education reform, don't ask, what is wrong with our education system? Instead, ask, what could a great school also be? The four people at each table now start a conversation around the question. Everyone takes notes on the butcher paper, using the crayons. After 20 minutes, a gong rings. Three people. Everyone except south in duplicate bridge terms. At each table get up and move to other tables at random. Through fresh random people sit down at each table. South now first explains to the newcomers what the notes on the tablecloth mean. This provides a kind of lightweight continuity from the previous conversation at this table. The three newcomers comment on these notes and add fresh comments. The best parts of what was said at their previous tables. These conversations unfold very naturally. Four strangers can easily have a friendly conversation about complex things that matter. They don't even have to introduce themselves. They contribute their wisdom and experiences. Not their resumes. Conversations now continue for another 20 minutes. The gong rings again, and the shuffling repeats. After two to three hours, the session is over and the butcher papers are gathered by the organizers into what is called the harvest. They are summarized in some time later. Perhaps, after lunch, the results are shared with all the stakeholders. Why this works so well? Someone pushing a bad idea of theirs at every table can spam at worst 27 people in three hours. A good idea. Introduced at the first table and repeated by all participants at subsequent tables will reach over 100,000 people or the majority of the audience, whichever is smaller. This is the filtering power of the World Caf\u00e9 protocol. Wisdom Salon is an online World Caf\u00e9. Sadly, the Wisdom Salon project has been suspended because of changing infrastructure and cost structure for online video transmissions, and because of lack of time on my part. It is possible to restart the project using current video technology and with funding and a larger team. If interested in contributing to this, please get in touch. What follows is the original high-level design specification, written in the present tense, design specification. The Wisdom Salon is a 24-7 online World Caf\u00e9 implemented as a video chat site. Conversations have four participants, but each conversation can also have a passive and quiet audience of any size. All conversations are always public. All conversation participants are known by their login identities. Why would anyone want to participate? The main purpose of Wisdom Salon is increased wisdom and improved clarity and complex issues for the participants. This is your main benefit. This is why you would want to participate. You will not get lags, but you might earn a local currency, called, Influence, that you can selectively use to extend your influence. Goal. The goal is specifically not to find the best grains of wisdom in the harvest. The grains are there mainly to provide continuity and shorten the time to get to talking about things that matter. The system is there to provide the users a chance to analyze large and complex issues with others in conversation and in exchange of experiences. Do not underestimate how different an interactive conversation is from a web search or reading a book. Have you ever spent days studying something without getting it only to have someone set you straight in two minutes of conversation? Have you ever been in a meeting where the resolution is something none of the participants even understood when the meeting started? Sample questions. What kinds of questions demonstrate the power of the Wisdom Salon? Consider these samples. I am considering a midlife career change. What matters? Where should I retire, and why there? Should I pursue a career in engineering or medicine? Lifestyle design in interesting times. What is the true promise of genetics research and why should I care? What movies should I let my children watch, and why? Musical education for my child. What matters? What instruments, and why? What is it really like to be a soldier in places like Afghanistan and Iraq? Should I retire in Costa Rica? User experience. People arrive when they want and leave when they want. They can engage in multiple ways. Upon entering the site, users are presented with the, at the moment, most popular conversation, the one with the largest audience. Below the conversation, there will be a list of other popular conversations, headed by conversations and topics the user may have watched or previously participated in. They can browse all ongoing conversations much like watching talk shows on television. They can select from hundreds of questions to find something that interests them, or add their own. Instead of a butcher paper, they can leave notes on each question known as, grains of wisdom, to provide the lightweight continuity from table to table. They can vote on these grains of wisdom so that they better result rise to the top. Results are immediately visible to all. They can observe what other people say and how they behave and modify their own social graph to improve their chances of interaction with the best people. A local currency is earned by passive engagement per hour, more of it is earned by participating in conversations, and the currency is used to pay for the privilege of posting a comment, because posting cost currency, spelling the grains of wisdom will be limited. A topic without currently active conversations still allows you to browse the grains of wisdom on the topic, and if you have influence, you can vote on the grains or notes that you like or otherwise agree with, and you can restart the topic by creating a table and hope others will join. Four main uses of wisdom salon. The site enables, but doesn't enforce the World Cafe protocol. You can use the site for several different purposes. As entertainment and education, passively watching conversations among your peers, much like flipping channels on television. To get both factual information and broad ranging personalized advice from experts. To share your expertise in fields you understand. To do micromantering. To find an audience for storytelling and sharing personal experiences from your life. To gain wisdom and personal clarity in complex issues. To debate the major issues of the day in person and productively selected and well behaved groups. To find new interesting and competent friends by observing their behavior and then befriending them, much like other social media. Any active conversation starts a 20 minute clock bar moving. You can leave anytime. System provides some incentive to stay the full 20 minutes. On the other hand, you don't have to leave after 20 minutes. If you like, you can continue conversation along as you want. But we expect a large fraction of people to adhere to the protocol. We believe this maximizes the wisdom gain per session. Without the right people, the system is worthless. Do not be discouraged. Facebook would be worthless with only 10 people on it. Wisdom salon really requires at least 50 people to be on the system before you are likely to find a conversation around a question you actually care about anytime you join. So nobody knows if this will work or not, and it may take a while before the system matures enough to attract a sufficient repeat audience to become what I designed it for. If you don't like it at first, please try again. It might well improve, and you might get lucky to get into an amazing conversation when you least expect it. Welcome to my experiment. The lavender pill. Model free AI. Don't model the world. Just model the mind. It's a lot easier. With some poetic freedom, I'd like to claim 1. Model the world. 10 billion lines of code. 2. Model the brain. 10 million lines of code. 3. Model the mind. 10,000 lines of code. Number one is regular programming. We make computers perform actions in a context that matches the programmer's mental model of some relevant parts of the world. Number two is neuroscience-based models of neurons, synapses and other biological structures and systems in brains. The number three is epistemology-based models of learning, understanding, reasoning, prediction, abstraction, and other holistic and emergent phenomena. Epistemology-based methods require a rather minimal infrastructure to support whatever operations these concepts require. I put models within irony quotes because they are strictly speaking metamodels because they are used in metascales. They are not about skills, such as English or folding proteins. They are about how to acquire such skills by learning from our mistakes. The purple pill. Corpus congruence. Understanding in brains and machines can be defined and measured as corpus congruence. Corpus congruence as a metric spans up almost all of NLP. Understanding in brains and machines can be defined and measured as corpus congruence. Let's consider this in the machine learning sense. If a machine is model-free, holistic, as all general understanders have to be in order to not get trapped into a limited model, then all it ever knows comes from the corpus it was trained on. And all it really can say is, this is more like my corpus than that. Or, this is more like these documents in my corpus than those corpus congruence as a metric spans up almost all of NLP. Because most of NLP is doxen in various guises. Given two documents A and B in some corpus, a classifier can say that an unknown document, which we can call U, is more like it than B given this capability we can build. Classification and clustering by using A, B, up to N as defining classes. Filtering by using A, wanted dox and B, unwanted dox. Summoned analysis by using A, negative dox and B, positive dox. Entity extraction by softly matching termed against lists of known entities. Doxen, find me more documents like this one. Reductionist and NLP uses all of these at the bag of words or word count levels for things like web search, span filtering, and clustering. Holistic NLU aims to do the same based on the meanings expressed in sentences and paragraphs. But semantic corpus congruence is still corpus congruence. Common sense now becomes, is the proposition before me congruent with my entire world model, as required by learning things from my training corpus. If it is well known, then we can likely ignore it this time, and if it is not, then the next question will be, is it close enough that it might be worth while extending the world model with this information? If the answer is no, then the input is by its definition nonsense. Otherwise it is either a new fact or a lie, but since we cannot tell, we have to accept it, possibly with a note that this is fresh, untested knowledge that may turn out to be irrelevant, false, counterproductive, or noise. Next we can note that it doesn't matter whether documents are text or images, or input from a point cloud of sensors for robots or autonomous vehicle sensors. And finally we can note that this definition also holds for humans if we take our corpus to be everything we've experienced since birth. Monika's Little Pills Chapter 1 Why I Works Intelligence equals understanding plus reasoning. Interest in artificial intelligence is exploding, and for good reasons, computers and cars, phone apps, and on the web can do amazing things that we simply could not do before 2012. What's going on? This is an attempt to explain the current state of AI to a general audience without using mathematics, computer science, or neuroscience, discussions at these levels with focus on how AI works. Here I will discuss this at the level of epistemology and will try to explain why it works. Epistemology sounds scary, but it really isn't. It's mostly scary because it is unknown, it is not taught in schools anymore, which is a problem, because we now desperately need this branch of philosophy to guide our AI development. Epistemology discusses things like reasoning, understanding, learning, novelty, problem solving in the abstract, how to create models of the world, etc. These are all concepts one would think would be useful when working with artificial intelligences, but most practitioners enter the field of AI without any exposure to epistemology which makes their work more mysterious and frustrating than it has to be. I think of it epistemology as the general base for everything related to knowledge and problem solving. Science forms a small special case subset domain where we solve well-formed problems of the kind that science is best at. In the epistemology outside of science we are free to productively also discuss pre-scientific problem solving strategies, which is what brains are using most of the time. More later, intelligence equals understanding plus reasoning. In his book, Thinking Fast and Slow, Daniel Kahneman discusses the idea that human minds use two different and complementary processes, two different modes of thinking, which we call understanding and reasoning. The idea has been discussed for decades and has been verified using psychological studies and by neuroscience. Subconscious intuitive understanding is the full name of the fast thinking or system one thinking. It is fast because the brain can perform many parts of this task in parallel. The brain spends a lot of effort on this task. Conscious logical reasoning is the full name of slow thinking or system two thinking. To many people's surprise, this is very rarely used in practice. By soundbite for this is, you can make breakfast without reasoning. Almost everything we do on a daily basis in our rich mundane reality is done without a need to reason about it. We just repeat whatever worked last time we performed this task. Real experience driven. Intuitive means that the system can very quickly provide solutions to very complex problems but those solutions may not be correct every time. Logical means that answers are always correct as long as input data is correct and sufficient, which is not true in our rich mundane reality. It can only be true in a mathematically pure model space. If you like logic, you must also like models. Subconscious means we have no conscious, introspective access to these processes. You are reading this sentence and you understand it fully but you cannot explain to anyone, including yourself, how or why you understand it. Conscious means we are aware of the thought, we can access it through introspection and we may find reasons to why we believe a certain idea. Expensive is on the list because brains spend most of their effort on this understanding part. We really shouldn't be surprised that AI now requires very powerful computers. More later. In contrast, reasoning is efficient. It is most useful when you are stuck in a novel situation or experience and understanding doesn't help you. Or perhaps you need to plan ahead or need to find reasons for why something happened after the fact. It is used at a formal level in the sciences. Reasoning is important but just rarely needed or used. Finally, understanding is model-free and reasoning is model-based. This is likely the most important distinction to people who are implementing intelligent systems since it provides a way to keep the implementation on the correct path when the going gets rough. We cannot discuss these issues quite yet but if you are curious you can watch the videos at Vimeo.com which discuss this distinction at length. Think of the appearance in this table as a kind of foreshadowing. All of this groundwork allows me to state the main point of this section. We have known for a long time that brains use these two modes. But the AI research community has been spending over much effort on the reasoning part and has been ignoring the understanding part for 60 years. We had several good reasons for this. Until quite recently, our machines were too small to run any useful sized neural network. And also, we didn't have a clue about how to implement this understanding. But that is exactly what changed in 2012 when a group of AI researchers from Toronto effectively demonstrated that deep neural networks could provide a simple kind of shallow and hollow proto-understanding. Well, they didn't call it that, but I do. I will look just a little into the future and overstate this just a little in order to make it more memorable. Deep neural networks can provide understanding. This new phase of AI took decades to develop, but it would never have happened without people like the group led by Jeffrey Hinton at the University of Toronto, who spent 34 plus years to develop the deep neural network technology we now call, deep learning. A number of breakthroughs from 1997 to 2006 led to a number of successful demonstrations, including first prizes in AI competitions in 2012. And we therefore count this year as the birth year of machine understanding. To an outsider, it may look like an older program or phone app might be understanding whatever the app is doing, but that understanding really only happened in the mind of the programmer creating the app. The programmer first simplified the problem in their own head by discarding a lot of irrelevant detail using programmer's understanding. The simplified mental model of the problem domain could then be explained to a computer in the form of a computer program. What is changing is that computers are now making these models themselves. The first bullet point describes regular programming, including old style AI programs. AI has, since 1955, provided many novel and brilliant algorithms that we now use in programs everywhere. But when you contrast old style AI to understanding systems, the old kind of AI is basically indistinguishable from any other kind of programming we do nowadays. The second bullet point describes the recent developments. Deep neural networks are so different from regular programs that we have to acknowledge them as a different computational paradigm. This is why they took almost four decades to develop. And the paradigm, being pre-scientific and model-free, is difficult to grasp if you receive a solid reductionist and model-based education. It takes a long time for an established AI practitioners or experienced programmer to switch. People who are just starting out in AI have an easier time assimilating this new paradigm since they haven't had a full career's worth of experience and success using old style AI techniques. The amount of work we have to do to get a deep neural network to understand is surprisingly small, and companies like Google and Cintiens are working on eliminating the remaining effort of programming neural networks. This is where things will get really weird. When the deep neural network, DNN, understands enough about the world and about the problem it is faced with, then we no longer need a programmer to acquire this understanding. Let me elaborate. Programmers are employed to bridge two different domains. They first have to study whatever application domain they are working on. For instance, if they are writing an airline ticket reservation system they will have to learn a lot of detailed information about airlines, airline tickets, flights, luggage, etc. and then know to provide features for unusual cases such as cancelled flights. And then the programmer uses their understanding of the problem domain to explain to a computer how it can reason about these things, but the programmer cannot make the system understand, it can only put in the hollow and fragile kind of reasoning, as a program with many of thin cases, and any misunderstandings the programmer has about the problem domain will become bugs in the computer program. Notice the shift in terminology. More later. But today, for certain classes of moderately complex problems, we can use a DNN to automatically learn for itself how to understand the problem, which means we no longer need a programmer to understand the problem. We have delegated our understanding to a machine, and if you think about that for a minute you will see that that's exactly what an AI should be doing. It should understand all kinds of things, so that we humans won't have to. And there are two common situations where this will be a really good idea. One is when we have a problem we cannot understand ourselves. We know a lot of those, starting with cellular biology. The other common case will be when we understand the problem well, but making the machine understand it well enough to get the job done is cheaper and easier than any alternative. MoonBoss accomplish this level of using old style AI methods, but I predict we will one day be flooded with similar, but DNN based devices that understand several aspects of domestic maintenance, as well as we do. Do machines really understand? If we give a picture like this to a DNN trained on images it will identify the important objects in the image and provide the rectangles, called, owning boxes as approximations to where the objects are. The text on the right says, woman in white dress standing with tennis racket to people in green behind her, which is not a bad description of the image. It could be used as the basis for a test for English skill level for adult education placement, for all practical purposes. This is understanding. We had no idea how to make our computers do this before 2012. This is a really big deal. This feat requires not only a new algorithm, it requires a new computational paradigm and images to a computer, a single long sequence of numbers denoting values for red, blue and green colors and values from 0 to 255. It also knows how wide the image is. How does it get from this very low level representation to knowing that there is a woman with a tennis racket in the image? This is what William Calvin has called, a river that flows uphill. There are very few mechanisms that can go in this direction, from low levels to high levels. Calvin used the term to describe evolution, and I can use this quote to describe understanding. I like to think of evolution as, nature's understanding because the phenomena are very similar at several levels. Evolution of species can bring forth advanced species starting from simpler species in the same manner that understanding is the discovery and reuse of high level concepts and low level input. In contrast, reasoning proceeds by breaking problems into sub-problems and solving those, which is a, flowing downhill, kind of strategy. In mathematics we accept, and many mathematicians only accept this reluctantly, that we need to use induction to move uphill in abstractions, and that's a very limited uphill movement at that. Epistemology allows for much stronger uphill moves. This is known as, jumping to conclusions on scant evidence and it's allowed in epistemology based pre-scientific systems. As an aside, here's a pretty deep related thought. In nature, evolution reuses anything that works. I like to think that understanding is a spandrel of evolution itself. Neural Darwinism certainly straddles this gap. Could be coincidence, or the only answer that will work at all. More later, we doubled our AI toolkit in 2012. We can now use these deep neural networks as components in our systems to provide understanding of certain things like vision, speech, and other problems that require that we discover high level concepts and low level data. The technical, epistemology level name for this uphill flow in processes, reduction, and we'll be using that term later after we explain what it means. Let's look at what the industry is doing with their new found toys. This is my view of what I think Tesla is doing, based on public sources in their self-driving, autopilot, cars, cameras feed vision understanding components based on deep learning, and radar feeds to radar understanding components. These supply bounding boxes in 2D or 3D with additional information like, there's a woman with a tennis racket ahead to a traffic reasoning component that uses regular programming, or some old style AI like a rule based system to actually control the car based on the vision and radar inputs, and the driver's desires. But this is not the only possible configuration. George Hopps at Comma.ai, a team at NVIDIA Corporation, and the deep Tesla class at MIT are using a simpler architecture with just a neural network that implements lane following and other simple driving behaviors directly in one single deep neural network. There's room for improvement, but there a big step in the direction we want to move in. Future automotive systems will likely integrate everything about driving into one single neural network, or something that effectively behaves as one. Vision, traffic, the car itself including various functionality like windscreen wipers, lights, and entertainment, how to drive in a safe and polite manner, and to understand also the drivers or car owners desires. And if we've gotten that far, then it is a given that we will have speech input and output so that the driver can have a conversation with the car while driving, and can just advise it in case it does something wrong. We are nowhere close to this today. But after a DNM breakthrough or two, who knows how quickly these kinds of systems become available. We can already see an increasing stream of new features built using understanding components. This article, and the next, are expansions of a talk given on June 10, 2017 at the San Francisco Bill Conference. A decade ago I created artificialintuition.com. I now have a lot more to say, but I need to split this meme package into digestible chunks. This takes a lot of effort to get right. If you liked this article and would like to see more like it then you can support my writing and my research in many ways, small to large, like and share these ideas with someone who might want to invest in sentience incorporated or might be otherwise interested in my research on a novel language understanding technology called organic learning. More on that later. I do not receive external funding from any investors for this research. You can support my research and writing directly at the donation section at artificialintuition.com. Chapter 2. Our Greatest Invention, Model Based Problem Solving. The first chapter, why AI works, provided the big picture of AI and understanding machines. Next we will focus on how to actually implement understanding in a computer. But before we can attack that core issue, we need to simplify the journey a bit by defining four important words and concepts. I'll define one in this section, two in the next, and the concept of reduction after that. We can then discuss the epistemology level algorithm for understanding itself. If you are already familiar with these concepts, just check the headings and definitions that follow in order to ensure we are using these words roughly the way you use them. You may have noticed I write certain, sometimes common words, such as model, with an uppercase first letter. This means I am using the word in a technical, well-defined, unchanging sense. I will define all such technical terms over time and I will try not to use these terms until I have defined them. We define 11 such terms in the first chapter, starting with understanding and reasoning. A dictionary of defined terms is in the works. Models are simplifications of reality in epistemology and science. Models are simplifications of reality. A rich mundane reality is too complex to land itself directly to computation. In OTB science fiction shows, we would sometimes hear. And then we fed all the information into the computer and this is what came out. Well, not anymore. Audiences now know that's not how regular computers work. Consider an automobile. It consists of thousands of parts, each with properties like materials, size, color, function, and sometimes complex interactions with other parts. What's all the information here? We can just feed all of those properties and measurements and facts into a computer and expect to get an answer. We need to ask a question and we also need to simplify the problem so that we can feed in just the facts or numbers that matter so that our question can be answered with minimum effort. How do we do that? We must identify or create, first in our minds, a very simple model of some sort of a generic automobile, and use that model for our computation. After we get the answer for the pure and simple model case, we apply the answer, with some care, back to our complex reality where the real automobile and the problem exists. What kind of model we choose depends on our goals. As an example of a model, Newton's second law states that force equals mass times acceleration, f equals ma. This equation is a classical scientific model. If we measure mass and acceleration of a car, then we can estimate how many horsepower the engine has. To use this equation, we engineers would model, in our minds, the car as a single small point mass with all the mass of the car in that point. Because if we don't, then we'd have to worry about the car rotating and other problems. This is how model-based science works. One or more scientists somehow derive a model for some phenomenon. The model is published as an equation, a formula, or a computer program. Scientists and engineers anywhere can now use this equation program model, treating it as a quick shortcut that works every time, as long as they have correct input data and are confidently applying the formula to a suitable problem in their reality. Our greatest invention, model-based problem solving, aka reductionism, is the greatest invention our species has ever made. The general strategy of simplifying problems before solving them must be tens of thousands of years old. In some sense, it is a prerequisite for all other inventions, including the use of fire. If you see a forest fire then you need to first imagine the utility of fire. As a model, before you can figure out that it might be useful to carry home a burning branch, we don't think of this problem solving strategy as an invention because it is already ubiquitous in our lives. We are all taught how to use model-based problem solving in school when we start solving story problems in math class, but most people never learn the names of these strategies and are missing the big epistemology level picture. This rarely matters until you start working with AI, where lack of an epistemological drowning may lead you astray into failing strategies. These little pills are an attempt to remedy that. Model-based methods were examined and refined into scientific methods over the past 450 years. Science is now a collection of thousands of models that taken together allow science competent people to solve problems quickly and efficiently without having to redo all the work that scientists, like Newton, put into creating these models in the first place. And the sum total of those models covers many problems we want to solve scientifically, such as how to build a bridge or travel to the moon. This reuse is what makes science so effective, but not all sciences can benefit equally from this model-making. It works well for physics, chemistry, and most of biochemistry. As I'm fond of saying, physics is for simple problems, but as you get to more and more complex sciences, as you get further away from physics and closer to life, it gets harder to make decent models. The models used by for instance psychology, ecology, physiology, and medicine are generally more complex but also less powerful than models in physics. Given some solid data, a physicist can compute the mass of the proton to six decimal places, but we would have a harder time predicting the number of muskrats in New England next summer because that outcome depends on millions of parameters. The life sciences base many of their models on statistics. Statistical models are among the weakest models used in science. These statistical models when more powerful models with better predictive capabilities cannot be used for complexity reasons. Models are apothesis, unverified models, scientific theories, models verified by peer review, equations, formulas, complex scientific models, simulations of climate, weather, etc. Naive models that we create to simplify our own lives. Computer programs, and what is mathematics? It is a system that allows us to manipulate our models to cover more cases. Mathematics is the purest, most context free of all scientific disciplines. As such, its greatest value to humanity is in its role as a help discipline to all other disciplines. Einstein's famous equals MC squared model was derived using mathematical manipulation of other models known to Einstein at the time. But perhaps mathematics isn't as much a scientific discipline as an epistemological one. I may explore this aside later. Model use requires understanding. A good model is context free, since it maximizes the number of contexts it can be applied in. Newton's second law, F equals MA, works pretty much everywhere. We have forces, masses, and accelerations. The trade-off for this flexibility is that we ourselves need to understand the problem domain. In rocket science, when maneuvering in space, F equals MA will often work perfectly, but when you are applying it to the acceleration of your car, you need to account for lots of effects like friction between the road and the wheels, wind resistance, and the like. So, F equals MA, applied naively would give you the wrong answer if friction is involved. This demonstrates the main disadvantage with models. They require that both the model maker, scientists like Newton and the model users, STEM competent people everywhere, understand enough about the problem domain to know whether the model is applicable or not, and how to use it. This understanding is the expensive part of science, since using science requires first getting a solid science education in order to avoid mistakes when using models. And since models require understanding, they cannot be used to create understanding. This is a major problem for AI implementers. Chapter 3 2 Dirty Words Reductionism is the use of models. Holism is the avoidance of models. Matters are scientific models, theories, hypotheses, formulas, equations, superstitions, and most computer programs. Reductionism and Holism. After having sorted out what models are, we can now discuss two complementary problem-solving strategies, or perhaps meta-strategies. There are in many ways each other's opposites, but the classification can become an argument about novel levels and definitions. I will initially pretend the division is clear and obvious, and will elaborate later. Reductionism is the use of models. In this series we will use exactly the above definition of the word, reductionism. If you look up the definition elsewhere you may find that some sources divide the strategy into sub-strategies. They also seem to miss the most important sub-strategy, which we'll discuss later. But what all these sub-strategies have in common is that they all provide ways to simplify observations of fragments of our rich mundane reality into much simpler models, which we use for reasoning, computation, and sharing. Reductionism is so central to how we do science, the heavy reliance on models, such as theories, equations and formulas, and physics, chemistry, etc. That we can speak of model-based sciences or reductionist sciences where such model-making is easy and effective, and this classification excludes those sciences, like psychology, where such model-making is difficult and less often rewarded with reliable results. After considering all the advantages of models we might wonder why we even bother discussing it. Too many people, especially those with a solid stem, science, technology, engineering, and mathematics education, it may well look like the only choice, but there's also the other strategy. Homism is the avoidance of models. This is where the questions start. This is where the paradox is surface. This is where your worldview may get shaken up. Seriously, especially if you are a scientist or engineer with a solid stem education and decades of professional success using science and models. In some sense, the goal of this entire series is to demonstrate that we need to use both problem-solving strategies when creating our artificial intelligences, because that is what it is going to take. We need holistic understanding. We established that in the first chapter, as a sample of the new ideas that we will have to deal with I will just mention, reasoning is reductionist. Understanding is holistic. Newer networks are holistic. Holistic systems can jump to conclusions on scant evidence. Holistic systems can themselves know what is important and what isn't. Holistic systems can solve problems we ourselves cannot or don't care to understand. Holistic systems are model-free. We do not use any a priori models of any problem domain. Reasoning systems inherit all problems and benefits of reductionism. Understanding systems inherit all problems and benefits of holism. Humans are born holistic. Humans each solve thousands of little problems every day, and we are solving almost all these problems holistically, using understanding, and without a need to reason at all. This includes fluent language use. A stem education instills a strict reductionist discipline in order to mitigate problems with fallibility of holistic human minds. Our intelligences are fallible. These claims all deserve individual treatments, and we'll get to all of them in later sections. But the major theme is clear. Humans are mainly holistic problem solvers. This must be true for our artificial intelligences. We had several reasons for focusing on reductionist methods, models, and reasoning during the first 60 years of AI. Our computers were too small to make neural networks work at all. But there were also ideological reasons. AI was born out of the math and computer science departments of our universities, and therefore most of the people working on AI were solidly oriented towards the goal of creating a logic-based reductionist infallible artificial mind. To build early AIs, like expert systems, we entered rules or programmed in lots of facts to reason about. But this was budding reductionist castles in the air, comprised of unanchored facts that didn't tie to any understanding whatsoever. The troubles with classical AI, such as bitterness, the tendency to make spectacular and expensive mistakes at the edges of their competence, can be directly traced to the lack of foundational understanding to support these attempts at reasoning. Understanding machines will not suffer from this brittleness, but will fail gracefully at the edges of their competence, much like humans. Most of the time they will know the answer beyond that they will guess, and the guesses they make are based on a lifetime of experience, gained through learning from a large corpus and so they have a good chance of being at least a workable choice, if not perfect. How can anyone solve problems without using models? A lot of people coming from a STEM background cannot even imagine how to solve problems without using models. But it's not hard, once you understand the difference, mostly it's a matter of doing what worked last time. The problem is now figuring out whether we are in a situation that's similar enough that it will work again. This is mostly a pattern matching problem. More later, what's the result? The holistic answer is a quick guess at the best action, based on experience with similar situations. Most of the time it's correct, sometimes it's a little wrong, and every now and then, there's a noticeable mistake. And if we get things a little wrong, we may notice the outcome and correct the action. We learn from our mistakes. If we practice something a lot, we will start doing it effectively and perfectly every time. Do we learn faster if we make more mistakes? Should we make mistakes on purpose? More later, in situations where you cannot use models, which are more common than many realize, the holistic guess may also be your only option. Conversely, if you have an adequately well-working model-based solution, just use that. My video, Model-Free Methods Workshop demonstrates how the group solves four different problems at a high level, using both reductionist and holistic methods. Why are these dirty words? Well, they are not dirty to epistemologists. Reductionism has been the default problem-solving paradigm because it's the one that has to be taught. We are born with a holistic problem-solving apparatus. But reductionist science doesn't come naturally. Therefore, it has to be taught in schools, practiced, and carried out according to certain rules. Perhaps that's why the sciences are called disciplines, because following the ideal scientific method requires practice and constant vigilance. J. C. Smutsbrook, Holism and Evolution, 1926 established the terminology in the epistemological literature. And no inchrodinger wrote, what is life, 1944, questioning the power of physics to provide useful explanations to the life sciences. Percy Grote, zen and the art of motorcycle maintenance, 1974, had contrast something very holistic, zen Buddhism, with something very reductionist, motorcycle maintenance. So the chasm between the strategies was identified a long time ago. The strategies are each other's opposites. H-O-L-E-L-I-S-M-based strategies for understanding can handle many important kinds of complexity and can quickly provide a guest answer. But these guesses are fallible, and often more expensive to compute. Reductionist education and strategies brought benefits of cheap model reuse and formal rigor to improve correctness, but cannot handle complexity and is therefore dependent on an external understander to determine applicability in real-world complexity rich situations. And as part of that education, we are told that holistic methods, such as jumping to conclusions unscanned evidence, are bad, in spite of the fact that our brains use holistic methods thousands of times each day to successfully understand the environment we live in. We can all use either strategy as appropriate. If we don't have a STEM education, we will still sometimes make naive models. But sometimes there is a choice and different people may prefer one or the other. When playing pool, some people estimate and compute bouncing angles and some people shoot by feel. But we have our preferences, and it may be tempting to label a person with an overly strong preference as a holistic or a reductionist. This is sometimes received badly, if perceived as a limitation. Some dictionaries even flag reductionist as derogatory. And yet, some people use it as a self-assigned label. I try to use these terms only as shorthand for a person with a stated strong preference for holistic or reductionist methods. The two terms were very useful in epistemology. But then someone invented the concept of holistic medicine. Instead of just shooting a single medical problem, you analyze the patient's entire situation, attempting to account for diet, exercise, sleep, work, habits, stress levels, allergies, family, friends, and environmental poisons. A good idea, in general. But the wide scope was unmanageable by the, traditionally reductionist medical establishment and the idea faded away. Instead, the whole idea of holism became tainted as woo-woo in the term, holistic medicine, became associated with woo-woo merchants selling crystals and aromatherapy. As explained above, holism is the avoidance of models, or better phrased, holism is the metastrategy of avoiding a priori models of the problem domain. That extra precision rarely matters. There's nothing woo-woo about it. It does say, science not required, but, you can make breakfast without reasoning. It is important to note that holistic methods are based on a lifetime of experience, in humans and a training corpus worth of experience, in neural networks. When you're making breakfast, you are relying on this experience, mostly repeating whatever worked yesterday. Some people claim they use reasoning while making breakfast, but they can make their breakfast while speaking to someone else on the phone. And as they hang up, they find themselves suddenly sitting at the breakfast table with their coffee and hot oatmeal. Same thing when driving to work. You may get lost in thought, and then you find yourself parked at work. You didn't need to reason, since all sub-problems that occur in driving had been solved multiple times, during years of driving. Sub-conscious understanding is used for simple things like sequencing our leg muscles as we walk. You have no idea how you are doing that, it just works. Same thing with vision. You understand that you are looking at a chair, but you do not have conscious access to the 15th rod cone pixel to the left of your center of vision, and have no idea how this understanding works. Same thing with understanding and generating language. You do not have any explanation for how you are able to understand the meaning of this sentence. Understanding is sub-conscious and holistic. So for the majority of things we do every day, we do not need reasoning or reductionist methods. Some people would like to think they are, logical thinkers, immune to most cognitive fallacies, but whether they are or not, at the lower levels, everyone is solving most of their problems holistically. I claim that reductionist reasoning requires holistic understanding. In other words, I need to understand the problem domain at hand before I can create and reuse models to enable me to reason about the domain. So holistic understanding is much more important than reductionist reasoning because it is the most used strategy, by far, and the former is also a prerequisite for the latter. But the fallibility of holistic understanding forced us to create reductionist science and to teach it in STEM education. It is as if the purpose of science is to keep holistic guessing in check, but this aversion to fallibility has a cost, because it means complexity bound and irreducible problems cannot be solved. Like language understanding, global resource allocation, and social interactions, reductionism and model-based science appeared around 1650 after a century of gestation. Excluding minor romantic interludes, it has held its position as the dominant paradigm for about 400 years. This is changing. The reductionist train is running out of track. The remaining hard problems facing humanity are problems of irreducible complexity in domains where reductionist model-based methods simply cannot work. Whether we like the idea or not, we need to accept these holistic methods into our AI toolkits. Starting now, we will use these methods either in their raw form, as model-free methods, or as understanding machines at any level from component to robot co-worker. Chapter 4. Reduction. Epistemic reduction is a process that discovers higher-level abstractions and lower-level data by discarding everything at the lower layer that it recognizes as irrelevant. We have seen the power of models. We have introduced the two problem-solving meta-strategies of reductionism and holism. We also noted that the creation and use of models requires an intelligent agent that understands the problem domain. Someone or something has to perform the reduction. I will now discuss reduction in some detail. Until 2012, only humans and other animals with brains could perform reduction. Now our deep neural networks, DNN, can perform limited reduction. How do brains and DNNs accomplish this? And how can we improve these algorithms? This may be, to some readers, the most rewarding part of this series, because it provides you the opportunity to learn a new and useful skill. Most people never think about the world at this level. Knowledge of reduction provides a new point of view that you can use to better understand your environment, other intelligent agents around you, and modern AI systems. Definition of reduction. Reduction is a process that discovers higher-level abstractions and lower-level data. We will initially note that reduction is exactly the same as abstraction. Why do we need a new word? Because the term abstraction is mostly used by scientists already operating in a pure model space, seeking a higher level of abstraction in that space. But to them, abstraction is something that just magically happens in their heads, since there are no scientific theories for how abstraction works. There cannot be, since abstraction is a concept in epistemology, not science. AI researchers are starting from something much closer to a rich mundane reality, where there is a lot of confounding context. We are solving the metal problem of how to move from there into a space that is sufficiently abstract to solve the problem at hand. Here, reduction is a much more appropriate term. We can abstract the red pixel or the letter B, but we can reduce a rich context containing that pixel or letter into a higher-level concept. We are swimming in reduction. Paradoxically, one of the hardest things about teaching reduction is that we don't see the need to learn about it because we all do it all the time, every millisecond, and the resulting reductions, models, become available to our conscious minds as if, by magic, brains reduce away 99.999% of their sensory input, but this process is subconscious and hence invisible to us. The situation is much like, supposedly, a fish swimming in water. We are all masters of reduction, but we don't know how we do it or that we even do it. We didn't know this would ever matter. And generally, it doesn't. Well, it matters in epistemology, and it matters in AI, since we need to actually implement that magic. We as epistemologists must know how abstraction is actually performed, and we give the epistemology-level equivalent of abstraction the name reduction, because that's the recipe for how to accomplish it. We reduce our rich mundane reality by discarding, reducing away, what's irrelevant. And by using the name reduction, we, as AI epistemologists, keep reminding ourselves how it is properly done. Consider the following descriptions of a car. The slide is meant to be read from the bottom up, to match abstraction levels from low to high. If I'm driving to work, I better be driving my car. If the police are looking for a stolen car, they would be looking for red 2010 Toyota Celica. If I'm buying a new car, then I might be looking for just a new Toyota Celica. And a self-driving car would likely only need to understand whether an obstacle is a vehicle or not, in order to model maximum speed for future movement. We see that we want to pick the appropriate level of abstraction to deal with the same object, or topic, in different situations. But more importantly, we see that we can get from a more detailed description, at the bottom, to a more generic one, higher up, by simply discarding some detail. I hasten to point out that reduction is more complicated than this simple example of decreasing specificity shows. What we need to start somewhere in this image allows us to form intuitions that will serve for a while. True reduction involves operations like shifting from syntax to semantics or from instance to type. The appearance of car as an abstraction of Toyota, and the step from my Toyota to a Toyota illustrates these steps. Algorithms for these things are known. Salience, part of the trick is to know what to discard. At each level of abstraction, something can typically be identified as the least important property. Red and Celica are more significant than 2010 for anyone looking for a car. If we had started from my red 2010 Toyota truck, then the word truck would not be discarded until the top level. Reduction requires understanding what's relevant. In reduction we keep that which is salient. More later, partial reductions. Most of the time we do not perform reduction all the way to models. I cannot stress this enough. We discuss reduction to models for pedagogical reasons. It is easy to initially see the context free model as the goal of reduction. In reality, in brains, we can stop reducing the moment we recognize that we have a working answer or response, such as a command to contract some muscle or having understood the meaning of a sentence subconsciously. At this point, there is still some residual context but we use that context productively rather than discard it to move to higher levels. Some people claim we use models for all our thinking, but I'm using capital M model only to describe a completely context free abstraction. F equals M A is an example of that. There is no need to check whether a car is a red car or a Toyota. The equation works not only for all cars but for all forces, masses and accelerations. We might come up with a special equation for acceleration of Tesla cars which would require different inputs like battery charge level and software settings. That would not be a context free model since it would not work on a Toyota. For almost all tasks, basically, in everything except science and even there, only rarely, we only perform as much reduction as is necessary to get the job done. When learning to ski, you only figure out how you yourself need to perform given your body and equipment. We do not need to parameterize our skiing skills for someone with twice the body mass because that would be useless to us for the purpose of our own skiing. But a scientist would have to go that far in order to parameterize away one more piece of context from the model they are creating. For instance, when creating a skiing video game or designing a new ski, if we consider the enormous amount of subconscious activity that happens in the brain, we can safely say that partial reductions are the most common reductions. For instance, when we take a step forward, our subconscious has analyzed our posture and velocity by using reduction based on low level nerve signals and is commanding leg muscles to contract an up precisely timed sequence. This activity is something we are unaware of. Most of us don't even know what leg muscles we have. And there would be no time to perform reduction all the way to models. That process takes a minimum of a half second and you don't have that kind of time available to respond to an imbalance when walking or skiing. Reduction in society. Most of us get paid to understand whatever we need to understand in order to perform our jobs. In other words, most of us get paid to do reduction. If you are approving building permits, you reduce a stack of forms to a one bit verdict of approved or rejected. We accelerate reduction, and this is the main reason most of us haven't been replaced by robots. But we see that when future understanding machines can perform reduction by themselves, then we are unlikely to get paid for it. Levels of reduction. Suppose a young man and a young woman fall in love, something happens to mess it all up, and then they sort this out and reunite. This is what happened in the man's, which mundane reality. Suppose the man wants to share this experience, because there was some moral to the story that he thinks would be interesting to others and possibly important. He could analyze what happened and figure out which were the key events in the saga and then have actors on a stage re-enact the story as a play. This is a reduction because the boring parts of the story would not be part of the play. They are discarded as irrelevant, but the story would be acted out by real people in front of a live audience. If you are in the audience, you can move your head to see behind any actor on the stage and you can clearly see everything on the stage, not just one actor speaking at a time. He can make a movie about it. Now your point of view is pre-defined by the camera angle and cropping. You can no longer see behind an actor, and you can often only see those actors that are involved in the main action. He could write a book about it. We no longer can see even the people described in the book, except in our imagination. A critic review in the theater play may reduce it to, Boy meets girl, Boy loses girl, Boy gets girl. A drama school graduate may summarize it as a double reversal plot. This is a description that is so free from context, doesn't even specify boys or girls that it could be argued it qualifies to be called a model. Plays, movies, books, stories, tropes, etc. are all partial reductions of reality, and some are more reduced than others. Just like in the red Toyota case, we need to find the appropriate level of abstraction to work with. The young man in the example, when writing a book or a screenplay, has much in common with a scientist trying to describe something in nature in a reusable context free manner by reducing it to a model. They are model makers, or are at least performing partial reduction. They are discarding the irrelevant bits. The opposite of reduction. We also need to be able to move in the opposite direction, from models to reality, or at least from more abstract partial models to partial models closer to reality. When an actor is given a screenplay, they know it only contains rough directions for what to do and what lines to say. The actor's job is to give a little of themselves to flesh out the screenplay to actual actions, including creating, synthesizing, the appropriate display of emotions, tone of voice, and body language. They use their experience as people and as actors. They use elements of their past lives and skills they have acquired by training to create something people in the audience might relate to. For example, they may repurpose a personal experience. He is sad as when my hamster died. Things they learned in drama school, such as speaking, singing, dancing, and swordplay, from other actors, what would bogart do, from fiction, from other movies and plays, etc. The actor's artist who convey whatever the script intends to convey, emotions, a morality cookie, a political position, titillation, surprise, and so on. Starting from the simple model, the screenplay, their job is similar to an engineer's when they are faced with a problem and use a model to solve it. The engineer would use their experience to decide that M is the mass of the car and not the tire pressure. The actor decides that sadness is more appropriate than grief for a certain scene, etc. I call this process, which is the opposite of reduction by the name it is used in problem solving application. We use a model to simplify a problem situation, moving it into an abstract and pure model space. We solve the problem there by performing math, perhaps, and then apply the answer to our rich reality to the problem we are trying to solve. Many of you may recognize the word application or its abbreviation, app. That's not as far-fetched as it might seem. Apps are software-based models. Reduction in application and brains. Back to the issue of partial reductions. Consider the actor reading a screenplay. They are using their eyes to gather pixels of color and orientation. The brain then performs pattern matching, reduction, from these low-level signals to letters, words, to language, to high-level concepts like love and separation, and eventually to a high-level understanding of the playwright's intents. The actor then takes this high-level understanding and by performing application, they add their own experience to the script to get closer to reality and their performance. Our brains are capable of moving up and down many levels of abstraction at once. Perhaps it tracks all of them simultaneously, keeping layers of abstraction separate. This is a clue for why deep neural networks perform better than shallow ones. Which is what we'll discuss next. Chapter 5. Why Deep Learning Works. Deep learning performs epistemic reduction. A math-free computer science-free description of why deep learning works. We have now built a base of theory for why AI works, what models are, and how to create them, what reductionism and holism are, and what the process of reduction is. These are the fundamentals of AI epistemology. This base allows us to discuss various strategies to move towards understanding machines in a well-understood and controlled manner. We are now ready to discuss why deep learning, DL, works. This is the fifth and last entry in the AI epistemology primer. Deep learning performs reduction. This is an unsurprising claim, considering the preceding chapters. There are several mutually compatible theories for how deep learning works. But just as in the first chapter, we will now discuss the epistemological aspects, why it works, from several viewpoints and levels, starting from the bottom. We would use examples from the TensorFlow system and API as a library, as a stand-in for all deep learning family algorithms and TF programs, because the available API functions heavily shape and constrain solutions that can be implemented in this space. And the generalization should be straightforward enough. Consider the following illustration of image understanding using Keras, an excellent abstraction layer on top of TensorFlow. I like to refer to the input layer as being on the bottom rather than at the far left as in this image. When viewing it my way, the low to high dimension we use in my rotated version of the image can be mentally mapped to a low to high stack of abstraction levels. I'm not the only one using this dimension this way. I hope this rotation isn't too confusing. We can see that there is an obvious data reduction and an obvious complexity reduction. Can we determine whether the system is also performing what I'd like to call the epistemic reduction? Is it reducing a way that which is unimportant? And if so, how does it accomplish this? How does an operator in a deep learning stack know what makes something important? Salient, up your data, reduction of sorts could be accomplished by compression schemes or even random deletion. This is undesirable. We need to discard the non-salient parts so that in the end, we are left with what is salient. Some people have not understood the importance of salient's based reduction and useless compression power of reversible algorithms as a measurement of intelligence, which is no more useful than believing a simple video camera can understand what it sees. So let me conjure up a bit like in the movie, Inside Out, a fairy tale of what goes on in a deep learning network, except we'll do it, bottom up. Suppose we have built a system for finding faces in an image with the intent of incorporating that as a feature in a camera. Many cameras have this feature already, so this is not a far-fetched example. We implement an image understanding neural network, show the system many kinds of images for a few days, perhaps using so-called supervised learning in order to improve this story, and then we show it an image of a family having a picnic in a park and ask the system to outline where the faces are so that the camera can focus sharply on them. The input image is converted from RGB color values to an input array and the data in this array is then shuffled through many layers of operators. And for many of these layers, there are fewer outputs than there are inputs, as you can see above, which means some things have to be discarded by the processing. Each layer receives initially signals, from below, that is, from the input, or from lower levels of abstraction, and produces some reduced output to send to the next layer operator above. To continue detail, at some early level, some operator is given a few adjacent pixels and determines that there is a vertical, slightly curved line dividing the darker green area from the lighter green area. So it tells the operator above the simpler line or color-based description using some encoding we don't really care about. The operator at the level above might have gotten another matching curve and says, these match what I saw a lot of when the label blade of grass was given as a ground truth label during supervised learning. If no label is known, then we again assume some other uninteresting representation. It is okay to propagate results without human-labeled signals because whatever signaling scheme is used will be learned by the level above. The operator above that says, when I get lots of blades of grass signals, I reduce all of that to a long signal as I send it upward. And eventually we reach the higher operator layers and someone there says, we are a face-finder application. We are completely uninterested in lawns and discards the lawn as non-cellient. What remains after you discard all non-faces are the faces. You cannot discard anything until you know what it is, or can at least estimate whether it's worth learning. Specifically, until you understand it at the level of abstraction you are operating at. The low-level blade of grass recognizers could not discard the grass because they had no clue about the high-level saliencies of lawn or not in face or not that the higher layers specialize in. You can only tell what salient or not, important or not at the level of understanding and abstraction you are operating at. Each layer receives lower-level descriptions from below, discards what it recognizes as irrelevant, and sends its own version of higher-level descriptions upward until we reach someone who knows what we are really looking for. This is of course why deep learning is deep. This idea itself is not new. It was discussed by Oliver Selfridge in 1959. He described an idea called, Pandemonium, which was largely ignored by the AI community because of its radical departure from the logic-based AI promoted by people like John McCarthy and Marvin Minsky. But Pandemonium presaged, by almost 60 years, the layer-by-layer architecture with signals passing up and down that is used today in all deep neural networks. This is the reason my online handle is at Pandemonica. So do any TensorFlow operators support this reduction? Let's start by examining the pooling operators. There are a few in the diagram. They are conceptually simple. There are over 50 pooling operators in TensorFlow. There is an operator named 2x2 Max Pool operator. In the diagram, it is used four times. It is given four inputs with varying values and propagates the highest value of those as its only output. Close to the input layer of these four values may be four adjacent pixels where their values might be a brightness in some color channel, but higher up they mean whatever they mean. In effect, the Max Pool 2x2 discards the least important 75% of its input data, preserving and propagating only one highest value. In the case of pixels, it might mean the brightest color value. In the case of blades of grass, it might mean there is at least one blade of grass here. The interpretation of what is discarded depends on the layer, because in a very real sense, layers represent levels of reduction, abstraction levels, if you prefer that term. And we should now be clearly seeing one of the most important ideas in deep neural networks, the reduction has to be done at multiple levels of abstraction. Each set of decisions about what is reduced away as irrelevant and what is kept as possibly relevant can only be made at an appropriate abstraction level. We cannot yet abstract away the lawn if all we know is there are dark and light green areas levels. This is a simplification. Decisions made in this manner will be heated only if they have contributed to positive outcomes in learning. Unreliable and useless decision makers will be ignored using any of several mechanisms that we may apply during learning. More later, for now, we continue by examining the most popular subset of all TensorFlow operators. The convolution family from the TensorFlow manual, note that although these ops are called convolution, they are strictly speaking cross correlation. Convolution layers discover cross correlations and co-occurrences of various kinds. Co-occurrences to known patterns in the image at various locations. Spatial relationships within an image itself, like Jeff Hinton's recent example of the mouth normally being found below the nose. And more obviously, in the supervised learning case, correlations between discovered patterns and the available meta-information, tags, labels that correlate with the patterns the system may discover. This is what allows an image-understander to tag the occurrence of a nose in an image with the text string nose. Beyond this, such systems may learn to understand concepts like behind and under. The information that is propagated to the higher levels in the network now describes these correlations. Uncorrelated information is viewed as non-salient and is discarded. In the Crescent diagram, this discarding is done by a max pooling layer after the convolution plus ReLU layers. ReLU is a kind of layer operator that discards negative values, introducing a non-linearity that is important for DL but not really important for our analysis. This pattern of three layers, convolution, then ReLU, then a pooling layer, is quite popular because this combination is performing one reliable reduction step. These three-layer types in this packaged sequence may appear many times in a DL computational graph. In each of these three-layer packages is reducing away things that levels below had no chance of evaluating for saliency because they didn't understand their input at the correct level. Again, this is why deep learning is deep because you can only do reduction by discarding the irrelevant if you understand what is relevant and irrelevant at each different level of abstraction. Is deep learning science or not? While the deep learning process can be described using mathematical notation, mostly using linear algebra, the process itself isn't scientific. We cannot explain how this system is capable of forming any kind of understanding by just staring at these equations, since understanding is an emergent effect of repeated reductions over many layers. Consider the convolution operators. As the TF manual quote clearly states, convolution layers discover correlations. Many blades of grass together typically means a lawn. In TF, a lot of cycles are spent on discovering these correlations. Once found, the correlation leads to some adjustments of some way to make the correct reduction more likely to be rediscovered the next round, because this reduction is done multiple times. But in essence, all correlations are forgotten and have to be rediscovered in every path through the deep learning loop of upward signaling and downward gradient descent with minute adjustments to erring variables. This system is in effect learning from its mistakes, which is a good sign, since that may well be the only way to learn anything. At least at these levels. This up and down may be repeated many times for each image in the learning set. This up and down makes some sense for image understanding. Some are using the same algorithms for text. Fortunately, in the text case, there are very efficient alternatives to this ridiculously expensive algorithm. For starters, we can represent the discovered correlations explicitly, using regular pointers or object references in our programming languages. Or, synapses in brains. This software neuron correlates with that software neuron says a synapse or reference connecting this to that. We shall discuss such systems in the section on organic learning, which is coming up next. Then either the deep learning family of algorithms, or organic learning, are scientific in any meaningful way. They jump to conclusions on scant evidence and trust correlations without insisting on provable causality. This is disallowed in scientific theory, where absolutely reliable causality is the coin of the realm. F equals m a or go home. The most deep neural network programming is uncomfortably close to trial and error, with only minor clues about how to improve the system when reaching mediocre results. Adding more layers doesn't always help. These kinds of problems are the everyday reality to most practitioners of deep neural networks. With no a priori models, there will be no a priori guarantees. The best estimate of the reliability and correctness of any deep neural network, or even any holistic system we can ever devise, is going to be extensive testing. We're on this later. Why would we ever use engineered systems that cannot be guaranteed to provide the correct answer? Because we have no choice. We only use holistic methods when the reliable reductionist methods are unavailable. As is the case when the task requires the ability to perform autonomous reduction of context rich slices of our rich complex reality as a whole. When the task requires understanding, don't we have an alternative to these under liable machines? Sure we do. There are billions of humans on the planet that are already masters of this complex task because they live in the rich world and need skills that are unavailable with reductionist methods, starting with low level things like object permanence. So you can replace a well performing but theoretically unproven contraption, a holistic understanding machine built out of deep neural networks, with a well performing human being using a deeply mystical kind of understanding hidden in their opaque heads. Who earns much more per hour. This doesn't look like much of an improvement. The machine cannot be proven correct because it doesn't function like normal computers. It is performing reduction, the skill formally restricted to animals. A holistic skill. My favorite soundbite is a mere corollary to the frame problem by McCarthy and Hayes. You have seen it and you will see it again, since it is one of the stronger results of AI epistemology. But we will, in but a few years, agree on a definition of intelligence that makes autonomous reduction a requirement. This once semi-heretic soundbite will then be obvious to all. If it isn't already, our intelligences are fallible. Chapter 6. Experimental Epistemology for AI We can now create computer based experimental implementations to epistemology level theories in order to test them and learn from the outcomes. Experimental epistemology is the use of the experimental methods of the cognitive sciences to shed light on debates within epistemology, the philosophical study of knowledge and rationally justified belief. Some skeptics contend that experimental epistemology or experimental philosophy more generally is an oxymoron. If you are doing experiments, they say, you are not doing philosophy. You are doing psychology or some other scientific activity. It is true that the part of experimental philosophy that is devoted to carrying out experiments and performing statistical analyses on the data obtained is primarily a scientific rather than a philosophical activity. However, because the experiments are designed to shed light on debates within philosophy, the experiments themselves grow out of mainstream philosophical debate and their results are injected back into the debate, with an item moving the debate forward. This part of experimental philosophy is indeed philosophy, not philosophy as usual perhaps, but philosophy nonetheless. Experimental epistemology by James R. B. B. Traditional experimental epistemology conducted experiments on interviews and psychological tests on human volunteers or relied on population statistics. As one of the newer branches of cognitive science, machine learning has now provided us with a very different approach to this domain. We can now create computer-based experimental implementations to epistemology level theories in order to test them and learn from the outcomes. In machine learning, the most important epistemology level concepts and hypotheses are about reasoning, understanding, learning, epistemic reduction, abstraction, creativity, prediction, attention, instincts, intuitions, concepts, resiliency, models, reductionism, wholism, and other things all sharing these features. One, science has no equations, formulas, or other models for how they work. They're epistemology level concepts, not science level concepts. Two, our theories about these concepts have to be sufficiently solid and detailed to allow for computer implementations. This is because science itself is built on top of epistemology level concepts, and practitioners need to be aware of this or they will experience cognitive dissonance-induced confusion and stress. The red pill of machine learning confronts the elephant in the room of machine learning. Machine learning is not scientific. What can we learn from AI epistemology? An excerpt from the red pill can say the following statements from the domain of epistemology and how each of them can be viewed as an implementation hint for AI designers. We are already able to measure their effects and system competence. You can only learn that which you already almost know. Patrick Winston, MIT. Our intelligences are fallible. Monica Anderson. In order to detect that something is new, you need to recognize everything old. Monica Anderson. You cannot reason about that which you do not understand. Monica Anderson. You are known by the company you keep, simple version of the Yanida Lemur from Category Theory and the justification for embeddings in deep learning. All useful novelty in the universe is due to processes of variation and selection. The selectionist manifesto. Selectionism is the generalization of Darwinism. This is right genetic algorithms work. Science has no equations for concepts like understanding, reasoning, learning, abstraction, or modeling since they are all epistemology level concepts. We cannot even start using science until we have decided what model to use. We must use our experience to perform epistemic reductions, discarding the irrelevant, starting from the messy real world problem situation until we are left with a scientific model we can use, such as an equation. The focus in AI research should be on exactly how we can get our machines to perform this pre-scientific epistemic reduction by themselves and the answer to that cannot be found inside of science. Chapter 7. The Red Pill of Machine Learning. Reductionism is the use of models. Holism is the avoidance of models. Models are scientific models, theories, hypotheses, formulas, equations, naive models based on personal experiences, superstitions, and traditional computer programs. The deep learning revolution of 2012 changed how we think about artificial intelligence, machine learning, and deep neural networks. What changed, and what does this mean going forward? The new cognitive capabilities in our machines are the result of a shift in the way we think about problem solving. It is the most significant change ever in artificial intelligence AI, if not in science as a whole. Machine learning, ML based systems are successfully attacking both simple and complex problems using novel methods that only became available after 2012. We are experiencing a revolution at the level of epistemology which will affect much more than just the field of machine learning. We want to add more of these novel methods to our standard problem solving toolkit, but we need to understand the trade-offs and the conflict. I argue that understanding deep neural networks, DNNs, and other ML technologies requires that practitioners adopt a holistic stance which is, at important levels, blatantly incompatible with the reductionist stance of modern science. As ML practitioners we have to make hard choices that seemingly contradict many of our core scientific convictions. As a result we may get the feeling something is wrong. The conflict is real and important and the seemingly counter-intuitive choices make sense only when viewed in the light of epistemology. Improved clarity in these matters should alleviate the cognitive dissonance experienced by some ML practitioners and should accelerate progress in these fields. The title refers to the eye-opening clarity some machine learning practitioners achieve when adopting a holistic stance. Parallel dichotomies sentient sync research is natural language understanding, NLU. We are creating novel systems that allow computers to learn to understand human natural languages. Any one of them, we use deep neural networks of our own design. The goal is to achieve some kind of human-like but not necessarily human-level understanding. This is very different from traditional natural language processing, NLP, which relies on human-made models of some language, such as English, and perhaps models of fragments of the world. The NLP and NLU disciplines have chosen opposite answers to their difficult two-way choices. They are now defined by these choices, and we can use their stances to highlight the main conflict. The split is so deep that it cuts through many layers of our reality. The following dichotomies are all manifestations of this incompatibility at different levels, listed by impact, but discussed in no particular order. The main science, the complex, including the mundane, epistemology, reductionism, realism, meanings, reasoning, understanding, problem solving, plan it, then do it, just do it. Artificial intelligence, 20th century, good, old-fashioned AI machine learning, deep neural networks, natural language and computers, NLP, NLU. The problem-solving level provides many familiar examples of these issues. In our mundane lives, we solve many kinds of problems every day but our strategies for solving them fall into just those two categories. For any complicated problem, we had better have a plan before we start, but most problems the brain deals with every day are things we never have to think about because we do not need to plan a reason about them. These are the millions of low-level problems we encountered in our mundane life every day, and this is the world that our AIs will have to operate in. Consider someone walking across the floor. Their brain signals their leg muscles to contract in the correct cadence. Do they need to consciously plan each step? Do they reason about how to maintain their balance? No. They probably don't even know what leg muscles they have. Consider understanding this sentence. Did you use reasoning? Did you use grammar? If you are a fluent speaker, you do not need grammars to understand or produce language, and you do not have time to reason about language while hearing it spoken. Reasoning is slow, but understanding is instantaneous. Consider someone braking for a stoplight. How hard should they push on the brake pedal? Do they compute the required differential equation? Should such equations be part of the driver's license test? Consider someone making breakfast. Did they have to reason about anything or plan anything, or did they just do what worked yesterday, without thinking about it? Without consciously planning it? Walking and talking, braking and breakfasting, like almost everything we use our brains for, rely on learning from our experiences in order to reuse anything that has worked in the past, and, over time, we learn to correct our mistakes. These strategies are simple enough that we can identify them in other life forms. Dogs understand a lot but do not reason much, and we can see how they could be implemented in something like neurons and brains. The split in our brains between reasoning and understanding was examined at length in Thinking Fast and Slow by Daniel Kahneman. The absolute majority of the brain's effort is spent processing low-level sensory input, mostly from the eyes. He calls this System 1. It provides understanding. Reasoning is done by System 2 based on the understanding from System 1. What most problems we deal with on a daily basis do not require System 2 at all. Artificial intelligence and machine learning computers can solve any suitable problem when given sufficient human help, such as a complete plan for the solution in the form of a computer program and valid input data. But since the AIML Revolution of 2012, we now know how to make computers understand certain problem domains through machine learning. The acquired understanding allows the machine to just do it for many different problems in the domain, without any human planning, reasoning, or programming, and using incomplete, unreliable, and noisy input data. This is changing how we are building systems with cognitive capabilities. Everyone working in ML or AI needs to understand the trade-offs we must make at the most fundamental, epistemological levels. Modern ML requires examining and seriously rethinking many things we were taught to vigilantly strive for in our science, technology, engineering, and mathematics STEM educations. Things like correlation is bad, but causality is good and do not jump to conclusions on scant evidence are still solid advice everywhere inside science. But when building understanding systems, these established strategies and modes of thinking no longer work, because correlation discovery and handling of sparse, unreliable, and inconsistent input data are exactly the kinds of tasks we will have to perform and perform well at these pre-scientific levels. In order to understand how to do this, we must switch to a holistic stance. A motivating example, beginning machine learning students are given exercises like this. They are given a large spreadsheet, which lists data about houses sold a certain year in the US. This information includes among other things, the zip code of the house, the living area and square feet, lot size, the number of bedrooms and bathrooms, the year the house was built, and the final sale price of the house. We would like to be able to predict this final sale price, given the corresponding data for current house we are about to list for sale. The given spreadsheet is the data the student will use to train a deep neural network. It is the entire learning corpus. It contains everything the system will ever know. These students can download deep neural network libraries like Keras and TensorFlow and runnable examples for many kinds of problems, including useful training data from places like Hugging Face and GitHub. Next the student trains, learns their network using the given data. This may take a while, but when learning finishes, they can give the system data for a house it has never seen and it will quite reliably predict what the house might sell for. This was the goal of the exercise. The student has created a system that understands how to estimate real estate prices from listings, but the student still does not understand anything about real estate. The predictive capability that many people working in real estate would be willing to pay money for is 100% based on understanding in the deep neural network, in the computer, and because all the libraries in many pre-sold examples of this nature were freely available, the student did not have to do much programming either. The vision. This is desirable. This is what AI should mean. The computer understands the problem so that we don't have to. Programming in the future will be like having a conversation with a competent coworker, and when the machine understands exactly what we want done, it will simply do it. No programming required on our part or on part of the machine, once a suitable, partially reductionist framework exists. The rest is learning and it can be done in any human language with equal ease. We are on the right track towards something worthy of the name AI with current machine learning. Going forward, there are thousands of paths to choose from, and the ability to choose wisely will depend on our ability to understand and adopt a holistic stance. Reductionism and Holism. These are important terms of the art in epistemology. Both of them have numerous correct, useful, and compatible definitions. We will henceforth use the following definitions for reasons of usefulness and simplicity. Reductionism is the use of models. Holism is the avoidance of models. Models are scientific models, theories, hypotheses, formulas, equations, naive models based on personal experiences, superstitions, if you can believe that, and traditional computer programs. In the reductionist paradigm, these models are created by humans, ostensibly by scientists, and are then used, ostensibly by engineers, to solve real world problems. Model creation and model use both require that these humans understand the problem domain, the problem at hand, the previously known shared models available, and how to design and use models. A PhD degree could be seen as a formal license to create new models. Mathematics can be seen as a discipline for model manipulation. But now, by avoiding the use of human-made models and switching to holistic methods, data scientists, programmers, and others do not themselves have to understand the problems they are given. They are no longer asked to provide a computer program or to otherwise solve a problem in a traditional reductionist or scientific way. Holistic systems like DNNs can provide solutions to many problems by first learning about the domain from data-insult examples, and then, in production, to match new situations to this gathered experience. These matches are guesses, but with sufficient learning, the results can be highly reliable. We will initially use computer-based holistic methods to solve individual and specific problems, such as self-driving cars. Over time, increasing numbers of artificial understanders will be able to provide immediate answers, guesses, to wider and wider ranges of problems. We can expect to see cell phone apps with such good command of language that it feels like talking to a competent co-worker. Voice will become the preferred way to interact with our personal AIs. Early and low-level but useful AI will manifest as computers that can solve problems we ourselves cannot or cannot be bothered to solve. They need not be superhuman. All they need to have in order to be extremely useful is exactly the ability to autonomously discover higher-level abstractions in some given problem domain, starting from low-level sensory input, for example, by learning from images or reading books. Such systems now exist. If we want to understand machine learning, then we need to understand all the strategies in the right most column in the tables that follow. They are all part of a holistic stance, and if we are working in machine learning, we need to adopt as many of them as possible. Differences at the level of epistemology. Reductionism in Science versus Holism in Machine Learning. The use of models versus the avoidance of models. Raising versus understanding requires human understanding versus provides human-like understanding. Problems are solved in an abstract model space versus problems are solved directly in the problem domain. Unbeatable strategy for dealing with a wide range of suitable problems faced by humans. Versus may handle some problems in domains where reductionist models cannot be created or used, known as bizarre domains. Handles many important complicated problems such as going to the moon or a highway system. Versus handles many important complex problems such as protein folding and playing go. Handles problems requiring planning or cooperation. Versus handles simple mundane problems such as understanding language or vision or making breakfast. Money rows in these tables discuss hard trade-offs where compromises are impossible or prohibitively expensive. These are identified by bold face numbers in the first column. The meaning rows may not be clear trade-offs or even disjoint alternatives. Mixed systems are described in a separate chapter. These form the core of these dichotomies and are discussed in most of what follows, but also in detail at the chapter on introducing AI epistemology and in videos of talks. A leather report is based on models in meteorology. To solve the problem directly in the problem domain, open a window to check if it smells like rain. Reductionism is the greatest invention our species has ever made. But reductionist models cannot be created or used when any one of the multitudes of blocking issues are present. Models work, in theory or in a laboratory where we can isolate a device, organism or phenomenon from a changing environment. However, complex situations may involve tracking and responding to a large number of conflicting and unreliable signals from a constantly changing world or environment. Reductionism is here at a severe disadvantage and can rarely perform above the level of statistical models. In contrast, holistic machine learning methods learning from unfiltered inputs can discover correlations that humans might miss and can construct internal pattern-based structures to provide recognition, epistemic reduction, abstraction, prediction, noise rejection and other cognitive capabilities. Humans generally use holistic methods for seemingly simple, but in reality, complex mundane problems like understanding vision, human language, learning to walk, or making breakfast. Computers use them for very complex problems and mel-based AI in general, such as protein folding and playing go, but also simpler ones, such as real estate pricing. Main trade-offs. Reductionism in science versus realism in machine learning. Optimality, the best answer, versus economy, reuse no useful answers. Completeness, all answers, versus promptness, except first use for a answer. Repeatability, same answer every time, versus learning, versus learning, results improve with practice. Extrapolation, in low-dimensionality domains, versus interpolation, even in high-dimensionality domains. Transparency, understand the process to get the answer, versus intuition, accept useful answers even if achieved by unknown or subconscious means. Explainability, understand the answer, versus positive ignorance, no need to even understand the problem or problem domain. Shareability, abstract models are taught in communicated using language or software, versus copyability. ML understanding, a competence can be copied as a memory image. Optimality, completeness, and repeatability are only available in theoretical model spaces and sometimes under laboratory conditions. Economy and promptness had much higher survival value in evolutionary history than optimality and completeness. The strongest hint that a system is holistic is that the results improve with practice because the system learns from its mistakes. In machine learning, a larger learning corpus is in general better than the smaller one because it provides more opportunities for making mistakes to learn from, such as corner cases. Models created by humans have manageable numbers of parameters because the scientist or engineer working on the problem has done a, hopefully correct, epistemic reduction from a complex and messy world to a computable model. This allows experimentation with what if scenarios by varying model parameters. It is up to the model user to determine which extrapolations are reasonable. In holistic ML systems, we are getting used to systems with millions or billions of parameters. These structures are very difficult to analyze, and just like with human intelligences, the best way to estimate their competence is through testing. Extrapolation is typically out of scope for holistic systems. The majority of end users will have no interest in how some machine came up with some obviously correct answer. They will just accept it the way we accept our own understanding of language, even though we do not know how we do it. We now find ourselves asking our machines to solve problems we either don't know how to solve, or can't be bothered to figure out how to solve. We have reached a major benefit of AI. We can be positively ignorant of many mundane things and will be happy to delegate such matters to our machines so that we may play or focus on more important things. Some schools of thought tend to overvalue explainability. To them, ML is a serious step down from results obtained scientifically where we can all inspect the causality, for instance in a reductionist production, expert systems. But the bottom line is that today we can often choose between one, understanding the problem domain, problem, the use of science and relevant models, and the answer. Or two, just getting a useful answer without even bothering to understand the problem or the problem domain. The latter, positive ignorance, is a lot closer to AI than the first, and we can expect the use of holistic methods to continue to increase. Science strives towards a consensus world model in order to facilitate communication and minimize costly engineering mistakes caused by ignorance and misunderstandings. Scientific communication requires a high-level context, a world model, shared by participants, and agreed upon signals such as words, math, and software. But direct understanding, such as the skills to become a just grandmaster or a downhill skier, cannot be shared using words. The experience must be acquired using individual practice. Computer-based systems that learn a skill through practice can share the entire understanding so acquired by copying the memory content to another machine. Advantages of Holistic Methods Reductionism in Science versus Holism in Machine Learning N.P. Hard Problems cannot be solved, versus fines-valid solutions by guessing well-based on a lifetime of experience. Geigo, garbage in, garbage out is a recognized problem, versus copes with missing, erroneous, and misleading inputs. Brightness. Experience catastrophic failures at edges of competence, versus anti-fragile. Learns from mistakes, especially almost correct guesses in small, correctable failures. The models of a constantly changing world are obsolete the moment they are created, versus incremental learning provides continuous adaptation to a constantly changing world. Algorithms may be incorrect or may be incorrectly implemented, versus self-repairing systems can tolerate or correct internal errors. It is because we desire certainty, optimality, completeness, etc. L.N.P. Hardness becomes a problem. There are many problems where it is relatively easy to find a provably valid solution, but where finding all solutions can be very expensive. Real-world traveling salesmen merrily travel long reasonable routes. If a reductionist system does not have complete and correct input data, it either cannot get started or produces questionable output. But it is an important requirement of real-world understanding machines that they be able to detect what is salient, important, in their input in order to avoid paying attention to, and learning from, noise. And they have to deal with incomplete, erroneous, and misleading input generated by millions of other intelligent agents with goals at odds with their own. They need to be able to detect omissions, duplications, errors, noise, lies, etc. And the only epistemologically plausible way to do this is to relate the input to similar input they have understood in the past, what they already know. They need to understand what matters but if they can also understand some of the noise. This is advertising, they can exploit that. There are many image and video apps available featuring image understanding based on deep learning. These apps can remove backgrounds, sharpen details like eyelashes, restore damaged photographs, etc. We need to keep in mind that the ability of holistic systems to fill in data and detect noise depends on them having learned from similar data in the past. We note that all the image improvements are confabulations based on prior experience from their learning corpora. But we can also note that image composition using these methods yields totally seamless images, very far from cut and paste of pixels. And quite similarly, we find language confabulation by systems like GPT-3 to flow seamlessly between sentences and topics. They have nothing to say, but they say it well. However, they bring us closer to meaningful language generation and when we achieve that, the public perception of what computers are capable of will totally change. Most of cognition is recognition. Being able to recognize that something has occurred before and knowing what might happen next has enormous survival value for any animal species. A mature human has used their eyes and other senses for decades. This represents an enormous learning corpus and they can understand anything they have prior experience of. The mistakes made by humans, animals and by holistic ML systems are very often of a near-miss variety which provides an opportunity to learn to do a better next time. Contrast is to reductionist software systems created for similar goals. Rule-based systems have long been infamous for their brittleness. As long as the rules and the rules that match the current input and reality perfectly, the results will be useful, repeatable and reliable. But at the edges of their competence, where the matches become more tenuous, the quality rapidly drops. Minor mistakes in the rule sets in the world modeling may lead such systems to return spectacularly incorrect results. Sometimes repeatability is important and sometimes tracking a changing world by continuously learning more about it is important. In ML, continuous incremental learning makes it possible to stay up to date. If we want repeatability, we can emit a condensed, cleaned and frozen competence file from a learner that can be loaded into non-learning, read-only, cloud-based understanding machines that serve the world and provide repeatability between scheduled software and competence releases. Three, in the case of reductionist systems, such as cell phone OS releases, we are used to getting well-tested new versions with minor bug fixes and occasional major features at regular intervals. Such systems learn only in the sense that the people who created them have learned more and put these insights into the new release. Reductionist systems working with complete incorrect input data are expected to provide correct and repeatable results according to the implementation of the algorithm. But both the algorithm and the implementation may have errors. If the algorithm does not adequately model its reality, then we have reduction errors. In the implementation, we may have bugs. Holistic software systems can be designed to a different standard of correctness. Since input data is normally incomplete and noisy, and results are based on emergent effects, we can expect similar enough results even if parts of the system have been damaged, for instance by catastrophic forgetting. Holistic systems can be made capable of self-repair using incremental learning. This has been observed in the deep learning community. Another technique is that when using multiple parallel threads in learning, there may be conflicts that would normally require locking of some values. But if the operations are simple enough, such as just incrementing a value, we can forego thread safety in the locking since the worst outcome is the loss of a single increment in a system that uses emergent results from millions of such values. And the mistake would, in a well-designed system, be self-correcting in the long run. At the cloud level, absolute consistency may not be as hard a requirement as it is for reductionist systems. Much larger mistakes can be expected to be attributable to misunderstandings of the corpus or poor corpus coverage. General strategies, decomposition into smaller problems, versus generalization may lead to an easier problem. Assuming discards everything irrelevant based on how new information matches existing experience, versus a machine discards everything irrelevant based on how new information matches existing experience, modularity, versus composability, gather valid, correct, and complete input data, versus use whatever information is available, and use all of it. Formal, rigorous methods, versus informal ad hoc methods, absolute control, versus creativity, intelligent design, versus evolution. The reductionist battle cries, the whole is equal to the sum of its parts, which gives us a license to split a large complicated problem into smaller problems to solve each of those using some suitable model, and then to combine all the sub-solutions into a model-based solution for the original, larger problem, such as in moonshots, highway systems, international banking, and generally in industrial intelligent design. This works in simple and some complicated domains, but cannot be done in complex domains, where everything potentially affects everything else. Spreading a complex system may cause any emergent effects to disappear, confounding analysis. Examples of complex problem domains are politics, neuroscience, ecology, economy, including stock markets, and cellular biology. Our life sciences operate in a complex problem domain because life itself is complex. Some say biology has physics envy, because in the life sciences, reductionist models are difficult to create and justify. On the other hand, physics is for simple problems. Problems with many complex interdependencies and unknown webs of causality can now be attacked using deep neural networks. These systems discover useful correlations and may often find solutions using mere hints in the input which match their prior experience. Reductionist strategies with correctness requirements outlaw this. It is notable that one of the larger triumphs of holistic methods is protein folding, which is a problem at the very core of the life sciences. So holistic understanding of a complex system can be acquired by observing it over time and learning from its behavior. There is no need to split the problem into pieces. Part of the holistic stance is that we give the machine everything. Holism comes from the Greek word, holos, amicron, lambda, amicron, sigma, in the written text. The whole, that is to say, all the information we have. If we start filtering the input data, by cleaning it up, then the system will effectively learn from a polyana version of the world, which will be confusing once it has to deal with real life inputs in a production environment. If we want our machines to learn to understand the world all by themselves, then we should not start by applying heavy-handed heuristic cleanup operations of our own design on their input data. Sometimes, reductionist strategies are clearly inferior. The natural language understanding is such a domain. Language understanding in a fluent speaker is almost 100% holistic because it is almost entirely based on prior exposure. We are now finding out that it is much easier to build a machine to learn any language on the planet from scratch than it is to build a good old-fashioned artificial intelligence, 20th century reductionist AI-based style machine that understands a single language such as English. The process where a human, by using their understanding, discards everything irrelevant to arrive at what matters is called the epistemic reduction and is discussed in the first five chapters in this book. This is the most important operation in reductionism, but for some reason discussions of reductionism in the past have tended to focus on other aspects. Perhaps this is a new result. ML systems discard with little fanfare anything that was expected and that has been seen before as boring, harmless, or otherwise ignorable. They may also discard things significantly outside of their experience as noise. Things can only be reduced away at the semantic level. They can be recognized that operations capable of epistemic reduction at multiple layers discard anything that's understood at that layer, and they may pass on upward to the next higher semantic layer, a summary of what they discarded plus everything they did not understand at their level. Empire levels do the same. This is why deep learning is deep. Intelligently designed systems are often made up out of interchangeable modules, which allow for easy replacement in case of failure, and in some cases, and especially in software, allow for customization of functionality by replacing or adding modules. These modules have well specified interfaces that allow for such interconnections. In the holistic case we can consider a human cell with thousands of proteins interact on contact or as required with many substances floating around in the cellular fluid. It is not the result of intelligent design, and it shows there are overlaps and redundancies that may contribute to more reliable operation, and there are multiple potentially complex mechanisms keeping each other in check, or we can consider music, or multiple notes in accord in different timbres in a symphony orchestra in a composition will conjure an emerging harmonic whole that sounds different than the sum of its parts, or consider spices in a soup, or opinions in a meeting that leads to a consensus. The word, composability, fits this capability in the holistic case. Unfortunately, in much literature it is merely used as a synonym for its reductionist counterpart, modularity. As discussed in the Geico case, in the section above, holistic ML systems can fill in missing details starting from various scant evidence. Compare for example confabulations of systems like GPT-3 and image enhancement apps. They supply the missing details by jumping to conclusions based on few clues and lots of experience. Since we are not omniscient and don't even know what is happening behind our backs, scant evidence is all we will ever have, but it is amazing how effective scant evidence can be in a familiar context. We can drive a car through fog or find an alarm clock in absolute darkness. The more the system has learned, the less input is needed to arrive at a reasonable identification of the problem and hence retrieve a previously discovered working solution. Formal methods and experimental rigorousness make for good science. On the other hand, holistic methods can follow tenuous threads, hoping for stronger threads or some solution, with little effort spent on backtracking or documentation because once a solution is found, it is the only thing that matters. Tracking has little value in non-repeating situations or when using holistic methods at massive scales, such as in deep learning. Absolute control requires that we know exactly what the problems and solutions are and all we need to do is implement them. Once deployed, systems frozen in this manner, which are exactly implementing the models of their creators, cannot improve by learning since there is no room for variation in the existing process and hence no experimentation and no way to discover further improvements. Only holistic systems can provide creativity and useful novelty. We also observe that, learning itself is a creative act, since it must fit new information into an existing network of prior experience. Just like the term, holism has been abused, so has intelligent design, which is a perfectly reasonable term for reductionist industrial end-to-end practice that consistently provides excellent results. On the holistic side, evolution in nature has created wonderful solutions to all kinds of problems that plants and animals need to handle. But we can put evolution, also known in the general sense as selectionism, to work for us in our holistic machines. They can create new, wonderful designs with a biological flavor to them that sometimes, depending on the problem, cannot perform intelligently designed alternatives. Evolution is the most holistic phenomenon we know. No goal functions, no models, no equations. Evolution is not a scientific theory. Science cannot contain it. It must be discussed in epistemology. Mixed systems, deep neural networks can perform autonomous epistemic reduction to find high-level representations for low-level input, such as pixels in an image or characters in text. Current vision understanding systems can reliably identify thousands of different kinds of objects from many different angles in a variety of lighting conditions and weather. They can classify what they see, but do not necessarily understand much beyond that, such as the expected behaviors of other, intentional Asians like cars, pedestrians, or cats. Therefore, at the moment in 2022, most deployments of machine learning use a mixture of reductionist and holistic methods, equations and formulas devised by humans implemented as computer code, and some inputs from a deep neural network solving a sub-problem that requires it, such as vision understanding. Self-driving cars use DNNs for understanding vision, radar, and lidar images, discovering high-level information like a pedestrian on the side of the road from pixel-based images and this understanding has, until recently, been fed to logic and role-based programs that implement the decision-making. Avoid driving into anything, period, that is used to control the car. The trend here is to move more and more responsibilities into the deep neural network, and over time to remove the hand-coded parts. In essence, the network learns not only to see, but learns to understand traffic. We are delegating more and more of our understanding of how to drive to the vehicle itself. This is desirable. Experimental Epistemology Epistemology is the theory of knowledge. It is concerned with the mind's relation to reality. This includes artificial minds. An introduction to epistemology should benefit anyone working in the IML field. Scientific statements look like F equals MA, Newton's second law, or E equals MC squared, Einstein's famous equation, and can all be proven and or derived from other accepted results or verified experimentally. Algebra is built on lemurs that are not part of algebra. They cannot be proven inside of algebra. Similarly, epistemological statements are not provable in science because science is built on top of epistemology. But when science is not helping, such as in bizarre domains, then setting scientific methodology aside and dropping down to the level of epistemology sometimes works. Epistemology is, just like philosophy in general, an armchair thinking exercise, and the results are judged on internal coherence and consistency with other accepted theory rather than by proofs or experiments. However, the availability of understanding machines, such as DNNs now suddenly provides the opportunity for actual experiments in epistemology. Consider the following statements from the domain of epistemology, and how each of them can be viewed as an implementation hint for AI designers. We are already able to measure their effects on system competence. You can only learn that which you already almost know. Patrick Winston, MIT. Our intelligences are fallible. Monica Anderson. In order to detect that something is new, you need to recognize everything old. Monica Anderson. You cannot reason about that which you do not understand. Monica Anderson. You are known by the company you keep, simple version of the yanni dilemma from category theory and the justification for embeddings in deep learning. All useful novelty in the universe is due to processes of variation and selection. The selectionist manifesto. Selectionism is the generalization of Darwinism. This is right genetic algorithms work. Science has no equations for concepts like understanding, reasoning, learning, abstraction, or modeling since they are all epistemology level concepts. We cannot even start using science until we have decided what model to use. We must use our experience to perform epistemic reductions, discarding the irrelevant, starting from the messy real world problem situation until we are left with a scientific model we can use, such as an equation. The focus in AI research should be on exactly how we can get our machines to perform this pre-scientific epistemic reduction by themselves and the answer to that cannot be found inside of science. Artificial General Intelligence Artificial General Intelligence, AGI, was a theoretical 20th century reductionist AI attempt to go beyond the narrow AIA of domain specific expert systems closer to a general intelligence they thought humans had. The term was mostly used by independent researchers, amateurs and enthusiasts. But the AGI term was not well enough defined and was not backed by sufficient theory to provide any AI implementation guidance and what little progress had been made by these groups was overtaken by holistic methods after 2012. Today we know that the entire premise of 20th century reductionist AGI was wrong. Humans are not general intelligences at birth. Instead, we are general learners capable of learning almost any skill or knowledge required in a wide range of problem domains. If we want human compatible cognitive systems, then we should build them in our image in this respect to build machines that learn and jump to conclusions on scant evidence. Decades ago, AGI implied a human programmed reductionist hand coded program based on logic and reasoning that can solve any problem because the programmers anticipated it. To argue against claims that this was impossible, the AGI community came up with a promise or threat of self-improving AI. But the amount of code in our cognitive systems has shrunk from 6 million propositions in sake around 1990 to 600 lines of code to play video games around 2017 to about 13 lines of cares code in some research reports. And now there's AutoML and other efforts at eliminating all remaining programming from ML. The problems are not in the code. There's almost no code left to improve in modern machine learning systems. All that matters is the corpus. We can now, after 2012, see that machine learning is an absolute requirement for anything worthy of the name AI, which makes recursive self-improvement leading to evil superhuman omniscience logic based godlike artificial general intelligence a 20th century reductionist AI myth. We must focus on artificial general learners. Afterward, science was created to stop people from overrating correlations and jumping to erroneous conclusions on scant evidence and then sharing those conclusions with others, leading to compounded mistakes and much wasted effort. Consequently, promoting a holistic stance has long been a career-ending move in academia, and especially in computer science. But now we suddenly have machine learning that performs cognitive tasks such as protein folding, playing go, and estimating house prices at useful levels using exactly a holistic stance. So now science itself has a cognitive dissonance. This is a conflict about what science is or should be. Inherence of these stances leads people to develop significant personal cognitive dissonances, which is why discussions about these issues are very unpopular among people with solid STEM educations. But the dichotomy is real. We need to deal with it. Our choices so far seem to have been too. Claim that dichotomy doesn't exist. But Schrodinger and Pursig also discuss it. Claim that the holistic stance doesn't work. But deep learning works. Claim that reductionist methods are requirement, hobbling our toolkits for a principle. The reductionist stance also makes it difficult to imagine and accept things like systems capable of autonomous epistemic reduction, systems that do not have a goal function, systems that improve with practice, systems that exploit emergent effects, systems that by themselves make decisions about what matters most, systems that occasionally give a wrong answer but are nevertheless very useful. So after a serious education in machine learning we don't actually need to do almost any programming at all. And we don't need to understand anybody else's problem domains. Because we don't have to perform any epistemic reduction ourselves. We should recognize this for what it is. AI was supposed to solve our problems for us so we would not have to learn or understand any new problem domains. To not have to think. And that's what we have today, in machine learning, and with holistic methods in general. Why are some people surprised or unhappy about this? In my opinion, this is AI, this is what we have been trying to accomplish for decades. People who claim machine understanding is not AI are asking for human level human-centric reasoning and are, at their peril, blind to the nascent ML-based understanding we can achieve today. With expected reasonable improvements in machine understanding capabilities, familiarity and acceptance of the holistic stance will become a requirement for ML and AI-based work. It will likely take years for our educational system to adjust. This has been Anonika's Little Pills, led to you by a computer. Thank you for listening.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 10.92, "text": " The Blue Pill. Self-improving AI. Self-improving AI is a meme that has been circulating since", "tokens": [440, 8510, 44656, 13, 16348, 12, 332, 4318, 798, 7318, 13, 16348, 12, 332, 4318, 798, 7318, 307, 257, 21701, 300, 575, 668, 39749, 1670], "temperature": 0.0, "avg_logprob": -0.24753377376458582, "compression_ratio": 1.4145077720207253, "no_speech_prob": 0.38426831364631653}, {"id": 1, "seek": 0, "start": 10.92, "end": 18.76, "text": " the 1980s. Current proponents of the idea include Wastram and Omihandro. My own summary goes", "tokens": [264, 13626, 82, 13, 15629, 2365, 40496, 295, 264, 1558, 4090, 343, 525, 2356, 293, 9757, 4247, 29173, 13, 1222, 1065, 12691, 1709], "temperature": 0.0, "avg_logprob": -0.24753377376458582, "compression_ratio": 1.4145077720207253, "no_speech_prob": 0.38426831364631653}, {"id": 2, "seek": 0, "start": 18.76, "end": 25.92, "text": " something like this. If we get any kind of AGI going, no matter how slow it is and how", "tokens": [746, 411, 341, 13, 759, 321, 483, 604, 733, 295, 316, 26252, 516, 11, 572, 1871, 577, 2964, 309, 307, 293, 577], "temperature": 0.0, "avg_logprob": -0.24753377376458582, "compression_ratio": 1.4145077720207253, "no_speech_prob": 0.38426831364631653}, {"id": 3, "seek": 2592, "start": 25.92, "end": 31.480000000000004, "text": " buggy it is, we can give it access to its own source code and let it analyze it and", "tokens": [7426, 1480, 309, 307, 11, 321, 393, 976, 309, 2105, 281, 1080, 1065, 4009, 3089, 293, 718, 309, 12477, 309, 293], "temperature": 0.0, "avg_logprob": -0.11585887273152669, "compression_ratio": 1.5681818181818181, "no_speech_prob": 9.927460632752627e-05}, {"id": 4, "seek": 2592, "start": 31.480000000000004, "end": 37.28, "text": " clean up and fix the bugs and then rewrite its code to be as good as it can make it.", "tokens": [2541, 493, 293, 3191, 264, 15120, 293, 550, 28132, 1080, 3089, 281, 312, 382, 665, 382, 309, 393, 652, 309, 13], "temperature": 0.0, "avg_logprob": -0.11585887273152669, "compression_ratio": 1.5681818181818181, "no_speech_prob": 9.927460632752627e-05}, {"id": 5, "seek": 2592, "start": 37.28, "end": 43.760000000000005, "text": " We then start up the slightly smarter AGI and repeat the process until the AGI's get", "tokens": [492, 550, 722, 493, 264, 4748, 20294, 316, 26252, 293, 7149, 264, 1399, 1826, 264, 316, 26252, 311, 483], "temperature": 0.0, "avg_logprob": -0.11585887273152669, "compression_ratio": 1.5681818181818181, "no_speech_prob": 9.927460632752627e-05}, {"id": 6, "seek": 2592, "start": 43.760000000000005, "end": 51.56, "text": " super intelligent. On the surface, this is irrefutable. We already have examples of systems", "tokens": [1687, 13232, 13, 1282, 264, 3753, 11, 341, 307, 16014, 69, 32148, 13, 492, 1217, 362, 5110, 295, 3652], "temperature": 0.0, "avg_logprob": -0.11585887273152669, "compression_ratio": 1.5681818181818181, "no_speech_prob": 9.927460632752627e-05}, {"id": 7, "seek": 5156, "start": 51.56, "end": 58.24, "text": " improving themselves. We can buy a cheap 3D printer and then quite cheaply print out parts", "tokens": [11470, 2969, 13, 492, 393, 2256, 257, 7084, 805, 35, 16671, 293, 550, 1596, 7084, 356, 4482, 484, 3166], "temperature": 0.0, "avg_logprob": -0.11172666220829405, "compression_ratio": 1.635135135135135, "no_speech_prob": 2.9435592296067625e-05}, {"id": 8, "seek": 5156, "start": 58.24, "end": 65.0, "text": " for a much better 3D printer. Or to make computer chips that go into computers that design better", "tokens": [337, 257, 709, 1101, 805, 35, 16671, 13, 1610, 281, 652, 3820, 11583, 300, 352, 666, 10807, 300, 1715, 1101], "temperature": 0.0, "avg_logprob": -0.11172666220829405, "compression_ratio": 1.635135135135135, "no_speech_prob": 2.9435592296067625e-05}, {"id": 9, "seek": 5156, "start": 65.0, "end": 72.16, "text": " computer chips. Not to mention evolution of all species in nature. I look at it from an", "tokens": [3820, 11583, 13, 1726, 281, 2152, 9303, 295, 439, 6172, 294, 3687, 13, 286, 574, 412, 309, 490, 364], "temperature": 0.0, "avg_logprob": -0.11172666220829405, "compression_ratio": 1.635135135135135, "no_speech_prob": 2.9435592296067625e-05}, {"id": 10, "seek": 5156, "start": 72.16, "end": 77.88, "text": " epistemologist point of view and say, that's a hard line reductionist idea that should", "tokens": [2388, 43958, 9201, 935, 295, 1910, 293, 584, 11, 300, 311, 257, 1152, 1622, 11004, 468, 1558, 300, 820], "temperature": 0.0, "avg_logprob": -0.11172666220829405, "compression_ratio": 1.635135135135135, "no_speech_prob": 2.9435592296067625e-05}, {"id": 11, "seek": 7788, "start": 77.88, "end": 85.64, "text": " not have made it out of the 20th century. The idea, as its inception, imagined an AGI", "tokens": [406, 362, 1027, 309, 484, 295, 264, 945, 392, 4901, 13, 440, 1558, 11, 382, 1080, 49834, 11, 16590, 364, 316, 26252], "temperature": 0.0, "avg_logprob": -0.10386073589324951, "compression_ratio": 1.5482456140350878, "no_speech_prob": 6.209770799614489e-05}, {"id": 12, "seek": 7788, "start": 85.64, "end": 91.47999999999999, "text": " as something that was written by teams of human programmers using software development", "tokens": [382, 746, 300, 390, 3720, 538, 5491, 295, 1952, 41504, 1228, 4722, 3250], "temperature": 0.0, "avg_logprob": -0.10386073589324951, "compression_ratio": 1.5482456140350878, "no_speech_prob": 6.209770799614489e-05}, {"id": 13, "seek": 7788, "start": 91.47999999999999, "end": 97.8, "text": " tools and mathematical equations. What I think the only thing that even approximates this", "tokens": [3873, 293, 18894, 11787, 13, 708, 286, 519, 264, 787, 551, 300, 754, 8542, 1024, 341], "temperature": 0.0, "avg_logprob": -0.10386073589324951, "compression_ratio": 1.5482456140350878, "no_speech_prob": 6.209770799614489e-05}, {"id": 14, "seek": 7788, "start": 97.8, "end": 103.75999999999999, "text": " outcome is that the code is perfect, and humans as well as machines all agree there are no", "tokens": [9700, 307, 300, 264, 3089, 307, 2176, 11, 293, 6255, 382, 731, 382, 8379, 439, 3986, 456, 366, 572], "temperature": 0.0, "avg_logprob": -0.10386073589324951, "compression_ratio": 1.5482456140350878, "no_speech_prob": 6.209770799614489e-05}, {"id": 15, "seek": 10376, "start": 103.76, "end": 111.12, "text": " more improvements to be made. And the resulting AGI's are still not super intelligent. The", "tokens": [544, 13797, 281, 312, 1027, 13, 400, 264, 16505, 316, 26252, 311, 366, 920, 406, 1687, 13232, 13, 440], "temperature": 0.0, "avg_logprob": -0.13540736111727628, "compression_ratio": 1.440217391304348, "no_speech_prob": 9.1084890300408e-05}, {"id": 16, "seek": 10376, "start": 111.12, "end": 116.48, "text": " most likely outcome is that we all realize the folly in this argument and won't even", "tokens": [881, 3700, 9700, 307, 300, 321, 439, 4325, 264, 726, 13020, 294, 341, 6770, 293, 1582, 380, 754], "temperature": 0.0, "avg_logprob": -0.13540736111727628, "compression_ratio": 1.440217391304348, "no_speech_prob": 9.1084890300408e-05}, {"id": 17, "seek": 10376, "start": 116.48, "end": 123.80000000000001, "text": " try. It's not about the code. The number of lines of code in AI related projects has been", "tokens": [853, 13, 467, 311, 406, 466, 264, 3089, 13, 440, 1230, 295, 3876, 295, 3089, 294, 7318, 4077, 4455, 575, 668], "temperature": 0.0, "avg_logprob": -0.13540736111727628, "compression_ratio": 1.440217391304348, "no_speech_prob": 9.1084890300408e-05}, {"id": 18, "seek": 12380, "start": 123.8, "end": 146.0, "text": " declining rapidly. 2012. 34,000 lines.py.kudukrzebski et al. for ImageNet. 2013. 1571 lines of", "tokens": [34298, 12910, 13, 9125, 13, 12790, 11, 1360, 3876, 13, 8200, 13, 74, 532, 2034, 81, 1381, 929, 2984, 1030, 419, 13, 337, 29903, 31890, 13, 9012, 13, 2119, 29985, 3876, 295], "temperature": 0.0, "avg_logprob": -0.39835747083028156, "compression_ratio": 1.032967032967033, "no_speech_prob": 0.0006439185817725956}, {"id": 19, "seek": 14600, "start": 146.0, "end": 157.4, "text": " Lua to Play Atari games. 2017. 196 lines of Keras to Implement Deep Dream. 2018. Less", "tokens": [441, 4398, 281, 5506, 41381, 2813, 13, 6591, 13, 7998, 3876, 295, 591, 6985, 281, 4331, 43704, 14895, 12105, 13, 6096, 13, 18649], "temperature": 0.0, "avg_logprob": -0.23218687375386557, "compression_ratio": 1.4081632653061225, "no_speech_prob": 7.555141928605735e-05}, {"id": 20, "seek": 14600, "start": 157.4, "end": 165.52, "text": " than 100 lines of Keras for research paper-level results. And all of these, except Saig, included", "tokens": [813, 2319, 3876, 295, 591, 6985, 337, 2132, 3035, 12, 12418, 3542, 13, 400, 439, 295, 613, 11, 3993, 6299, 328, 11, 5556], "temperature": 0.0, "avg_logprob": -0.23218687375386557, "compression_ratio": 1.4081632653061225, "no_speech_prob": 7.555141928605735e-05}, {"id": 21, "seek": 14600, "start": 165.52, "end": 172.0, "text": " as the most famous example of a 20th century reductionist AI system, demonstrates new levels", "tokens": [382, 264, 881, 4618, 1365, 295, 257, 945, 392, 4901, 11004, 468, 7318, 1185, 11, 31034, 777, 4358], "temperature": 0.0, "avg_logprob": -0.23218687375386557, "compression_ratio": 1.4081632653061225, "no_speech_prob": 7.555141928605735e-05}, {"id": 22, "seek": 17200, "start": 172.0, "end": 178.68, "text": " of power of machine learning. The limits to intelligence are not in the code. In fact,", "tokens": [295, 1347, 295, 3479, 2539, 13, 440, 10406, 281, 7599, 366, 406, 294, 264, 3089, 13, 682, 1186, 11], "temperature": 0.0, "avg_logprob": -0.1515669650342091, "compression_ratio": 1.6829268292682926, "no_speech_prob": 0.00011842451203847304}, {"id": 23, "seek": 17200, "start": 178.68, "end": 185.32, "text": " they are not even technological. The limit of intelligence is the complexity of the", "tokens": [436, 366, 406, 754, 18439, 13, 440, 4948, 295, 7599, 307, 264, 14024, 295, 264], "temperature": 0.0, "avg_logprob": -0.1515669650342091, "compression_ratio": 1.6829268292682926, "no_speech_prob": 0.00011842451203847304}, {"id": 24, "seek": 17200, "start": 185.32, "end": 192.08, "text": " world. Admission is unavailable. The main purpose of intelligence is to guess, to jump", "tokens": [1002, 13, 1999, 29797, 307, 36541, 32699, 13, 440, 2135, 4334, 295, 7599, 307, 281, 2041, 11, 281, 3012], "temperature": 0.0, "avg_logprob": -0.1515669650342091, "compression_ratio": 1.6829268292682926, "no_speech_prob": 0.00011842451203847304}, {"id": 25, "seek": 17200, "start": 192.08, "end": 198.44, "text": " to conclusions on scant evidence, and to do it well, based on a large set of historical", "tokens": [281, 22865, 322, 795, 394, 4467, 11, 293, 281, 360, 309, 731, 11, 2361, 322, 257, 2416, 992, 295, 8584], "temperature": 0.0, "avg_logprob": -0.1515669650342091, "compression_ratio": 1.6829268292682926, "no_speech_prob": 0.00011842451203847304}, {"id": 26, "seek": 19844, "start": 198.44, "end": 204.84, "text": " patterns of problems and their solutions or events and their consequences. Because scant", "tokens": [8294, 295, 2740, 293, 641, 6547, 420, 3931, 293, 641, 10098, 13, 1436, 795, 394], "temperature": 0.0, "avg_logprob": -0.10139722090501052, "compression_ratio": 1.614678899082569, "no_speech_prob": 9.859772399067879e-05}, {"id": 27, "seek": 19844, "start": 204.84, "end": 211.6, "text": " evidence is all we will ever have, we don't even know what goes on behind our back. And", "tokens": [4467, 307, 439, 321, 486, 1562, 362, 11, 321, 500, 380, 754, 458, 437, 1709, 322, 2261, 527, 646, 13, 400], "temperature": 0.0, "avg_logprob": -0.10139722090501052, "compression_ratio": 1.614678899082569, "no_speech_prob": 9.859772399067879e-05}, {"id": 28, "seek": 19844, "start": 211.6, "end": 217.24, "text": " because our intelligence is guessing, I have repeatedly claimed that, all intelligences", "tokens": [570, 527, 7599, 307, 17939, 11, 286, 362, 18227, 12941, 300, 11, 439, 5613, 2667], "temperature": 0.0, "avg_logprob": -0.10139722090501052, "compression_ratio": 1.614678899082569, "no_speech_prob": 9.859772399067879e-05}, {"id": 29, "seek": 19844, "start": 217.24, "end": 223.48, "text": " are fallible. We are already making machines that are better than humans in some aspect", "tokens": [366, 2100, 964, 13, 492, 366, 1217, 1455, 8379, 300, 366, 1101, 813, 6255, 294, 512, 4171], "temperature": 0.0, "avg_logprob": -0.10139722090501052, "compression_ratio": 1.614678899082569, "no_speech_prob": 9.859772399067879e-05}, {"id": 30, "seek": 22348, "start": 223.48, "end": 229.95999999999998, "text": " of guessing. Protein folding and playing go are examples of this. And these machines", "tokens": [295, 17939, 13, 43371, 259, 25335, 293, 2433, 352, 366, 5110, 295, 341, 13, 400, 613, 8379], "temperature": 0.0, "avg_logprob": -0.11435784233940972, "compression_ratio": 1.5172413793103448, "no_speech_prob": 8.868105214787647e-05}, {"id": 31, "seek": 22348, "start": 229.95999999999998, "end": 235.12, "text": " will get bigger and better at what they do and will be superhuman in various ways and", "tokens": [486, 483, 3801, 293, 1101, 412, 437, 436, 360, 293, 486, 312, 1687, 18796, 294, 3683, 2098, 293], "temperature": 0.0, "avg_logprob": -0.11435784233940972, "compression_ratio": 1.5172413793103448, "no_speech_prob": 8.868105214787647e-05}, {"id": 32, "seek": 22348, "start": 235.12, "end": 242.51999999999998, "text": " in many problem domains, simply based on larger capacity to hold, look up, or search useful", "tokens": [294, 867, 1154, 25514, 11, 2935, 2361, 322, 4833, 6042, 281, 1797, 11, 574, 493, 11, 420, 3164, 4420], "temperature": 0.0, "avg_logprob": -0.11435784233940972, "compression_ratio": 1.5172413793103448, "no_speech_prob": 8.868105214787647e-05}, {"id": 33, "seek": 22348, "start": 242.51999999999998, "end": 249.07999999999998, "text": " patterns. The code doing that can be hand optimized to the point where any AI improvement", "tokens": [8294, 13, 440, 3089, 884, 300, 393, 312, 1011, 26941, 281, 264, 935, 689, 604, 7318, 10444], "temperature": 0.0, "avg_logprob": -0.11435784233940972, "compression_ratio": 1.5172413793103448, "no_speech_prob": 8.868105214787647e-05}, {"id": 34, "seek": 24908, "start": 249.08, "end": 255.28, "text": " would be insignificant. My own code in the inner loop for understanding any language", "tokens": [576, 312, 43685, 13, 1222, 1065, 3089, 294, 264, 7284, 6367, 337, 3701, 604, 2856], "temperature": 0.0, "avg_logprob": -0.14747320372482825, "compression_ratio": 1.4871794871794872, "no_speech_prob": 9.052152745425701e-05}, {"id": 35, "seek": 24908, "start": 255.28, "end": 262.2, "text": " on the planet, once it has learned it, in inference mode is about 90 lines of Java.", "tokens": [322, 264, 5054, 11, 1564, 309, 575, 3264, 309, 11, 294, 38253, 4391, 307, 466, 4289, 3876, 295, 10745, 13], "temperature": 0.0, "avg_logprob": -0.14747320372482825, "compression_ratio": 1.4871794871794872, "no_speech_prob": 9.052152745425701e-05}, {"id": 36, "seek": 24908, "start": 262.2, "end": 269.04, "text": " We can expect a best minor improvements to efficiency and speed. It comes down to the", "tokens": [492, 393, 2066, 257, 1151, 6696, 13797, 281, 10493, 293, 3073, 13, 467, 1487, 760, 281, 264], "temperature": 0.0, "avg_logprob": -0.14747320372482825, "compression_ratio": 1.4871794871794872, "no_speech_prob": 9.052152745425701e-05}, {"id": 37, "seek": 24908, "start": 269.04, "end": 278.24, "text": " corpus. In my domain, NLU, simple tests can be scored at 100% after a few minutes of learning", "tokens": [1181, 31624, 13, 682, 452, 9274, 11, 426, 43, 52, 11, 2199, 6921, 393, 312, 18139, 412, 2319, 4, 934, 257, 1326, 2077, 295, 2539], "temperature": 0.0, "avg_logprob": -0.14747320372482825, "compression_ratio": 1.4871794871794872, "no_speech_prob": 9.052152745425701e-05}, {"id": 38, "seek": 27824, "start": 278.24, "end": 284.84000000000003, "text": " on a laptop. Continue learning for days and weeks would provide a larger sample set of", "tokens": [322, 257, 10732, 13, 24472, 2539, 337, 1708, 293, 3259, 576, 2893, 257, 4833, 6889, 992, 295], "temperature": 0.0, "avg_logprob": -0.14462832400673314, "compression_ratio": 1.5258620689655173, "no_speech_prob": 5.6234079238492996e-05}, {"id": 39, "seek": 27824, "start": 284.84000000000003, "end": 290.16, "text": " vocabulary in appropriate contexts, which would mainly correct misunderstandings in", "tokens": [19864, 294, 6854, 30628, 11, 597, 576, 8704, 3006, 35736, 1109, 294], "temperature": 0.0, "avg_logprob": -0.14462832400673314, "compression_ratio": 1.5258620689655173, "no_speech_prob": 5.6234079238492996e-05}, {"id": 40, "seek": 27824, "start": 290.16, "end": 298.24, "text": " corner cases. But these corporal are not comparable by several orders of magnitude, to the gathered", "tokens": [4538, 3331, 13, 583, 613, 6804, 304, 366, 406, 25323, 538, 2940, 9470, 295, 15668, 11, 281, 264, 13032], "temperature": 0.0, "avg_logprob": -0.14462832400673314, "compression_ratio": 1.5258620689655173, "no_speech_prob": 5.6234079238492996e-05}, {"id": 41, "seek": 27824, "start": 298.24, "end": 305.28000000000003, "text": " life experience of a human at age 25. The main limit of intelligence is corpus size", "tokens": [993, 1752, 295, 257, 1952, 412, 3205, 3552, 13, 440, 2135, 4948, 295, 7599, 307, 1181, 31624, 2744], "temperature": 0.0, "avg_logprob": -0.14462832400673314, "compression_ratio": 1.5258620689655173, "no_speech_prob": 5.6234079238492996e-05}, {"id": 42, "seek": 30528, "start": 305.28, "end": 312.44, "text": " in ML situation. Future artificial intelligences will be nothing like what AGI fans have been", "tokens": [294, 21601, 2590, 13, 20805, 11677, 5613, 2667, 486, 312, 1825, 411, 437, 316, 26252, 4499, 362, 668], "temperature": 0.0, "avg_logprob": -0.11561633291698638, "compression_ratio": 1.5165289256198347, "no_speech_prob": 6.708447472192347e-05}, {"id": 43, "seek": 30528, "start": 312.44, "end": 319.52, "text": " fearmongering about. These are 20th century reductionist AI ideas. The components are", "tokens": [4240, 76, 556, 1794, 466, 13, 1981, 366, 945, 392, 4901, 11004, 468, 7318, 3487, 13, 440, 6677, 366], "temperature": 0.0, "avg_logprob": -0.11561633291698638, "compression_ratio": 1.5165289256198347, "no_speech_prob": 6.708447472192347e-05}, {"id": 44, "seek": 30528, "start": 319.52, "end": 326.79999999999995, "text": " blind to the most fundamental basics of epistemology. Reductionist good old fashioned AI has been", "tokens": [6865, 281, 264, 881, 8088, 14688, 295, 2388, 43958, 1793, 13, 4477, 27549, 468, 665, 1331, 40646, 7318, 575, 668], "temperature": 0.0, "avg_logprob": -0.11561633291698638, "compression_ratio": 1.5165289256198347, "no_speech_prob": 6.708447472192347e-05}, {"id": 45, "seek": 30528, "start": 326.79999999999995, "end": 332.47999999999996, "text": " demonstrated to being inferior in their own domains to even semi-trivial machine learning", "tokens": [18772, 281, 885, 24249, 294, 641, 1065, 25514, 281, 754, 12909, 12, 83, 470, 22640, 3479, 2539], "temperature": 0.0, "avg_logprob": -0.11561633291698638, "compression_ratio": 1.5165289256198347, "no_speech_prob": 6.708447472192347e-05}, {"id": 46, "seek": 33248, "start": 332.48, "end": 342.20000000000005, "text": " methods. We need AGL, not AGI. Machines learning to code. As of this writing, there are a handful", "tokens": [7150, 13, 492, 643, 316, 19440, 11, 406, 316, 26252, 13, 12089, 1652, 2539, 281, 3089, 13, 1018, 295, 341, 3579, 11, 456, 366, 257, 16458], "temperature": 0.0, "avg_logprob": -0.17127874318291159, "compression_ratio": 1.4093264248704662, "no_speech_prob": 9.071256499737501e-05}, {"id": 47, "seek": 33248, "start": 342.20000000000005, "end": 347.48, "text": " of available code writing systems based on ML technology that has learned from large", "tokens": [295, 2435, 3089, 3579, 3652, 2361, 322, 21601, 2899, 300, 575, 3264, 490, 2416], "temperature": 0.0, "avg_logprob": -0.17127874318291159, "compression_ratio": 1.4093264248704662, "no_speech_prob": 9.071256499737501e-05}, {"id": 48, "seek": 33248, "start": 347.48, "end": 355.36, "text": " quantities of open source code. For example GitHub Copilot, OpenAI Codex, and Amazon Code", "tokens": [22927, 295, 1269, 4009, 3089, 13, 1171, 1365, 23331, 11579, 31516, 11, 7238, 48698, 15549, 87, 11, 293, 6795, 15549], "temperature": 0.0, "avg_logprob": -0.17127874318291159, "compression_ratio": 1.4093264248704662, "no_speech_prob": 9.071256499737501e-05}, {"id": 49, "seek": 35536, "start": 355.36, "end": 362.8, "text": " Whisperer. They have not yet surpassed human programmers. But it's not about writing code", "tokens": [41132, 610, 260, 13, 814, 362, 406, 1939, 27650, 292, 1952, 41504, 13, 583, 309, 311, 406, 466, 3579, 3089], "temperature": 0.0, "avg_logprob": -0.15568020177441974, "compression_ratio": 1.6318181818181818, "no_speech_prob": 8.92779280547984e-05}, {"id": 50, "seek": 35536, "start": 362.8, "end": 369.04, "text": " either. AI's writing code is about as silly as AI magazine covers with pictures of robots", "tokens": [2139, 13, 7318, 311, 3579, 3089, 307, 466, 382, 11774, 382, 7318, 11332, 10538, 365, 5242, 295, 14733], "temperature": 0.0, "avg_logprob": -0.15568020177441974, "compression_ratio": 1.6318181818181818, "no_speech_prob": 8.92779280547984e-05}, {"id": 51, "seek": 35536, "start": 369.04, "end": 376.84000000000003, "text": " typing, wink wink. In the future, if we want the computer to do something, we will have", "tokens": [18444, 11, 44212, 44212, 13, 682, 264, 2027, 11, 498, 321, 528, 264, 3820, 281, 360, 746, 11, 321, 486, 362], "temperature": 0.0, "avg_logprob": -0.15568020177441974, "compression_ratio": 1.6318181818181818, "no_speech_prob": 8.92779280547984e-05}, {"id": 52, "seek": 35536, "start": 376.84000000000003, "end": 383.8, "text": " a conversation, speaking and listening, with the computer. The conversation is at the level", "tokens": [257, 3761, 11, 4124, 293, 4764, 11, 365, 264, 3820, 13, 440, 3761, 307, 412, 264, 1496], "temperature": 0.0, "avg_logprob": -0.15568020177441974, "compression_ratio": 1.6318181818181818, "no_speech_prob": 8.92779280547984e-05}, {"id": 53, "seek": 38380, "start": 383.8, "end": 390.56, "text": " of discussing a problem with a competent coworker or professional. It may spontaneously ask", "tokens": [295, 10850, 257, 1154, 365, 257, 29998, 31998, 260, 420, 4843, 13, 467, 815, 47632, 1029], "temperature": 0.0, "avg_logprob": -0.18462285762879907, "compression_ratio": 1.5269709543568464, "no_speech_prob": 5.75956073589623e-05}, {"id": 54, "seek": 38380, "start": 390.56, "end": 399.08000000000004, "text": " clarifying questions. I call this, contiguously rolling topic, mixed initiative dialogue, others", "tokens": [6093, 5489, 1651, 13, 286, 818, 341, 11, 660, 16397, 5098, 9439, 4829, 11, 7467, 11552, 10221, 11, 2357], "temperature": 0.0, "avg_logprob": -0.18462285762879907, "compression_ratio": 1.5269709543568464, "no_speech_prob": 5.75956073589623e-05}, {"id": 55, "seek": 38380, "start": 399.08000000000004, "end": 406.08000000000004, "text": " talk of these bots as dialogue Asians. But this will go beyond Siri or Alexa, and when", "tokens": [751, 295, 613, 35410, 382, 10221, 47724, 13, 583, 341, 486, 352, 4399, 33682, 420, 22595, 11, 293, 562], "temperature": 0.0, "avg_logprob": -0.18462285762879907, "compression_ratio": 1.5269709543568464, "no_speech_prob": 5.75956073589623e-05}, {"id": 56, "seek": 38380, "start": 406.08000000000004, "end": 413.08000000000004, "text": " the computer understands exactly what you want done. It just does it. Why would reductionist", "tokens": [264, 3820, 15146, 2293, 437, 291, 528, 1096, 13, 467, 445, 775, 309, 13, 1545, 576, 11004, 468], "temperature": 0.0, "avg_logprob": -0.18462285762879907, "compression_ratio": 1.5269709543568464, "no_speech_prob": 5.75956073589623e-05}, {"id": 57, "seek": 41308, "start": 413.08, "end": 420.0, "text": " style programming be a necessary step? Yes, there will still be lots of places where we", "tokens": [3758, 9410, 312, 257, 4818, 1823, 30, 1079, 11, 456, 486, 920, 312, 3195, 295, 3190, 689, 321], "temperature": 0.0, "avg_logprob": -0.16391956943205033, "compression_ratio": 1.5381165919282511, "no_speech_prob": 9.398607653565705e-05}, {"id": 58, "seek": 41308, "start": 420.0, "end": 425.96, "text": " want to use code. But whether that code is written by humans or AI's will make much", "tokens": [528, 281, 764, 3089, 13, 583, 1968, 300, 3089, 307, 3720, 538, 6255, 420, 7318, 311, 486, 652, 709], "temperature": 0.0, "avg_logprob": -0.16391956943205033, "compression_ratio": 1.5381165919282511, "no_speech_prob": 9.398607653565705e-05}, {"id": 59, "seek": 41308, "start": 425.96, "end": 432.12, "text": " less of a difference than we might expect based on today's use of computers.", "tokens": [1570, 295, 257, 2649, 813, 321, 1062, 2066, 2361, 322, 965, 311, 764, 295, 10807, 13], "temperature": 0.0, "avg_logprob": -0.16391956943205033, "compression_ratio": 1.5381165919282511, "no_speech_prob": 9.398607653565705e-05}, {"id": 60, "seek": 41308, "start": 432.12, "end": 441.15999999999997, "text": " The Pink Pill. The Wisdom Salon. Wisdom Salon is an online world cafe. The World Cafe protocol", "tokens": [440, 17118, 44656, 13, 440, 34143, 4121, 5996, 266, 13, 34143, 4121, 5996, 266, 307, 364, 2950, 1002, 17773, 13, 440, 3937, 35864, 10336], "temperature": 0.0, "avg_logprob": -0.16391956943205033, "compression_ratio": 1.5381165919282511, "no_speech_prob": 9.398607653565705e-05}, {"id": 61, "seek": 44116, "start": 441.16, "end": 448.72, "text": " is a recipe for organizing conversations that matter on a large scale. Thousands of people", "tokens": [307, 257, 6782, 337, 17608, 7315, 300, 1871, 322, 257, 2416, 4373, 13, 40535, 295, 561], "temperature": 0.0, "avg_logprob": -0.13901101089105372, "compression_ratio": 1.5638766519823788, "no_speech_prob": 8.2100996223744e-05}, {"id": 62, "seek": 44116, "start": 448.72, "end": 456.04, "text": " can cooperate in order to bring clarity to complex issues. This is a post-mortem summary", "tokens": [393, 26667, 294, 1668, 281, 1565, 16992, 281, 3997, 2663, 13, 639, 307, 257, 2183, 12, 76, 477, 443, 12691], "temperature": 0.0, "avg_logprob": -0.13901101089105372, "compression_ratio": 1.5638766519823788, "no_speech_prob": 8.2100996223744e-05}, {"id": 63, "seek": 44116, "start": 456.04, "end": 463.28000000000003, "text": " for my interrupted wisdom salon project. I have all the code in an archive, but it requires", "tokens": [337, 452, 30329, 10712, 27768, 1716, 13, 286, 362, 439, 264, 3089, 294, 364, 23507, 11, 457, 309, 7029], "temperature": 0.0, "avg_logprob": -0.13901101089105372, "compression_ratio": 1.5638766519823788, "no_speech_prob": 8.2100996223744e-05}, {"id": 64, "seek": 44116, "start": 463.28000000000003, "end": 469.68, "text": " a complete rewrite in order to fix the two biggest problems. The switch from flash,", "tokens": [257, 3566, 28132, 294, 1668, 281, 3191, 264, 732, 3880, 2740, 13, 440, 3679, 490, 7319, 11], "temperature": 0.0, "avg_logprob": -0.13901101089105372, "compression_ratio": 1.5638766519823788, "no_speech_prob": 8.2100996223744e-05}, {"id": 65, "seek": 46968, "start": 469.68, "end": 477.40000000000003, "text": " hack, to HTML5 for video and the cost of video connections. I know how to fix these but I'm", "tokens": [10339, 11, 281, 17995, 20, 337, 960, 293, 264, 2063, 295, 960, 9271, 13, 286, 458, 577, 281, 3191, 613, 457, 286, 478], "temperature": 0.0, "avg_logprob": -0.13567023330859923, "compression_ratio": 1.5517241379310345, "no_speech_prob": 0.0001733387034619227}, {"id": 66, "seek": 46968, "start": 477.40000000000003, "end": 484.04, "text": " busy working on understanding machines. At the moment, I am looking for someone to take", "tokens": [5856, 1364, 322, 3701, 8379, 13, 1711, 264, 1623, 11, 286, 669, 1237, 337, 1580, 281, 747], "temperature": 0.0, "avg_logprob": -0.13567023330859923, "compression_ratio": 1.5517241379310345, "no_speech_prob": 0.0001733387034619227}, {"id": 67, "seek": 46968, "start": 484.04, "end": 490.96000000000004, "text": " this over. I also observe that there is a need for something like this. I see things discussed", "tokens": [341, 670, 13, 286, 611, 11441, 300, 456, 307, 257, 643, 337, 746, 411, 341, 13, 286, 536, 721, 7152], "temperature": 0.0, "avg_logprob": -0.13567023330859923, "compression_ratio": 1.5517241379310345, "no_speech_prob": 0.0001733387034619227}, {"id": 68, "seek": 46968, "start": 490.96000000000004, "end": 497.36, "text": " on Quora that would make good topics for a wisdom salon. I happen to believe video in", "tokens": [322, 2326, 3252, 300, 576, 652, 665, 8378, 337, 257, 10712, 27768, 13, 286, 1051, 281, 1697, 960, 294], "temperature": 0.0, "avg_logprob": -0.13567023330859923, "compression_ratio": 1.5517241379310345, "no_speech_prob": 0.0001733387034619227}, {"id": 69, "seek": 49736, "start": 497.36, "end": 501.72, "text": " spoken words are an important component for many reasons.", "tokens": [10759, 2283, 366, 364, 1021, 6542, 337, 867, 4112, 13], "temperature": 0.0, "avg_logprob": -0.16970643130215732, "compression_ratio": 1.5951219512195123, "no_speech_prob": 0.00012679578503593802}, {"id": 70, "seek": 49736, "start": 501.72, "end": 510.72, "text": " Wisdom. Knowledge and information can easily be found on the web. But what about wisdom?", "tokens": [34143, 4121, 13, 32906, 293, 1589, 393, 3612, 312, 1352, 322, 264, 3670, 13, 583, 437, 466, 10712, 30], "temperature": 0.0, "avg_logprob": -0.16970643130215732, "compression_ratio": 1.5951219512195123, "no_speech_prob": 0.00012679578503593802}, {"id": 71, "seek": 49736, "start": 510.72, "end": 516.44, "text": " Intelligence is based on gathered knowledge. Wisdom is based on gathered experience. To", "tokens": [27274, 307, 2361, 322, 13032, 3601, 13, 34143, 4121, 307, 2361, 322, 13032, 1752, 13, 1407], "temperature": 0.0, "avg_logprob": -0.16970643130215732, "compression_ratio": 1.5951219512195123, "no_speech_prob": 0.00012679578503593802}, {"id": 72, "seek": 49736, "start": 516.44, "end": 526.28, "text": " get wiser, seek out more experiences. Engage yourself. Do more stuff. Travel. Talk to people", "tokens": [483, 261, 6694, 11, 8075, 484, 544, 5235, 13, 2469, 609, 1803, 13, 1144, 544, 1507, 13, 20610, 13, 8780, 281, 561], "temperature": 0.0, "avg_logprob": -0.16970643130215732, "compression_ratio": 1.5951219512195123, "no_speech_prob": 0.00012679578503593802}, {"id": 73, "seek": 52628, "start": 526.28, "end": 533.4399999999999, "text": " to share their experiences. Conversation with others is the easiest way to gain wisdom.", "tokens": [281, 2073, 641, 5235, 13, 33247, 399, 365, 2357, 307, 264, 12889, 636, 281, 6052, 10712, 13], "temperature": 0.0, "avg_logprob": -0.15454073076124314, "compression_ratio": 1.708133971291866, "no_speech_prob": 7.714834646321833e-05}, {"id": 74, "seek": 52628, "start": 533.4399999999999, "end": 540.4, "text": " But not all conversations are equal. We want conversations that matter. The World Caf\u00e9", "tokens": [583, 406, 439, 7315, 366, 2681, 13, 492, 528, 7315, 300, 1871, 13, 440, 3937, 46701, 526], "temperature": 0.0, "avg_logprob": -0.15454073076124314, "compression_ratio": 1.708133971291866, "no_speech_prob": 7.714834646321833e-05}, {"id": 75, "seek": 52628, "start": 540.4, "end": 547.36, "text": " Protocol. The World Caf\u00e9 Protocol is a recipe for organizing such conversations that matter", "tokens": [48753, 13, 440, 3937, 46701, 526, 48753, 307, 257, 6782, 337, 17608, 1270, 7315, 300, 1871], "temperature": 0.0, "avg_logprob": -0.15454073076124314, "compression_ratio": 1.708133971291866, "no_speech_prob": 7.714834646321833e-05}, {"id": 76, "seek": 52628, "start": 547.36, "end": 554.16, "text": " on a large scale. Thousands of people can cooperate in order to bring clarity to complex", "tokens": [322, 257, 2416, 4373, 13, 40535, 295, 561, 393, 26667, 294, 1668, 281, 1565, 16992, 281, 3997], "temperature": 0.0, "avg_logprob": -0.15454073076124314, "compression_ratio": 1.708133971291866, "no_speech_prob": 7.714834646321833e-05}, {"id": 77, "seek": 55416, "start": 554.16, "end": 561.4399999999999, "text": " issues. To find out more, buy the book or study the World Caf\u00e9 website. But this is", "tokens": [2663, 13, 1407, 915, 484, 544, 11, 2256, 264, 1446, 420, 2979, 264, 3937, 46701, 526, 3144, 13, 583, 341, 307], "temperature": 0.0, "avg_logprob": -0.08995915518866646, "compression_ratio": 1.4448979591836735, "no_speech_prob": 8.888285083230585e-05}, {"id": 78, "seek": 55416, "start": 561.4399999999999, "end": 568.3199999999999, "text": " how it typically works. In some conference facilities or gymnasium, the organizers provide", "tokens": [577, 309, 5850, 1985, 13, 682, 512, 7586, 9406, 420, 9222, 18979, 2197, 11, 264, 35071, 2893], "temperature": 0.0, "avg_logprob": -0.08995915518866646, "compression_ratio": 1.4448979591836735, "no_speech_prob": 8.888285083230585e-05}, {"id": 79, "seek": 55416, "start": 568.3199999999999, "end": 575.8, "text": " dozens to hundreds of square tables. Each has four chairs, a box of crayons, and a piece", "tokens": [18431, 281, 6779, 295, 3732, 8020, 13, 6947, 575, 1451, 18299, 11, 257, 2424, 295, 33073, 892, 11, 293, 257, 2522], "temperature": 0.0, "avg_logprob": -0.08995915518866646, "compression_ratio": 1.4448979591836735, "no_speech_prob": 8.888285083230585e-05}, {"id": 80, "seek": 55416, "start": 575.8, "end": 582.6, "text": " of butcher paper as a tablecloth. Stakeholders from all walks of life get invited and sit", "tokens": [295, 41579, 3035, 382, 257, 3199, 3474, 900, 13, 745, 619, 12916, 490, 439, 12896, 295, 993, 483, 9185, 293, 1394], "temperature": 0.0, "avg_logprob": -0.08995915518866646, "compression_ratio": 1.4448979591836735, "no_speech_prob": 8.888285083230585e-05}, {"id": 81, "seek": 58260, "start": 582.6, "end": 590.24, "text": " down at the tables. This could be a mixture of farmers, teachers, politicians, in corporate", "tokens": [760, 412, 264, 8020, 13, 639, 727, 312, 257, 9925, 295, 11339, 11, 6023, 11, 14756, 11, 294, 10896], "temperature": 0.0, "avg_logprob": -0.11536998513304157, "compression_ratio": 1.5254237288135593, "no_speech_prob": 0.00014062922855373472}, {"id": 82, "seek": 58260, "start": 590.24, "end": 597.8000000000001, "text": " environments. Sometimes this is everybody in the company. Organizers now unveil a carefully", "tokens": [12388, 13, 4803, 341, 307, 2201, 294, 264, 2237, 13, 12538, 22525, 586, 31009, 388, 257, 7500], "temperature": 0.0, "avg_logprob": -0.11536998513304157, "compression_ratio": 1.5254237288135593, "no_speech_prob": 0.00014062922855373472}, {"id": 83, "seek": 58260, "start": 597.8000000000001, "end": 603.96, "text": " phrased focusing question as the topic of the conversations. It is important that the", "tokens": [7636, 1937, 8416, 1168, 382, 264, 4829, 295, 264, 7315, 13, 467, 307, 1021, 300, 264], "temperature": 0.0, "avg_logprob": -0.11536998513304157, "compression_ratio": 1.5254237288135593, "no_speech_prob": 0.00014062922855373472}, {"id": 84, "seek": 58260, "start": 603.96, "end": 611.1600000000001, "text": " question is positive and focusing. For education reform, don't ask, what is wrong with our", "tokens": [1168, 307, 3353, 293, 8416, 13, 1171, 3309, 8290, 11, 500, 380, 1029, 11, 437, 307, 2085, 365, 527], "temperature": 0.0, "avg_logprob": -0.11536998513304157, "compression_ratio": 1.5254237288135593, "no_speech_prob": 0.00014062922855373472}, {"id": 85, "seek": 61116, "start": 611.16, "end": 618.48, "text": " education system? Instead, ask, what could a great school also be? The four people at", "tokens": [3309, 1185, 30, 7156, 11, 1029, 11, 437, 727, 257, 869, 1395, 611, 312, 30, 440, 1451, 561, 412], "temperature": 0.0, "avg_logprob": -0.15033683544251977, "compression_ratio": 1.5401785714285714, "no_speech_prob": 6.135493458714336e-05}, {"id": 86, "seek": 61116, "start": 618.48, "end": 624.6, "text": " each table now start a conversation around the question. Everyone takes notes on the", "tokens": [1184, 3199, 586, 722, 257, 3761, 926, 264, 1168, 13, 5198, 2516, 5570, 322, 264], "temperature": 0.0, "avg_logprob": -0.15033683544251977, "compression_ratio": 1.5401785714285714, "no_speech_prob": 6.135493458714336e-05}, {"id": 87, "seek": 61116, "start": 624.6, "end": 633.4, "text": " butcher paper, using the crayons. After 20 minutes, a gong rings. Three people. Everyone", "tokens": [41579, 3035, 11, 1228, 264, 33073, 892, 13, 2381, 945, 2077, 11, 257, 290, 556, 11136, 13, 6244, 561, 13, 5198], "temperature": 0.0, "avg_logprob": -0.15033683544251977, "compression_ratio": 1.5401785714285714, "no_speech_prob": 6.135493458714336e-05}, {"id": 88, "seek": 61116, "start": 633.4, "end": 640.04, "text": " except south in duplicate bridge terms. At each table get up and move to other tables", "tokens": [3993, 7377, 294, 23976, 7283, 2115, 13, 1711, 1184, 3199, 483, 493, 293, 1286, 281, 661, 8020], "temperature": 0.0, "avg_logprob": -0.15033683544251977, "compression_ratio": 1.5401785714285714, "no_speech_prob": 6.135493458714336e-05}, {"id": 89, "seek": 64004, "start": 640.04, "end": 647.28, "text": " at random. Through fresh random people sit down at each table. South now first explains", "tokens": [412, 4974, 13, 8927, 4451, 4974, 561, 1394, 760, 412, 1184, 3199, 13, 4242, 586, 700, 13948], "temperature": 0.0, "avg_logprob": -0.09726827959471111, "compression_ratio": 1.7135922330097086, "no_speech_prob": 4.731677836389281e-05}, {"id": 90, "seek": 64004, "start": 647.28, "end": 653.36, "text": " to the newcomers what the notes on the tablecloth mean. This provides a kind of lightweight", "tokens": [281, 264, 40014, 433, 437, 264, 5570, 322, 264, 3199, 3474, 900, 914, 13, 639, 6417, 257, 733, 295, 22052], "temperature": 0.0, "avg_logprob": -0.09726827959471111, "compression_ratio": 1.7135922330097086, "no_speech_prob": 4.731677836389281e-05}, {"id": 91, "seek": 64004, "start": 653.36, "end": 659.48, "text": " continuity from the previous conversation at this table. The three newcomers comment", "tokens": [23807, 490, 264, 3894, 3761, 412, 341, 3199, 13, 440, 1045, 40014, 433, 2871], "temperature": 0.0, "avg_logprob": -0.09726827959471111, "compression_ratio": 1.7135922330097086, "no_speech_prob": 4.731677836389281e-05}, {"id": 92, "seek": 64004, "start": 659.48, "end": 665.16, "text": " on these notes and add fresh comments. The best parts of what was said at their previous", "tokens": [322, 613, 5570, 293, 909, 4451, 3053, 13, 440, 1151, 3166, 295, 437, 390, 848, 412, 641, 3894], "temperature": 0.0, "avg_logprob": -0.09726827959471111, "compression_ratio": 1.7135922330097086, "no_speech_prob": 4.731677836389281e-05}, {"id": 93, "seek": 66516, "start": 665.16, "end": 672.92, "text": " tables. These conversations unfold very naturally. Four strangers can easily have a friendly", "tokens": [8020, 13, 1981, 7315, 17980, 588, 8195, 13, 7451, 22724, 393, 3612, 362, 257, 9208], "temperature": 0.0, "avg_logprob": -0.1283183009536178, "compression_ratio": 1.547486033519553, "no_speech_prob": 0.00013199230306781828}, {"id": 94, "seek": 66516, "start": 672.92, "end": 679.88, "text": " conversation about complex things that matter. They don't even have to introduce themselves.", "tokens": [3761, 466, 3997, 721, 300, 1871, 13, 814, 500, 380, 754, 362, 281, 5366, 2969, 13], "temperature": 0.0, "avg_logprob": -0.1283183009536178, "compression_ratio": 1.547486033519553, "no_speech_prob": 0.00013199230306781828}, {"id": 95, "seek": 66516, "start": 679.88, "end": 687.68, "text": " They contribute their wisdom and experiences. Not their resumes. Conversations now continue", "tokens": [814, 10586, 641, 10712, 293, 5235, 13, 1726, 641, 48068, 13, 33247, 763, 586, 2354], "temperature": 0.0, "avg_logprob": -0.1283183009536178, "compression_ratio": 1.547486033519553, "no_speech_prob": 0.00013199230306781828}, {"id": 96, "seek": 68768, "start": 687.68, "end": 695.4399999999999, "text": " for another 20 minutes. The gong rings again, and the shuffling repeats. After two to three", "tokens": [337, 1071, 945, 2077, 13, 440, 290, 556, 11136, 797, 11, 293, 264, 402, 1245, 1688, 35038, 13, 2381, 732, 281, 1045], "temperature": 0.0, "avg_logprob": -0.16611213462297306, "compression_ratio": 1.5474137931034482, "no_speech_prob": 0.00010019947512773797}, {"id": 97, "seek": 68768, "start": 695.4399999999999, "end": 700.88, "text": " hours, the session is over and the butcher papers are gathered by the organizers into", "tokens": [2496, 11, 264, 5481, 307, 670, 293, 264, 41579, 10577, 366, 13032, 538, 264, 35071, 666], "temperature": 0.0, "avg_logprob": -0.16611213462297306, "compression_ratio": 1.5474137931034482, "no_speech_prob": 0.00010019947512773797}, {"id": 98, "seek": 68768, "start": 700.88, "end": 708.92, "text": " what is called the harvest. They are summarized in some time later. Perhaps, after lunch,", "tokens": [437, 307, 1219, 264, 11917, 13, 814, 366, 14611, 1602, 294, 512, 565, 1780, 13, 10517, 11, 934, 6349, 11], "temperature": 0.0, "avg_logprob": -0.16611213462297306, "compression_ratio": 1.5474137931034482, "no_speech_prob": 0.00010019947512773797}, {"id": 99, "seek": 68768, "start": 708.92, "end": 716.56, "text": " the results are shared with all the stakeholders. Why this works so well? Someone pushing a", "tokens": [264, 3542, 366, 5507, 365, 439, 264, 17779, 13, 1545, 341, 1985, 370, 731, 30, 8734, 7380, 257], "temperature": 0.0, "avg_logprob": -0.16611213462297306, "compression_ratio": 1.5474137931034482, "no_speech_prob": 0.00010019947512773797}, {"id": 100, "seek": 71656, "start": 716.56, "end": 723.76, "text": " bad idea of theirs at every table can spam at worst 27 people in three hours. A good", "tokens": [1578, 1558, 295, 22760, 412, 633, 3199, 393, 24028, 412, 5855, 7634, 561, 294, 1045, 2496, 13, 316, 665], "temperature": 0.0, "avg_logprob": -0.18697185975959502, "compression_ratio": 1.497854077253219, "no_speech_prob": 0.00011780282511608675}, {"id": 101, "seek": 71656, "start": 723.76, "end": 730.68, "text": " idea. Introduced at the first table and repeated by all participants at subsequent tables will", "tokens": [1558, 13, 27193, 1232, 412, 264, 700, 3199, 293, 10477, 538, 439, 10503, 412, 19962, 8020, 486], "temperature": 0.0, "avg_logprob": -0.18697185975959502, "compression_ratio": 1.497854077253219, "no_speech_prob": 0.00011780282511608675}, {"id": 102, "seek": 71656, "start": 730.68, "end": 737.8399999999999, "text": " reach over 100,000 people or the majority of the audience, whichever is smaller. This", "tokens": [2524, 670, 2319, 11, 1360, 561, 420, 264, 6286, 295, 264, 4034, 11, 24123, 307, 4356, 13, 639], "temperature": 0.0, "avg_logprob": -0.18697185975959502, "compression_ratio": 1.497854077253219, "no_speech_prob": 0.00011780282511608675}, {"id": 103, "seek": 71656, "start": 737.8399999999999, "end": 744.0, "text": " is the filtering power of the World Caf\u00e9 protocol. Wisdom Salon is an online World", "tokens": [307, 264, 30822, 1347, 295, 264, 3937, 46701, 526, 10336, 13, 34143, 4121, 5996, 266, 307, 364, 2950, 3937], "temperature": 0.0, "avg_logprob": -0.18697185975959502, "compression_ratio": 1.497854077253219, "no_speech_prob": 0.00011780282511608675}, {"id": 104, "seek": 74400, "start": 744.0, "end": 751.4, "text": " Caf\u00e9. Sadly, the Wisdom Salon project has been suspended because of changing infrastructure", "tokens": [46701, 526, 13, 29628, 11, 264, 34143, 4121, 5996, 266, 1716, 575, 668, 23437, 570, 295, 4473, 6896], "temperature": 0.0, "avg_logprob": -0.13475613421704397, "compression_ratio": 1.5708333333333333, "no_speech_prob": 0.00017703708726912737}, {"id": 105, "seek": 74400, "start": 751.4, "end": 758.44, "text": " and cost structure for online video transmissions, and because of lack of time on my part. It", "tokens": [293, 2063, 3877, 337, 2950, 960, 7715, 7922, 11, 293, 570, 295, 5011, 295, 565, 322, 452, 644, 13, 467], "temperature": 0.0, "avg_logprob": -0.13475613421704397, "compression_ratio": 1.5708333333333333, "no_speech_prob": 0.00017703708726912737}, {"id": 106, "seek": 74400, "start": 758.44, "end": 764.68, "text": " is possible to restart the project using current video technology and with funding and a larger", "tokens": [307, 1944, 281, 21022, 264, 1716, 1228, 2190, 960, 2899, 293, 365, 6137, 293, 257, 4833], "temperature": 0.0, "avg_logprob": -0.13475613421704397, "compression_ratio": 1.5708333333333333, "no_speech_prob": 0.00017703708726912737}, {"id": 107, "seek": 74400, "start": 764.68, "end": 772.28, "text": " team. If interested in contributing to this, please get in touch. What follows is the original", "tokens": [1469, 13, 759, 3102, 294, 19270, 281, 341, 11, 1767, 483, 294, 2557, 13, 708, 10002, 307, 264, 3380], "temperature": 0.0, "avg_logprob": -0.13475613421704397, "compression_ratio": 1.5708333333333333, "no_speech_prob": 0.00017703708726912737}, {"id": 108, "seek": 77228, "start": 772.28, "end": 780.28, "text": " high-level design specification, written in the present tense, design specification.", "tokens": [1090, 12, 12418, 1715, 31256, 11, 3720, 294, 264, 1974, 18760, 11, 1715, 31256, 13], "temperature": 0.0, "avg_logprob": -0.14401246680588017, "compression_ratio": 1.467032967032967, "no_speech_prob": 0.00011588446795940399}, {"id": 109, "seek": 77228, "start": 780.28, "end": 788.8399999999999, "text": " The Wisdom Salon is a 24-7 online World Caf\u00e9 implemented as a video chat site. Conversations", "tokens": [440, 34143, 4121, 5996, 266, 307, 257, 4022, 12, 22, 2950, 3937, 46701, 526, 12270, 382, 257, 960, 5081, 3621, 13, 33247, 763], "temperature": 0.0, "avg_logprob": -0.14401246680588017, "compression_ratio": 1.467032967032967, "no_speech_prob": 0.00011588446795940399}, {"id": 110, "seek": 77228, "start": 788.8399999999999, "end": 794.64, "text": " have four participants, but each conversation can also have a passive and quiet audience", "tokens": [362, 1451, 10503, 11, 457, 1184, 3761, 393, 611, 362, 257, 14975, 293, 5677, 4034], "temperature": 0.0, "avg_logprob": -0.14401246680588017, "compression_ratio": 1.467032967032967, "no_speech_prob": 0.00011588446795940399}, {"id": 111, "seek": 79464, "start": 794.64, "end": 802.8, "text": " of any size. All conversations are always public. All conversation participants are", "tokens": [295, 604, 2744, 13, 1057, 7315, 366, 1009, 1908, 13, 1057, 3761, 10503, 366], "temperature": 0.0, "avg_logprob": -0.113113935891684, "compression_ratio": 1.7033492822966507, "no_speech_prob": 0.00013785470218863338}, {"id": 112, "seek": 79464, "start": 802.8, "end": 809.96, "text": " known by their login identities. Why would anyone want to participate? The main purpose", "tokens": [2570, 538, 641, 24276, 24239, 13, 1545, 576, 2878, 528, 281, 8197, 30, 440, 2135, 4334], "temperature": 0.0, "avg_logprob": -0.113113935891684, "compression_ratio": 1.7033492822966507, "no_speech_prob": 0.00013785470218863338}, {"id": 113, "seek": 79464, "start": 809.96, "end": 816.64, "text": " of Wisdom Salon is increased wisdom and improved clarity and complex issues for the participants.", "tokens": [295, 34143, 4121, 5996, 266, 307, 6505, 10712, 293, 9689, 16992, 293, 3997, 2663, 337, 264, 10503, 13], "temperature": 0.0, "avg_logprob": -0.113113935891684, "compression_ratio": 1.7033492822966507, "no_speech_prob": 0.00013785470218863338}, {"id": 114, "seek": 79464, "start": 816.64, "end": 823.36, "text": " This is your main benefit. This is why you would want to participate. You will not get", "tokens": [639, 307, 428, 2135, 5121, 13, 639, 307, 983, 291, 576, 528, 281, 8197, 13, 509, 486, 406, 483], "temperature": 0.0, "avg_logprob": -0.113113935891684, "compression_ratio": 1.7033492822966507, "no_speech_prob": 0.00013785470218863338}, {"id": 115, "seek": 82336, "start": 823.36, "end": 829.92, "text": " lags, but you might earn a local currency, called, Influence, that you can selectively", "tokens": [8953, 82, 11, 457, 291, 1062, 6012, 257, 2654, 13346, 11, 1219, 11, 11537, 40432, 11, 300, 291, 393, 3048, 3413], "temperature": 0.0, "avg_logprob": -0.17548934273097827, "compression_ratio": 1.6437768240343347, "no_speech_prob": 0.00017720666073728353}, {"id": 116, "seek": 82336, "start": 829.92, "end": 832.6, "text": " use to extend your influence.", "tokens": [764, 281, 10101, 428, 6503, 13], "temperature": 0.0, "avg_logprob": -0.17548934273097827, "compression_ratio": 1.6437768240343347, "no_speech_prob": 0.00017720666073728353}, {"id": 117, "seek": 82336, "start": 832.6, "end": 839.72, "text": " Goal. The goal is specifically not to find the best grains of wisdom in the harvest.", "tokens": [1037, 304, 13, 440, 3387, 307, 4682, 406, 281, 915, 264, 1151, 22908, 295, 10712, 294, 264, 11917, 13], "temperature": 0.0, "avg_logprob": -0.17548934273097827, "compression_ratio": 1.6437768240343347, "no_speech_prob": 0.00017720666073728353}, {"id": 118, "seek": 82336, "start": 839.72, "end": 845.92, "text": " The grains are there mainly to provide continuity and shorten the time to get to talking about", "tokens": [440, 22908, 366, 456, 8704, 281, 2893, 23807, 293, 39632, 264, 565, 281, 483, 281, 1417, 466], "temperature": 0.0, "avg_logprob": -0.17548934273097827, "compression_ratio": 1.6437768240343347, "no_speech_prob": 0.00017720666073728353}, {"id": 119, "seek": 82336, "start": 845.92, "end": 852.2, "text": " things that matter. The system is there to provide the users a chance to analyze large", "tokens": [721, 300, 1871, 13, 440, 1185, 307, 456, 281, 2893, 264, 5022, 257, 2931, 281, 12477, 2416], "temperature": 0.0, "avg_logprob": -0.17548934273097827, "compression_ratio": 1.6437768240343347, "no_speech_prob": 0.00017720666073728353}, {"id": 120, "seek": 85220, "start": 852.2, "end": 860.72, "text": " and complex issues with others in conversation and in exchange of experiences. Do not underestimate", "tokens": [293, 3997, 2663, 365, 2357, 294, 3761, 293, 294, 7742, 295, 5235, 13, 1144, 406, 35826], "temperature": 0.0, "avg_logprob": -0.14655558268229166, "compression_ratio": 1.6666666666666667, "no_speech_prob": 8.749114931561053e-05}, {"id": 121, "seek": 85220, "start": 860.72, "end": 867.5200000000001, "text": " how different an interactive conversation is from a web search or reading a book. Have", "tokens": [577, 819, 364, 15141, 3761, 307, 490, 257, 3670, 3164, 420, 3760, 257, 1446, 13, 3560], "temperature": 0.0, "avg_logprob": -0.14655558268229166, "compression_ratio": 1.6666666666666667, "no_speech_prob": 8.749114931561053e-05}, {"id": 122, "seek": 85220, "start": 867.5200000000001, "end": 874.0400000000001, "text": " you ever spent days studying something without getting it only to have someone set you straight", "tokens": [291, 1562, 4418, 1708, 7601, 746, 1553, 1242, 309, 787, 281, 362, 1580, 992, 291, 2997], "temperature": 0.0, "avg_logprob": -0.14655558268229166, "compression_ratio": 1.6666666666666667, "no_speech_prob": 8.749114931561053e-05}, {"id": 123, "seek": 85220, "start": 874.0400000000001, "end": 880.2800000000001, "text": " in two minutes of conversation? Have you ever been in a meeting where the resolution is", "tokens": [294, 732, 2077, 295, 3761, 30, 3560, 291, 1562, 668, 294, 257, 3440, 689, 264, 8669, 307], "temperature": 0.0, "avg_logprob": -0.14655558268229166, "compression_ratio": 1.6666666666666667, "no_speech_prob": 8.749114931561053e-05}, {"id": 124, "seek": 88028, "start": 880.28, "end": 886.0, "text": " something none of the participants even understood when the meeting started?", "tokens": [746, 6022, 295, 264, 10503, 754, 7320, 562, 264, 3440, 1409, 30], "temperature": 0.0, "avg_logprob": -0.19690985361735025, "compression_ratio": 1.603864734299517, "no_speech_prob": 0.00013917969772592187}, {"id": 125, "seek": 88028, "start": 886.0, "end": 893.04, "text": " Sample questions. What kinds of questions demonstrate the power of the Wisdom Salon?", "tokens": [4832, 781, 1651, 13, 708, 3685, 295, 1651, 11698, 264, 1347, 295, 264, 34143, 4121, 5996, 266, 30], "temperature": 0.0, "avg_logprob": -0.19690985361735025, "compression_ratio": 1.603864734299517, "no_speech_prob": 0.00013917969772592187}, {"id": 126, "seek": 88028, "start": 893.04, "end": 901.0799999999999, "text": " Consider these samples. I am considering a midlife career change. What matters? Where", "tokens": [17416, 613, 10938, 13, 286, 669, 8079, 257, 2062, 9073, 3988, 1319, 13, 708, 7001, 30, 2305], "temperature": 0.0, "avg_logprob": -0.19690985361735025, "compression_ratio": 1.603864734299517, "no_speech_prob": 0.00013917969772592187}, {"id": 127, "seek": 88028, "start": 901.0799999999999, "end": 909.56, "text": " should I retire, and why there? Should I pursue a career in engineering or medicine?", "tokens": [820, 286, 10731, 11, 293, 983, 456, 30, 6454, 286, 12392, 257, 3988, 294, 7043, 420, 7195, 30], "temperature": 0.0, "avg_logprob": -0.19690985361735025, "compression_ratio": 1.603864734299517, "no_speech_prob": 0.00013917969772592187}, {"id": 128, "seek": 90956, "start": 909.56, "end": 916.4, "text": " Lifestyle design in interesting times. What is the true promise of genetics research and", "tokens": [31946, 9985, 1715, 294, 1880, 1413, 13, 708, 307, 264, 2074, 6228, 295, 26516, 2132, 293], "temperature": 0.0, "avg_logprob": -0.1625247637430827, "compression_ratio": 1.5590062111801242, "no_speech_prob": 0.00012096583668608218}, {"id": 129, "seek": 90956, "start": 916.4, "end": 923.8, "text": " why should I care? What movies should I let my children watch, and why?", "tokens": [983, 820, 286, 1127, 30, 708, 6233, 820, 286, 718, 452, 2227, 1159, 11, 293, 983, 30], "temperature": 0.0, "avg_logprob": -0.1625247637430827, "compression_ratio": 1.5590062111801242, "no_speech_prob": 0.00012096583668608218}, {"id": 130, "seek": 90956, "start": 923.8, "end": 932.7199999999999, "text": " Musical education for my child. What matters? What instruments, and why? What is it really", "tokens": [42527, 3309, 337, 452, 1440, 13, 708, 7001, 30, 708, 12190, 11, 293, 983, 30, 708, 307, 309, 534], "temperature": 0.0, "avg_logprob": -0.1625247637430827, "compression_ratio": 1.5590062111801242, "no_speech_prob": 0.00012096583668608218}, {"id": 131, "seek": 93272, "start": 932.72, "end": 941.64, "text": " like to be a soldier in places like Afghanistan and Iraq? Should I retire in Costa Rica? User", "tokens": [411, 281, 312, 257, 15632, 294, 3190, 411, 13658, 293, 11818, 30, 6454, 286, 10731, 294, 28440, 42080, 30, 32127], "temperature": 0.0, "avg_logprob": -0.17458739209530957, "compression_ratio": 1.4948453608247423, "no_speech_prob": 6.649745773756877e-05}, {"id": 132, "seek": 93272, "start": 941.64, "end": 949.0, "text": " experience. People arrive when they want and leave when they want. They can engage in multiple", "tokens": [1752, 13, 3432, 8881, 562, 436, 528, 293, 1856, 562, 436, 528, 13, 814, 393, 4683, 294, 3866], "temperature": 0.0, "avg_logprob": -0.17458739209530957, "compression_ratio": 1.4948453608247423, "no_speech_prob": 6.649745773756877e-05}, {"id": 133, "seek": 93272, "start": 949.0, "end": 958.0, "text": " ways. Upon entering the site, users are presented with the, at the moment, most popular conversation,", "tokens": [2098, 13, 25184, 11104, 264, 3621, 11, 5022, 366, 8212, 365, 264, 11, 412, 264, 1623, 11, 881, 3743, 3761, 11], "temperature": 0.0, "avg_logprob": -0.17458739209530957, "compression_ratio": 1.4948453608247423, "no_speech_prob": 6.649745773756877e-05}, {"id": 134, "seek": 95800, "start": 958.0, "end": 964.64, "text": " the one with the largest audience. Below the conversation, there will be a list of other", "tokens": [264, 472, 365, 264, 6443, 4034, 13, 36261, 264, 3761, 11, 456, 486, 312, 257, 1329, 295, 661], "temperature": 0.0, "avg_logprob": -0.11262264516618517, "compression_ratio": 1.6866359447004609, "no_speech_prob": 5.208213769947179e-05}, {"id": 135, "seek": 95800, "start": 964.64, "end": 971.8, "text": " popular conversations, headed by conversations and topics the user may have watched or previously", "tokens": [3743, 7315, 11, 12798, 538, 7315, 293, 8378, 264, 4195, 815, 362, 6337, 420, 8046], "temperature": 0.0, "avg_logprob": -0.11262264516618517, "compression_ratio": 1.6866359447004609, "no_speech_prob": 5.208213769947179e-05}, {"id": 136, "seek": 95800, "start": 971.8, "end": 978.8, "text": " participated in. They can browse all ongoing conversations much like watching talk shows", "tokens": [17978, 294, 13, 814, 393, 31442, 439, 10452, 7315, 709, 411, 1976, 751, 3110], "temperature": 0.0, "avg_logprob": -0.11262264516618517, "compression_ratio": 1.6866359447004609, "no_speech_prob": 5.208213769947179e-05}, {"id": 137, "seek": 95800, "start": 978.8, "end": 985.64, "text": " on television. They can select from hundreds of questions to find something that interests", "tokens": [322, 8815, 13, 814, 393, 3048, 490, 6779, 295, 1651, 281, 915, 746, 300, 8847], "temperature": 0.0, "avg_logprob": -0.11262264516618517, "compression_ratio": 1.6866359447004609, "no_speech_prob": 5.208213769947179e-05}, {"id": 138, "seek": 98564, "start": 985.64, "end": 993.0, "text": " them, or add their own. Instead of a butcher paper, they can leave notes on each question", "tokens": [552, 11, 420, 909, 641, 1065, 13, 7156, 295, 257, 41579, 3035, 11, 436, 393, 1856, 5570, 322, 1184, 1168], "temperature": 0.0, "avg_logprob": -0.14787429854983375, "compression_ratio": 1.6543778801843319, "no_speech_prob": 8.130294736474752e-05}, {"id": 139, "seek": 98564, "start": 993.0, "end": 999.68, "text": " known as, grains of wisdom, to provide the lightweight continuity from table to table.", "tokens": [2570, 382, 11, 22908, 295, 10712, 11, 281, 2893, 264, 22052, 23807, 490, 3199, 281, 3199, 13], "temperature": 0.0, "avg_logprob": -0.14787429854983375, "compression_ratio": 1.6543778801843319, "no_speech_prob": 8.130294736474752e-05}, {"id": 140, "seek": 98564, "start": 999.68, "end": 1007.12, "text": " They can vote on these grains of wisdom so that they better result rise to the top. Results", "tokens": [814, 393, 4740, 322, 613, 22908, 295, 10712, 370, 300, 436, 1101, 1874, 6272, 281, 264, 1192, 13, 5015, 33361], "temperature": 0.0, "avg_logprob": -0.14787429854983375, "compression_ratio": 1.6543778801843319, "no_speech_prob": 8.130294736474752e-05}, {"id": 141, "seek": 98564, "start": 1007.12, "end": 1014.08, "text": " are immediately visible to all. They can observe what other people say and how they behave", "tokens": [366, 4258, 8974, 281, 439, 13, 814, 393, 11441, 437, 661, 561, 584, 293, 577, 436, 15158], "temperature": 0.0, "avg_logprob": -0.14787429854983375, "compression_ratio": 1.6543778801843319, "no_speech_prob": 8.130294736474752e-05}, {"id": 142, "seek": 101408, "start": 1014.08, "end": 1019.88, "text": " and modify their own social graph to improve their chances of interaction with the best", "tokens": [293, 16927, 641, 1065, 2093, 4295, 281, 3470, 641, 10486, 295, 9285, 365, 264, 1151], "temperature": 0.0, "avg_logprob": -0.12846360335478912, "compression_ratio": 1.681159420289855, "no_speech_prob": 6.754406786058098e-05}, {"id": 143, "seek": 101408, "start": 1019.88, "end": 1026.88, "text": " people. A local currency is earned by passive engagement per hour, more of it is earned", "tokens": [561, 13, 316, 2654, 13346, 307, 12283, 538, 14975, 8742, 680, 1773, 11, 544, 295, 309, 307, 12283], "temperature": 0.0, "avg_logprob": -0.12846360335478912, "compression_ratio": 1.681159420289855, "no_speech_prob": 6.754406786058098e-05}, {"id": 144, "seek": 101408, "start": 1026.88, "end": 1032.6000000000001, "text": " by participating in conversations, and the currency is used to pay for the privilege", "tokens": [538, 13950, 294, 7315, 11, 293, 264, 13346, 307, 1143, 281, 1689, 337, 264, 12122], "temperature": 0.0, "avg_logprob": -0.12846360335478912, "compression_ratio": 1.681159420289855, "no_speech_prob": 6.754406786058098e-05}, {"id": 145, "seek": 101408, "start": 1032.6000000000001, "end": 1039.4, "text": " of posting a comment, because posting cost currency, spelling the grains of wisdom will", "tokens": [295, 15978, 257, 2871, 11, 570, 15978, 2063, 13346, 11, 22254, 264, 22908, 295, 10712, 486], "temperature": 0.0, "avg_logprob": -0.12846360335478912, "compression_ratio": 1.681159420289855, "no_speech_prob": 6.754406786058098e-05}, {"id": 146, "seek": 103940, "start": 1039.4, "end": 1046.4, "text": " be limited. A topic without currently active conversations still allows you to browse the", "tokens": [312, 5567, 13, 316, 4829, 1553, 4362, 4967, 7315, 920, 4045, 291, 281, 31442, 264], "temperature": 0.0, "avg_logprob": -0.12552289735703243, "compression_ratio": 1.6559633027522935, "no_speech_prob": 0.00010381497122580186}, {"id": 147, "seek": 103940, "start": 1046.4, "end": 1053.8000000000002, "text": " grains of wisdom on the topic, and if you have influence, you can vote on the grains or notes", "tokens": [22908, 295, 10712, 322, 264, 4829, 11, 293, 498, 291, 362, 6503, 11, 291, 393, 4740, 322, 264, 22908, 420, 5570], "temperature": 0.0, "avg_logprob": -0.12552289735703243, "compression_ratio": 1.6559633027522935, "no_speech_prob": 0.00010381497122580186}, {"id": 148, "seek": 103940, "start": 1053.8000000000002, "end": 1060.0, "text": " that you like or otherwise agree with, and you can restart the topic by creating a table", "tokens": [300, 291, 411, 420, 5911, 3986, 365, 11, 293, 291, 393, 21022, 264, 4829, 538, 4084, 257, 3199], "temperature": 0.0, "avg_logprob": -0.12552289735703243, "compression_ratio": 1.6559633027522935, "no_speech_prob": 0.00010381497122580186}, {"id": 149, "seek": 103940, "start": 1060.0, "end": 1067.64, "text": " and hope others will join. Four main uses of wisdom salon. The site enables, but doesn't", "tokens": [293, 1454, 2357, 486, 3917, 13, 7451, 2135, 4960, 295, 10712, 27768, 13, 440, 3621, 17077, 11, 457, 1177, 380], "temperature": 0.0, "avg_logprob": -0.12552289735703243, "compression_ratio": 1.6559633027522935, "no_speech_prob": 0.00010381497122580186}, {"id": 150, "seek": 106764, "start": 1067.64, "end": 1075.2, "text": " enforce the World Cafe protocol. You can use the site for several different purposes.", "tokens": [24825, 264, 3937, 35864, 10336, 13, 509, 393, 764, 264, 3621, 337, 2940, 819, 9932, 13], "temperature": 0.0, "avg_logprob": -0.17899834408479579, "compression_ratio": 1.5401785714285714, "no_speech_prob": 0.00018835024093277752}, {"id": 151, "seek": 106764, "start": 1075.2, "end": 1081.5200000000002, "text": " As entertainment and education, passively watching conversations among your peers,", "tokens": [1018, 12393, 293, 3309, 11, 1320, 3413, 1976, 7315, 3654, 428, 16739, 11], "temperature": 0.0, "avg_logprob": -0.17899834408479579, "compression_ratio": 1.5401785714285714, "no_speech_prob": 0.00018835024093277752}, {"id": 152, "seek": 106764, "start": 1081.5200000000002, "end": 1088.0800000000002, "text": " much like flipping channels on television. To get both factual information and broad", "tokens": [709, 411, 26886, 9235, 322, 8815, 13, 1407, 483, 1293, 48029, 1589, 293, 4152], "temperature": 0.0, "avg_logprob": -0.17899834408479579, "compression_ratio": 1.5401785714285714, "no_speech_prob": 0.00018835024093277752}, {"id": 153, "seek": 106764, "start": 1088.0800000000002, "end": 1096.0800000000002, "text": " ranging personalized advice from experts. To share your expertise in fields you understand.", "tokens": [25532, 28415, 5192, 490, 8572, 13, 1407, 2073, 428, 11769, 294, 7909, 291, 1223, 13], "temperature": 0.0, "avg_logprob": -0.17899834408479579, "compression_ratio": 1.5401785714285714, "no_speech_prob": 0.00018835024093277752}, {"id": 154, "seek": 109608, "start": 1096.08, "end": 1104.08, "text": " To do micromantering. To find an audience for storytelling and sharing personal experiences", "tokens": [1407, 360, 3123, 81, 4277, 34200, 13, 1407, 915, 364, 4034, 337, 21479, 293, 5414, 2973, 5235], "temperature": 0.0, "avg_logprob": -0.13694083384978464, "compression_ratio": 1.7156398104265402, "no_speech_prob": 0.0001088599965441972}, {"id": 155, "seek": 109608, "start": 1104.08, "end": 1112.6399999999999, "text": " from your life. To gain wisdom and personal clarity in complex issues. To debate the major", "tokens": [490, 428, 993, 13, 1407, 6052, 10712, 293, 2973, 16992, 294, 3997, 2663, 13, 1407, 7958, 264, 2563], "temperature": 0.0, "avg_logprob": -0.13694083384978464, "compression_ratio": 1.7156398104265402, "no_speech_prob": 0.0001088599965441972}, {"id": 156, "seek": 109608, "start": 1112.6399999999999, "end": 1119.6799999999998, "text": " issues of the day in person and productively selected and well behaved groups. To find", "tokens": [2663, 295, 264, 786, 294, 954, 293, 1674, 3413, 8209, 293, 731, 48249, 3935, 13, 1407, 915], "temperature": 0.0, "avg_logprob": -0.13694083384978464, "compression_ratio": 1.7156398104265402, "no_speech_prob": 0.0001088599965441972}, {"id": 157, "seek": 109608, "start": 1119.6799999999998, "end": 1125.6, "text": " new interesting and competent friends by observing their behavior and then befriending them,", "tokens": [777, 1880, 293, 29998, 1855, 538, 22107, 641, 5223, 293, 550, 21312, 470, 2029, 552, 11], "temperature": 0.0, "avg_logprob": -0.13694083384978464, "compression_ratio": 1.7156398104265402, "no_speech_prob": 0.0001088599965441972}, {"id": 158, "seek": 112560, "start": 1125.6, "end": 1134.32, "text": " much like other social media. Any active conversation starts a 20 minute clock bar moving. You", "tokens": [709, 411, 661, 2093, 3021, 13, 2639, 4967, 3761, 3719, 257, 945, 3456, 7830, 2159, 2684, 13, 509], "temperature": 0.0, "avg_logprob": -0.15700069069862366, "compression_ratio": 1.547486033519553, "no_speech_prob": 0.00018820373225025833}, {"id": 159, "seek": 112560, "start": 1134.32, "end": 1141.56, "text": " can leave anytime. System provides some incentive to stay the full 20 minutes. On the other", "tokens": [393, 1856, 13038, 13, 8910, 6417, 512, 22346, 281, 1754, 264, 1577, 945, 2077, 13, 1282, 264, 661], "temperature": 0.0, "avg_logprob": -0.15700069069862366, "compression_ratio": 1.547486033519553, "no_speech_prob": 0.00018820373225025833}, {"id": 160, "seek": 112560, "start": 1141.56, "end": 1149.0, "text": " hand, you don't have to leave after 20 minutes. If you like, you can continue conversation", "tokens": [1011, 11, 291, 500, 380, 362, 281, 1856, 934, 945, 2077, 13, 759, 291, 411, 11, 291, 393, 2354, 3761], "temperature": 0.0, "avg_logprob": -0.15700069069862366, "compression_ratio": 1.547486033519553, "no_speech_prob": 0.00018820373225025833}, {"id": 161, "seek": 114900, "start": 1149.0, "end": 1156.92, "text": " along as you want. But we expect a large fraction of people to adhere to the protocol. We believe", "tokens": [2051, 382, 291, 528, 13, 583, 321, 2066, 257, 2416, 14135, 295, 561, 281, 33584, 281, 264, 10336, 13, 492, 1697], "temperature": 0.0, "avg_logprob": -0.12618990377946335, "compression_ratio": 1.5892116182572613, "no_speech_prob": 8.436061762040481e-05}, {"id": 162, "seek": 114900, "start": 1156.92, "end": 1164.24, "text": " this maximizes the wisdom gain per session. Without the right people, the system is worthless.", "tokens": [341, 5138, 5660, 264, 10712, 6052, 680, 5481, 13, 9129, 264, 558, 561, 11, 264, 1185, 307, 34857, 13], "temperature": 0.0, "avg_logprob": -0.12618990377946335, "compression_ratio": 1.5892116182572613, "no_speech_prob": 8.436061762040481e-05}, {"id": 163, "seek": 114900, "start": 1164.24, "end": 1172.0, "text": " Do not be discouraged. Facebook would be worthless with only 10 people on it. Wisdom salon really", "tokens": [1144, 406, 312, 35010, 13, 4384, 576, 312, 34857, 365, 787, 1266, 561, 322, 309, 13, 34143, 4121, 27768, 534], "temperature": 0.0, "avg_logprob": -0.12618990377946335, "compression_ratio": 1.5892116182572613, "no_speech_prob": 8.436061762040481e-05}, {"id": 164, "seek": 114900, "start": 1172.0, "end": 1178.48, "text": " requires at least 50 people to be on the system before you are likely to find a conversation", "tokens": [7029, 412, 1935, 2625, 561, 281, 312, 322, 264, 1185, 949, 291, 366, 3700, 281, 915, 257, 3761], "temperature": 0.0, "avg_logprob": -0.12618990377946335, "compression_ratio": 1.5892116182572613, "no_speech_prob": 8.436061762040481e-05}, {"id": 165, "seek": 117848, "start": 1178.48, "end": 1184.96, "text": " around a question you actually care about anytime you join. So nobody knows if this", "tokens": [926, 257, 1168, 291, 767, 1127, 466, 13038, 291, 3917, 13, 407, 5079, 3255, 498, 341], "temperature": 0.0, "avg_logprob": -0.10170374720929617, "compression_ratio": 1.5315315315315314, "no_speech_prob": 0.0001010070409392938}, {"id": 166, "seek": 117848, "start": 1184.96, "end": 1190.6, "text": " will work or not, and it may take a while before the system matures enough to attract", "tokens": [486, 589, 420, 406, 11, 293, 309, 815, 747, 257, 1339, 949, 264, 1185, 275, 3377, 1547, 281, 5049], "temperature": 0.0, "avg_logprob": -0.10170374720929617, "compression_ratio": 1.5315315315315314, "no_speech_prob": 0.0001010070409392938}, {"id": 167, "seek": 117848, "start": 1190.6, "end": 1196.4, "text": " a sufficient repeat audience to become what I designed it for. If you don't like it", "tokens": [257, 11563, 7149, 4034, 281, 1813, 437, 286, 4761, 309, 337, 13, 759, 291, 500, 380, 411, 309], "temperature": 0.0, "avg_logprob": -0.10170374720929617, "compression_ratio": 1.5315315315315314, "no_speech_prob": 0.0001010070409392938}, {"id": 168, "seek": 117848, "start": 1196.4, "end": 1203.0, "text": " at first, please try again. It might well improve, and you might get lucky to get into", "tokens": [412, 700, 11, 1767, 853, 797, 13, 467, 1062, 731, 3470, 11, 293, 291, 1062, 483, 6356, 281, 483, 666], "temperature": 0.0, "avg_logprob": -0.10170374720929617, "compression_ratio": 1.5315315315315314, "no_speech_prob": 0.0001010070409392938}, {"id": 169, "seek": 120300, "start": 1203.0, "end": 1210.36, "text": " an amazing conversation when you least expect it. Welcome to my experiment.", "tokens": [364, 2243, 3761, 562, 291, 1935, 2066, 309, 13, 4027, 281, 452, 5120, 13], "temperature": 0.0, "avg_logprob": -0.18391382333004114, "compression_ratio": 1.4293785310734464, "no_speech_prob": 0.0001314524852205068}, {"id": 170, "seek": 120300, "start": 1210.36, "end": 1219.36, "text": " The lavender pill. Model free AI. Don't model the world. Just model the mind. It's a lot", "tokens": [440, 43757, 8100, 13, 17105, 1737, 7318, 13, 1468, 380, 2316, 264, 1002, 13, 1449, 2316, 264, 1575, 13, 467, 311, 257, 688], "temperature": 0.0, "avg_logprob": -0.18391382333004114, "compression_ratio": 1.4293785310734464, "no_speech_prob": 0.0001314524852205068}, {"id": 171, "seek": 120300, "start": 1219.36, "end": 1229.12, "text": " easier. With some poetic freedom, I'd like to claim 1. Model the world. 10 billion lines", "tokens": [3571, 13, 2022, 512, 41080, 5645, 11, 286, 1116, 411, 281, 3932, 502, 13, 17105, 264, 1002, 13, 1266, 5218, 3876], "temperature": 0.0, "avg_logprob": -0.18391382333004114, "compression_ratio": 1.4293785310734464, "no_speech_prob": 0.0001314524852205068}, {"id": 172, "seek": 122912, "start": 1229.12, "end": 1240.9599999999998, "text": " of code. 2. Model the brain. 10 million lines of code. 3. Model the mind. 10,000 lines of", "tokens": [295, 3089, 13, 568, 13, 17105, 264, 3567, 13, 1266, 2459, 3876, 295, 3089, 13, 805, 13, 17105, 264, 1575, 13, 1266, 11, 1360, 3876, 295], "temperature": 0.0, "avg_logprob": -0.12178893522782759, "compression_ratio": 1.5529411764705883, "no_speech_prob": 0.00011843912216136232}, {"id": 173, "seek": 122912, "start": 1240.9599999999998, "end": 1248.6399999999999, "text": " code. Number one is regular programming. We make computers perform actions in a context", "tokens": [3089, 13, 5118, 472, 307, 3890, 9410, 13, 492, 652, 10807, 2042, 5909, 294, 257, 4319], "temperature": 0.0, "avg_logprob": -0.12178893522782759, "compression_ratio": 1.5529411764705883, "no_speech_prob": 0.00011843912216136232}, {"id": 174, "seek": 122912, "start": 1248.6399999999999, "end": 1254.4799999999998, "text": " that matches the programmer's mental model of some relevant parts of the world. Number", "tokens": [300, 10676, 264, 32116, 311, 4973, 2316, 295, 512, 7340, 3166, 295, 264, 1002, 13, 5118], "temperature": 0.0, "avg_logprob": -0.12178893522782759, "compression_ratio": 1.5529411764705883, "no_speech_prob": 0.00011843912216136232}, {"id": 175, "seek": 125448, "start": 1254.48, "end": 1261.16, "text": " two is neuroscience-based models of neurons, synapses and other biological structures and", "tokens": [732, 307, 42762, 12, 6032, 5245, 295, 22027, 11, 5451, 2382, 279, 293, 661, 13910, 9227, 293], "temperature": 0.0, "avg_logprob": -0.13988429230528993, "compression_ratio": 1.7235023041474655, "no_speech_prob": 6.461836892412975e-05}, {"id": 176, "seek": 125448, "start": 1261.16, "end": 1269.92, "text": " systems in brains. The number three is epistemology-based models of learning, understanding, reasoning,", "tokens": [3652, 294, 15442, 13, 440, 1230, 1045, 307, 2388, 43958, 1793, 12, 6032, 5245, 295, 2539, 11, 3701, 11, 21577, 11], "temperature": 0.0, "avg_logprob": -0.13988429230528993, "compression_ratio": 1.7235023041474655, "no_speech_prob": 6.461836892412975e-05}, {"id": 177, "seek": 125448, "start": 1269.92, "end": 1277.16, "text": " prediction, abstraction, and other holistic and emergent phenomena. Epistemology-based", "tokens": [17630, 11, 37765, 11, 293, 661, 30334, 293, 4345, 6930, 22004, 13, 9970, 43958, 1793, 12, 6032], "temperature": 0.0, "avg_logprob": -0.13988429230528993, "compression_ratio": 1.7235023041474655, "no_speech_prob": 6.461836892412975e-05}, {"id": 178, "seek": 125448, "start": 1277.16, "end": 1283.44, "text": " methods require a rather minimal infrastructure to support whatever operations these concepts", "tokens": [7150, 3651, 257, 2831, 13206, 6896, 281, 1406, 2035, 7705, 613, 10392], "temperature": 0.0, "avg_logprob": -0.13988429230528993, "compression_ratio": 1.7235023041474655, "no_speech_prob": 6.461836892412975e-05}, {"id": 179, "seek": 128344, "start": 1283.44, "end": 1290.68, "text": " require. I put models within irony quotes because they are strictly speaking metamodels", "tokens": [3651, 13, 286, 829, 5245, 1951, 35365, 19963, 570, 436, 366, 20792, 4124, 1131, 335, 378, 1625], "temperature": 0.0, "avg_logprob": -0.1783422028146139, "compression_ratio": 1.6267281105990783, "no_speech_prob": 8.778930350672454e-05}, {"id": 180, "seek": 128344, "start": 1290.68, "end": 1297.3600000000001, "text": " because they are used in metascales. They are not about skills, such as English or folding", "tokens": [570, 436, 366, 1143, 294, 1131, 4806, 4229, 13, 814, 366, 406, 466, 3942, 11, 1270, 382, 3669, 420, 25335], "temperature": 0.0, "avg_logprob": -0.1783422028146139, "compression_ratio": 1.6267281105990783, "no_speech_prob": 8.778930350672454e-05}, {"id": 181, "seek": 128344, "start": 1297.3600000000001, "end": 1304.28, "text": " proteins. They are about how to acquire such skills by learning from our mistakes.", "tokens": [15577, 13, 814, 366, 466, 577, 281, 20001, 1270, 3942, 538, 2539, 490, 527, 8038, 13], "temperature": 0.0, "avg_logprob": -0.1783422028146139, "compression_ratio": 1.6267281105990783, "no_speech_prob": 8.778930350672454e-05}, {"id": 182, "seek": 128344, "start": 1304.28, "end": 1312.8400000000001, "text": " The purple pill. Corpus congruence. Understanding in brains and machines can be defined and", "tokens": [440, 9656, 8100, 13, 3925, 31624, 8882, 84, 655, 13, 36858, 294, 15442, 293, 8379, 393, 312, 7642, 293], "temperature": 0.0, "avg_logprob": -0.1783422028146139, "compression_ratio": 1.6267281105990783, "no_speech_prob": 8.778930350672454e-05}, {"id": 183, "seek": 131284, "start": 1312.84, "end": 1321.9599999999998, "text": " measured as corpus congruence. Corpus congruence as a metric spans up almost all of NLP. Understanding", "tokens": [12690, 382, 1181, 31624, 8882, 84, 655, 13, 3925, 31624, 8882, 84, 655, 382, 257, 20678, 44086, 493, 1920, 439, 295, 426, 45196, 13, 36858], "temperature": 0.0, "avg_logprob": -0.0915725330511729, "compression_ratio": 1.7330316742081449, "no_speech_prob": 0.00010295005631633103}, {"id": 184, "seek": 131284, "start": 1321.9599999999998, "end": 1328.4399999999998, "text": " in brains and machines can be defined and measured as corpus congruence. Let's consider", "tokens": [294, 15442, 293, 8379, 393, 312, 7642, 293, 12690, 382, 1181, 31624, 8882, 84, 655, 13, 961, 311, 1949], "temperature": 0.0, "avg_logprob": -0.0915725330511729, "compression_ratio": 1.7330316742081449, "no_speech_prob": 0.00010295005631633103}, {"id": 185, "seek": 131284, "start": 1328.4399999999998, "end": 1336.04, "text": " this in the machine learning sense. If a machine is model-free, holistic, as all general understanders", "tokens": [341, 294, 264, 3479, 2539, 2020, 13, 759, 257, 3479, 307, 2316, 12, 10792, 11, 30334, 11, 382, 439, 2674, 1223, 433], "temperature": 0.0, "avg_logprob": -0.0915725330511729, "compression_ratio": 1.7330316742081449, "no_speech_prob": 0.00010295005631633103}, {"id": 186, "seek": 131284, "start": 1336.04, "end": 1341.9599999999998, "text": " have to be in order to not get trapped into a limited model, then all it ever knows comes", "tokens": [362, 281, 312, 294, 1668, 281, 406, 483, 14994, 666, 257, 5567, 2316, 11, 550, 439, 309, 1562, 3255, 1487], "temperature": 0.0, "avg_logprob": -0.0915725330511729, "compression_ratio": 1.7330316742081449, "no_speech_prob": 0.00010295005631633103}, {"id": 187, "seek": 134196, "start": 1341.96, "end": 1348.08, "text": " from the corpus it was trained on. And all it really can say is, this is more like my", "tokens": [490, 264, 1181, 31624, 309, 390, 8895, 322, 13, 400, 439, 309, 534, 393, 584, 307, 11, 341, 307, 544, 411, 452], "temperature": 0.0, "avg_logprob": -0.1423320770263672, "compression_ratio": 1.7175925925925926, "no_speech_prob": 0.0001269741333089769}, {"id": 188, "seek": 134196, "start": 1348.08, "end": 1355.48, "text": " corpus than that. Or, this is more like these documents in my corpus than those corpus congruence", "tokens": [1181, 31624, 813, 300, 13, 1610, 11, 341, 307, 544, 411, 613, 8512, 294, 452, 1181, 31624, 813, 729, 1181, 31624, 8882, 84, 655], "temperature": 0.0, "avg_logprob": -0.1423320770263672, "compression_ratio": 1.7175925925925926, "no_speech_prob": 0.0001269741333089769}, {"id": 189, "seek": 134196, "start": 1355.48, "end": 1364.16, "text": " as a metric spans up almost all of NLP. Because most of NLP is doxen in various guises. Given", "tokens": [382, 257, 20678, 44086, 493, 1920, 439, 295, 426, 45196, 13, 1436, 881, 295, 426, 45196, 307, 360, 87, 268, 294, 3683, 695, 3598, 13, 18600], "temperature": 0.0, "avg_logprob": -0.1423320770263672, "compression_ratio": 1.7175925925925926, "no_speech_prob": 0.0001269741333089769}, {"id": 190, "seek": 134196, "start": 1364.16, "end": 1370.88, "text": " two documents A and B in some corpus, a classifier can say that an unknown document, which we", "tokens": [732, 8512, 316, 293, 363, 294, 512, 1181, 31624, 11, 257, 1508, 9902, 393, 584, 300, 364, 9841, 4166, 11, 597, 321], "temperature": 0.0, "avg_logprob": -0.1423320770263672, "compression_ratio": 1.7175925925925926, "no_speech_prob": 0.0001269741333089769}, {"id": 191, "seek": 137088, "start": 1370.88, "end": 1379.3600000000001, "text": " can call U, is more like it than B given this capability we can build. Classification and", "tokens": [393, 818, 624, 11, 307, 544, 411, 309, 813, 363, 2212, 341, 13759, 321, 393, 1322, 13, 9471, 3774, 293], "temperature": 0.0, "avg_logprob": -0.21345590591430663, "compression_ratio": 1.3615384615384616, "no_speech_prob": 6.898294668644667e-05}, {"id": 192, "seek": 137088, "start": 1379.3600000000001, "end": 1389.64, "text": " clustering by using A, B, up to N as defining classes. Filtering by using A, wanted dox", "tokens": [596, 48673, 538, 1228, 316, 11, 363, 11, 493, 281, 426, 382, 17827, 5359, 13, 7905, 34200, 538, 1228, 316, 11, 1415, 360, 87], "temperature": 0.0, "avg_logprob": -0.21345590591430663, "compression_ratio": 1.3615384615384616, "no_speech_prob": 6.898294668644667e-05}, {"id": 193, "seek": 138964, "start": 1389.64, "end": 1401.44, "text": " and B, unwanted dox. Summoned analysis by using A, negative dox and B, positive dox.", "tokens": [293, 363, 11, 33745, 360, 87, 13, 8626, 3317, 292, 5215, 538, 1228, 316, 11, 3671, 360, 87, 293, 363, 11, 3353, 360, 87, 13], "temperature": 0.0, "avg_logprob": -0.17198725004453916, "compression_ratio": 1.424731182795699, "no_speech_prob": 0.00011741231719497591}, {"id": 194, "seek": 138964, "start": 1401.44, "end": 1409.44, "text": " Entity extraction by softly matching termed against lists of known entities. Doxen, find", "tokens": [3951, 507, 30197, 538, 30832, 14324, 1433, 292, 1970, 14511, 295, 2570, 16667, 13, 1144, 87, 268, 11, 915], "temperature": 0.0, "avg_logprob": -0.17198725004453916, "compression_ratio": 1.424731182795699, "no_speech_prob": 0.00011741231719497591}, {"id": 195, "seek": 138964, "start": 1409.44, "end": 1417.0800000000002, "text": " me more documents like this one. Reductionist and NLP uses all of these at the bag of words", "tokens": [385, 544, 8512, 411, 341, 472, 13, 4477, 27549, 468, 293, 426, 45196, 4960, 439, 295, 613, 412, 264, 3411, 295, 2283], "temperature": 0.0, "avg_logprob": -0.17198725004453916, "compression_ratio": 1.424731182795699, "no_speech_prob": 0.00011741231719497591}, {"id": 196, "seek": 141708, "start": 1417.08, "end": 1425.32, "text": " or word count levels for things like web search, span filtering, and clustering. Holistic", "tokens": [420, 1349, 1207, 4358, 337, 721, 411, 3670, 3164, 11, 16174, 30822, 11, 293, 596, 48673, 13, 11086, 3142], "temperature": 0.0, "avg_logprob": -0.14520139133228976, "compression_ratio": 1.5866666666666667, "no_speech_prob": 6.233097519725561e-05}, {"id": 197, "seek": 141708, "start": 1425.32, "end": 1432.6, "text": " NLU aims to do the same based on the meanings expressed in sentences and paragraphs. But", "tokens": [426, 43, 52, 24683, 281, 360, 264, 912, 2361, 322, 264, 28138, 12675, 294, 16579, 293, 48910, 13, 583], "temperature": 0.0, "avg_logprob": -0.14520139133228976, "compression_ratio": 1.5866666666666667, "no_speech_prob": 6.233097519725561e-05}, {"id": 198, "seek": 141708, "start": 1432.6, "end": 1439.72, "text": " semantic corpus congruence is still corpus congruence. Common sense now becomes, is the", "tokens": [47982, 1181, 31624, 8882, 84, 655, 307, 920, 1181, 31624, 8882, 84, 655, 13, 18235, 2020, 586, 3643, 11, 307, 264], "temperature": 0.0, "avg_logprob": -0.14520139133228976, "compression_ratio": 1.5866666666666667, "no_speech_prob": 6.233097519725561e-05}, {"id": 199, "seek": 141708, "start": 1439.72, "end": 1446.0, "text": " proposition before me congruent with my entire world model, as required by learning things", "tokens": [24830, 949, 385, 8882, 19226, 365, 452, 2302, 1002, 2316, 11, 382, 4739, 538, 2539, 721], "temperature": 0.0, "avg_logprob": -0.14520139133228976, "compression_ratio": 1.5866666666666667, "no_speech_prob": 6.233097519725561e-05}, {"id": 200, "seek": 144600, "start": 1446.0, "end": 1453.44, "text": " from my training corpus. If it is well known, then we can likely ignore it this time, and", "tokens": [490, 452, 3097, 1181, 31624, 13, 759, 309, 307, 731, 2570, 11, 550, 321, 393, 3700, 11200, 309, 341, 565, 11, 293], "temperature": 0.0, "avg_logprob": -0.11590514712863498, "compression_ratio": 1.6118721461187215, "no_speech_prob": 8.547770994482562e-05}, {"id": 201, "seek": 144600, "start": 1453.44, "end": 1458.96, "text": " if it is not, then the next question will be, is it close enough that it might be worth", "tokens": [498, 309, 307, 406, 11, 550, 264, 958, 1168, 486, 312, 11, 307, 309, 1998, 1547, 300, 309, 1062, 312, 3163], "temperature": 0.0, "avg_logprob": -0.11590514712863498, "compression_ratio": 1.6118721461187215, "no_speech_prob": 8.547770994482562e-05}, {"id": 202, "seek": 144600, "start": 1458.96, "end": 1465.32, "text": " while extending the world model with this information? If the answer is no, then the", "tokens": [1339, 24360, 264, 1002, 2316, 365, 341, 1589, 30, 759, 264, 1867, 307, 572, 11, 550, 264], "temperature": 0.0, "avg_logprob": -0.11590514712863498, "compression_ratio": 1.6118721461187215, "no_speech_prob": 8.547770994482562e-05}, {"id": 203, "seek": 144600, "start": 1465.32, "end": 1472.16, "text": " input is by its definition nonsense. Otherwise it is either a new fact or a lie, but since", "tokens": [4846, 307, 538, 1080, 7123, 14925, 13, 10328, 309, 307, 2139, 257, 777, 1186, 420, 257, 4544, 11, 457, 1670], "temperature": 0.0, "avg_logprob": -0.11590514712863498, "compression_ratio": 1.6118721461187215, "no_speech_prob": 8.547770994482562e-05}, {"id": 204, "seek": 147216, "start": 1472.16, "end": 1479.76, "text": " we cannot tell, we have to accept it, possibly with a note that this is fresh, untested knowledge", "tokens": [321, 2644, 980, 11, 321, 362, 281, 3241, 309, 11, 6264, 365, 257, 3637, 300, 341, 307, 4451, 11, 1701, 21885, 3601], "temperature": 0.0, "avg_logprob": -0.0928952337681562, "compression_ratio": 1.587719298245614, "no_speech_prob": 7.055315654724836e-05}, {"id": 205, "seek": 147216, "start": 1479.76, "end": 1487.3600000000001, "text": " that may turn out to be irrelevant, false, counterproductive, or noise. Next we can note", "tokens": [300, 815, 1261, 484, 281, 312, 28682, 11, 7908, 11, 5682, 14314, 20221, 11, 420, 5658, 13, 3087, 321, 393, 3637], "temperature": 0.0, "avg_logprob": -0.0928952337681562, "compression_ratio": 1.587719298245614, "no_speech_prob": 7.055315654724836e-05}, {"id": 206, "seek": 147216, "start": 1487.3600000000001, "end": 1494.0400000000002, "text": " that it doesn't matter whether documents are text or images, or input from a point cloud", "tokens": [300, 309, 1177, 380, 1871, 1968, 8512, 366, 2487, 420, 5267, 11, 420, 4846, 490, 257, 935, 4588], "temperature": 0.0, "avg_logprob": -0.0928952337681562, "compression_ratio": 1.587719298245614, "no_speech_prob": 7.055315654724836e-05}, {"id": 207, "seek": 147216, "start": 1494.0400000000002, "end": 1500.28, "text": " of sensors for robots or autonomous vehicle sensors. And finally we can note that this", "tokens": [295, 14840, 337, 14733, 420, 23797, 5864, 14840, 13, 400, 2721, 321, 393, 3637, 300, 341], "temperature": 0.0, "avg_logprob": -0.0928952337681562, "compression_ratio": 1.587719298245614, "no_speech_prob": 7.055315654724836e-05}, {"id": 208, "seek": 150028, "start": 1500.28, "end": 1507.6399999999999, "text": " definition also holds for humans if we take our corpus to be everything we've experienced", "tokens": [7123, 611, 9190, 337, 6255, 498, 321, 747, 527, 1181, 31624, 281, 312, 1203, 321, 600, 6751], "temperature": 0.0, "avg_logprob": -0.3742421360339149, "compression_ratio": 1.3743016759776536, "no_speech_prob": 0.00014233450929168612}, {"id": 209, "seek": 150028, "start": 1507.6399999999999, "end": 1509.6399999999999, "text": " since birth.", "tokens": [1670, 3965, 13], "temperature": 0.0, "avg_logprob": -0.3742421360339149, "compression_ratio": 1.3743016759776536, "no_speech_prob": 0.00014233450929168612}, {"id": 210, "seek": 150028, "start": 1509.6399999999999, "end": 1512.6399999999999, "text": " Monika's Little Pills", "tokens": [4713, 5439, 311, 8022, 430, 2565], "temperature": 0.0, "avg_logprob": -0.3742421360339149, "compression_ratio": 1.3743016759776536, "no_speech_prob": 0.00014233450929168612}, {"id": 211, "seek": 150028, "start": 1512.6399999999999, "end": 1514.16, "text": " Chapter 1", "tokens": [18874, 502], "temperature": 0.0, "avg_logprob": -0.3742421360339149, "compression_ratio": 1.3743016759776536, "no_speech_prob": 0.00014233450929168612}, {"id": 212, "seek": 150028, "start": 1514.16, "end": 1517.76, "text": " Why I Works", "tokens": [1545, 286, 27914], "temperature": 0.0, "avg_logprob": -0.3742421360339149, "compression_ratio": 1.3743016759776536, "no_speech_prob": 0.00014233450929168612}, {"id": 213, "seek": 150028, "start": 1517.76, "end": 1524.76, "text": " Intelligence equals understanding plus reasoning. Interest in artificial intelligence is exploding,", "tokens": [27274, 6915, 3701, 1804, 21577, 13, 5751, 377, 294, 11677, 7599, 307, 35175, 11], "temperature": 0.0, "avg_logprob": -0.3742421360339149, "compression_ratio": 1.3743016759776536, "no_speech_prob": 0.00014233450929168612}, {"id": 214, "seek": 152476, "start": 1524.76, "end": 1531.76, "text": " and for good reasons, computers and cars, phone apps, and on the web can do amazing", "tokens": [293, 337, 665, 4112, 11, 10807, 293, 5163, 11, 2593, 7733, 11, 293, 322, 264, 3670, 393, 360, 2243], "temperature": 0.0, "avg_logprob": -0.12810454717496547, "compression_ratio": 1.5240174672489082, "no_speech_prob": 0.00019440203323028982}, {"id": 215, "seek": 152476, "start": 1531.76, "end": 1539.28, "text": " things that we simply could not do before 2012. What's going on? This is an attempt", "tokens": [721, 300, 321, 2935, 727, 406, 360, 949, 9125, 13, 708, 311, 516, 322, 30, 639, 307, 364, 5217], "temperature": 0.0, "avg_logprob": -0.12810454717496547, "compression_ratio": 1.5240174672489082, "no_speech_prob": 0.00019440203323028982}, {"id": 216, "seek": 152476, "start": 1539.28, "end": 1545.84, "text": " to explain the current state of AI to a general audience without using mathematics, computer", "tokens": [281, 2903, 264, 2190, 1785, 295, 7318, 281, 257, 2674, 4034, 1553, 1228, 18666, 11, 3820], "temperature": 0.0, "avg_logprob": -0.12810454717496547, "compression_ratio": 1.5240174672489082, "no_speech_prob": 0.00019440203323028982}, {"id": 217, "seek": 152476, "start": 1545.84, "end": 1553.48, "text": " science, or neuroscience, discussions at these levels with focus on how AI works. Here I", "tokens": [3497, 11, 420, 42762, 11, 11088, 412, 613, 4358, 365, 1879, 322, 577, 7318, 1985, 13, 1692, 286], "temperature": 0.0, "avg_logprob": -0.12810454717496547, "compression_ratio": 1.5240174672489082, "no_speech_prob": 0.00019440203323028982}, {"id": 218, "seek": 155348, "start": 1553.48, "end": 1561.04, "text": " will discuss this at the level of epistemology and will try to explain why it works. Epistemology", "tokens": [486, 2248, 341, 412, 264, 1496, 295, 2388, 43958, 1793, 293, 486, 853, 281, 2903, 983, 309, 1985, 13, 9970, 43958, 1793], "temperature": 0.0, "avg_logprob": -0.10274054663521903, "compression_ratio": 1.5078534031413613, "no_speech_prob": 7.452037971233949e-05}, {"id": 219, "seek": 155348, "start": 1561.04, "end": 1568.44, "text": " sounds scary, but it really isn't. It's mostly scary because it is unknown, it is not taught", "tokens": [3263, 6958, 11, 457, 309, 534, 1943, 380, 13, 467, 311, 5240, 6958, 570, 309, 307, 9841, 11, 309, 307, 406, 5928], "temperature": 0.0, "avg_logprob": -0.10274054663521903, "compression_ratio": 1.5078534031413613, "no_speech_prob": 7.452037971233949e-05}, {"id": 220, "seek": 155348, "start": 1568.44, "end": 1575.84, "text": " in schools anymore, which is a problem, because we now desperately need this branch of philosophy", "tokens": [294, 4656, 3602, 11, 597, 307, 257, 1154, 11, 570, 321, 586, 23726, 643, 341, 9819, 295, 10675], "temperature": 0.0, "avg_logprob": -0.10274054663521903, "compression_ratio": 1.5078534031413613, "no_speech_prob": 7.452037971233949e-05}, {"id": 221, "seek": 157584, "start": 1575.84, "end": 1584.1999999999998, "text": " to guide our AI development. Epistemology discusses things like reasoning, understanding, learning,", "tokens": [281, 5934, 527, 7318, 3250, 13, 9970, 43958, 1793, 2248, 279, 721, 411, 21577, 11, 3701, 11, 2539, 11], "temperature": 0.0, "avg_logprob": -0.16224052906036376, "compression_ratio": 1.5527426160337552, "no_speech_prob": 4.4786698708776385e-05}, {"id": 222, "seek": 157584, "start": 1584.1999999999998, "end": 1591.9599999999998, "text": " novelty, problem solving in the abstract, how to create models of the world, etc. These", "tokens": [44805, 11, 1154, 12606, 294, 264, 12649, 11, 577, 281, 1884, 5245, 295, 264, 1002, 11, 5183, 13, 1981], "temperature": 0.0, "avg_logprob": -0.16224052906036376, "compression_ratio": 1.5527426160337552, "no_speech_prob": 4.4786698708776385e-05}, {"id": 223, "seek": 157584, "start": 1591.9599999999998, "end": 1598.08, "text": " are all concepts one would think would be useful when working with artificial intelligences,", "tokens": [366, 439, 10392, 472, 576, 519, 576, 312, 4420, 562, 1364, 365, 11677, 5613, 2667, 11], "temperature": 0.0, "avg_logprob": -0.16224052906036376, "compression_ratio": 1.5527426160337552, "no_speech_prob": 4.4786698708776385e-05}, {"id": 224, "seek": 157584, "start": 1598.08, "end": 1604.6, "text": " but most practitioners enter the field of AI without any exposure to epistemology which", "tokens": [457, 881, 25742, 3242, 264, 2519, 295, 7318, 1553, 604, 10420, 281, 2388, 43958, 1793, 597], "temperature": 0.0, "avg_logprob": -0.16224052906036376, "compression_ratio": 1.5527426160337552, "no_speech_prob": 4.4786698708776385e-05}, {"id": 225, "seek": 160460, "start": 1604.6, "end": 1611.32, "text": " makes their work more mysterious and frustrating than it has to be. I think of it epistemology", "tokens": [1669, 641, 589, 544, 13831, 293, 16522, 813, 309, 575, 281, 312, 13, 286, 519, 295, 309, 2388, 43958, 1793], "temperature": 0.0, "avg_logprob": -0.10036144023988305, "compression_ratio": 1.6441441441441442, "no_speech_prob": 7.900322088971734e-05}, {"id": 226, "seek": 160460, "start": 1611.32, "end": 1617.6399999999999, "text": " as the general base for everything related to knowledge and problem solving. Science forms", "tokens": [382, 264, 2674, 3096, 337, 1203, 4077, 281, 3601, 293, 1154, 12606, 13, 8976, 6422], "temperature": 0.0, "avg_logprob": -0.10036144023988305, "compression_ratio": 1.6441441441441442, "no_speech_prob": 7.900322088971734e-05}, {"id": 227, "seek": 160460, "start": 1617.6399999999999, "end": 1623.32, "text": " a small special case subset domain where we solve well-formed problems of the kind that", "tokens": [257, 1359, 2121, 1389, 25993, 9274, 689, 321, 5039, 731, 12, 22892, 2740, 295, 264, 733, 300], "temperature": 0.0, "avg_logprob": -0.10036144023988305, "compression_ratio": 1.6441441441441442, "no_speech_prob": 7.900322088971734e-05}, {"id": 228, "seek": 160460, "start": 1623.32, "end": 1630.04, "text": " science is best at. In the epistemology outside of science we are free to productively also", "tokens": [3497, 307, 1151, 412, 13, 682, 264, 2388, 43958, 1793, 2380, 295, 3497, 321, 366, 1737, 281, 1674, 3413, 611], "temperature": 0.0, "avg_logprob": -0.10036144023988305, "compression_ratio": 1.6441441441441442, "no_speech_prob": 7.900322088971734e-05}, {"id": 229, "seek": 163004, "start": 1630.04, "end": 1636.3999999999999, "text": " discuss pre-scientific problem solving strategies, which is what brains are using most of the", "tokens": [2248, 659, 12, 82, 5412, 1089, 1154, 12606, 9029, 11, 597, 307, 437, 15442, 366, 1228, 881, 295, 264], "temperature": 0.0, "avg_logprob": -0.19985153675079345, "compression_ratio": 1.6288209606986899, "no_speech_prob": 0.00010400483733974397}, {"id": 230, "seek": 163004, "start": 1636.3999999999999, "end": 1644.84, "text": " time. More later, intelligence equals understanding plus reasoning. In his book, Thinking Fast", "tokens": [565, 13, 5048, 1780, 11, 7599, 6915, 3701, 1804, 21577, 13, 682, 702, 1446, 11, 24460, 15968], "temperature": 0.0, "avg_logprob": -0.19985153675079345, "compression_ratio": 1.6288209606986899, "no_speech_prob": 0.00010400483733974397}, {"id": 231, "seek": 163004, "start": 1644.84, "end": 1651.8, "text": " and Slow, Daniel Kahneman discusses the idea that human minds use two different and complementary", "tokens": [293, 17703, 11, 8033, 591, 12140, 15023, 2248, 279, 264, 1558, 300, 1952, 9634, 764, 732, 819, 293, 40705], "temperature": 0.0, "avg_logprob": -0.19985153675079345, "compression_ratio": 1.6288209606986899, "no_speech_prob": 0.00010400483733974397}, {"id": 232, "seek": 163004, "start": 1651.8, "end": 1658.44, "text": " processes, two different modes of thinking, which we call understanding and reasoning.", "tokens": [7555, 11, 732, 819, 14068, 295, 1953, 11, 597, 321, 818, 3701, 293, 21577, 13], "temperature": 0.0, "avg_logprob": -0.19985153675079345, "compression_ratio": 1.6288209606986899, "no_speech_prob": 0.00010400483733974397}, {"id": 233, "seek": 165844, "start": 1658.44, "end": 1664.6000000000001, "text": " The idea has been discussed for decades and has been verified using psychological studies", "tokens": [440, 1558, 575, 668, 7152, 337, 7878, 293, 575, 668, 31197, 1228, 14346, 5313], "temperature": 0.0, "avg_logprob": -0.1248172124226888, "compression_ratio": 1.6761904761904762, "no_speech_prob": 3.0228393370634876e-05}, {"id": 234, "seek": 165844, "start": 1664.6000000000001, "end": 1672.2, "text": " and by neuroscience. Subconscious intuitive understanding is the full name of the fast", "tokens": [293, 538, 42762, 13, 8511, 19877, 21769, 3701, 307, 264, 1577, 1315, 295, 264, 2370], "temperature": 0.0, "avg_logprob": -0.1248172124226888, "compression_ratio": 1.6761904761904762, "no_speech_prob": 3.0228393370634876e-05}, {"id": 235, "seek": 165844, "start": 1672.2, "end": 1679.28, "text": " thinking or system one thinking. It is fast because the brain can perform many parts of", "tokens": [1953, 420, 1185, 472, 1953, 13, 467, 307, 2370, 570, 264, 3567, 393, 2042, 867, 3166, 295], "temperature": 0.0, "avg_logprob": -0.1248172124226888, "compression_ratio": 1.6761904761904762, "no_speech_prob": 3.0228393370634876e-05}, {"id": 236, "seek": 165844, "start": 1679.28, "end": 1686.24, "text": " this task in parallel. The brain spends a lot of effort on this task. Conscious logical", "tokens": [341, 5633, 294, 8952, 13, 440, 3567, 25620, 257, 688, 295, 4630, 322, 341, 5633, 13, 6923, 4139, 14978], "temperature": 0.0, "avg_logprob": -0.1248172124226888, "compression_ratio": 1.6761904761904762, "no_speech_prob": 3.0228393370634876e-05}, {"id": 237, "seek": 168624, "start": 1686.24, "end": 1693.84, "text": " reasoning is the full name of slow thinking or system two thinking. To many people's", "tokens": [21577, 307, 264, 1577, 1315, 295, 2964, 1953, 420, 1185, 732, 1953, 13, 1407, 867, 561, 311], "temperature": 0.0, "avg_logprob": -0.12800738323165708, "compression_ratio": 1.5892857142857142, "no_speech_prob": 3.853904490824789e-05}, {"id": 238, "seek": 168624, "start": 1693.84, "end": 1701.44, "text": " surprise, this is very rarely used in practice. By soundbite for this is, you can make breakfast", "tokens": [6365, 11, 341, 307, 588, 13752, 1143, 294, 3124, 13, 3146, 1626, 65, 642, 337, 341, 307, 11, 291, 393, 652, 8201], "temperature": 0.0, "avg_logprob": -0.12800738323165708, "compression_ratio": 1.5892857142857142, "no_speech_prob": 3.853904490824789e-05}, {"id": 239, "seek": 168624, "start": 1701.44, "end": 1707.84, "text": " without reasoning. Almost everything we do on a daily basis in our rich mundane reality", "tokens": [1553, 21577, 13, 12627, 1203, 321, 360, 322, 257, 5212, 5143, 294, 527, 4593, 43497, 4103], "temperature": 0.0, "avg_logprob": -0.12800738323165708, "compression_ratio": 1.5892857142857142, "no_speech_prob": 3.853904490824789e-05}, {"id": 240, "seek": 168624, "start": 1707.84, "end": 1713.8, "text": " is done without a need to reason about it. We just repeat whatever worked last time we", "tokens": [307, 1096, 1553, 257, 643, 281, 1778, 466, 309, 13, 492, 445, 7149, 2035, 2732, 1036, 565, 321], "temperature": 0.0, "avg_logprob": -0.12800738323165708, "compression_ratio": 1.5892857142857142, "no_speech_prob": 3.853904490824789e-05}, {"id": 241, "seek": 171380, "start": 1713.8, "end": 1721.96, "text": " performed this task. Real experience driven. Intuitive means that the system can very quickly", "tokens": [10332, 341, 5633, 13, 8467, 1752, 9555, 13, 5681, 48314, 1355, 300, 264, 1185, 393, 588, 2661], "temperature": 0.0, "avg_logprob": -0.13753076142902615, "compression_ratio": 1.6327433628318584, "no_speech_prob": 5.4201580496737733e-05}, {"id": 242, "seek": 171380, "start": 1721.96, "end": 1728.3999999999999, "text": " provide solutions to very complex problems but those solutions may not be correct every", "tokens": [2893, 6547, 281, 588, 3997, 2740, 457, 729, 6547, 815, 406, 312, 3006, 633], "temperature": 0.0, "avg_logprob": -0.13753076142902615, "compression_ratio": 1.6327433628318584, "no_speech_prob": 5.4201580496737733e-05}, {"id": 243, "seek": 171380, "start": 1728.3999999999999, "end": 1734.96, "text": " time. Logical means that answers are always correct as long as input data is correct and", "tokens": [565, 13, 10824, 804, 1355, 300, 6338, 366, 1009, 3006, 382, 938, 382, 4846, 1412, 307, 3006, 293], "temperature": 0.0, "avg_logprob": -0.13753076142902615, "compression_ratio": 1.6327433628318584, "no_speech_prob": 5.4201580496737733e-05}, {"id": 244, "seek": 171380, "start": 1734.96, "end": 1742.24, "text": " sufficient, which is not true in our rich mundane reality. It can only be true in a mathematically", "tokens": [11563, 11, 597, 307, 406, 2074, 294, 527, 4593, 43497, 4103, 13, 467, 393, 787, 312, 2074, 294, 257, 44003], "temperature": 0.0, "avg_logprob": -0.13753076142902615, "compression_ratio": 1.6327433628318584, "no_speech_prob": 5.4201580496737733e-05}, {"id": 245, "seek": 174224, "start": 1742.24, "end": 1751.1200000000001, "text": " pure model space. If you like logic, you must also like models. Subconscious means we have", "tokens": [6075, 2316, 1901, 13, 759, 291, 411, 9952, 11, 291, 1633, 611, 411, 5245, 13, 8511, 19877, 1355, 321, 362], "temperature": 0.0, "avg_logprob": -0.1057857937282986, "compression_ratio": 1.6634615384615385, "no_speech_prob": 7.448092947015539e-05}, {"id": 246, "seek": 174224, "start": 1751.1200000000001, "end": 1758.02, "text": " no conscious, introspective access to these processes. You are reading this sentence", "tokens": [572, 6648, 11, 560, 28713, 488, 2105, 281, 613, 7555, 13, 509, 366, 3760, 341, 8174], "temperature": 0.0, "avg_logprob": -0.1057857937282986, "compression_ratio": 1.6634615384615385, "no_speech_prob": 7.448092947015539e-05}, {"id": 247, "seek": 174224, "start": 1758.02, "end": 1764.32, "text": " and you understand it fully but you cannot explain to anyone, including yourself, how", "tokens": [293, 291, 1223, 309, 4498, 457, 291, 2644, 2903, 281, 2878, 11, 3009, 1803, 11, 577], "temperature": 0.0, "avg_logprob": -0.1057857937282986, "compression_ratio": 1.6634615384615385, "no_speech_prob": 7.448092947015539e-05}, {"id": 248, "seek": 174224, "start": 1764.32, "end": 1770.84, "text": " or why you understand it. Conscious means we are aware of the thought, we can access", "tokens": [420, 983, 291, 1223, 309, 13, 6923, 4139, 1355, 321, 366, 3650, 295, 264, 1194, 11, 321, 393, 2105], "temperature": 0.0, "avg_logprob": -0.1057857937282986, "compression_ratio": 1.6634615384615385, "no_speech_prob": 7.448092947015539e-05}, {"id": 249, "seek": 177084, "start": 1770.84, "end": 1777.9599999999998, "text": " it through introspection and we may find reasons to why we believe a certain idea. Expensive", "tokens": [309, 807, 560, 2635, 19997, 293, 321, 815, 915, 4112, 281, 983, 321, 1697, 257, 1629, 1558, 13, 21391, 2953], "temperature": 0.0, "avg_logprob": -0.15628545078230494, "compression_ratio": 1.5281385281385282, "no_speech_prob": 5.302133649820462e-05}, {"id": 250, "seek": 177084, "start": 1777.9599999999998, "end": 1784.1599999999999, "text": " is on the list because brains spend most of their effort on this understanding part. We", "tokens": [307, 322, 264, 1329, 570, 15442, 3496, 881, 295, 641, 4630, 322, 341, 3701, 644, 13, 492], "temperature": 0.0, "avg_logprob": -0.15628545078230494, "compression_ratio": 1.5281385281385282, "no_speech_prob": 5.302133649820462e-05}, {"id": 251, "seek": 177084, "start": 1784.1599999999999, "end": 1791.8, "text": " really shouldn't be surprised that AI now requires very powerful computers. More later.", "tokens": [534, 4659, 380, 312, 6100, 300, 7318, 586, 7029, 588, 4005, 10807, 13, 5048, 1780, 13], "temperature": 0.0, "avg_logprob": -0.15628545078230494, "compression_ratio": 1.5281385281385282, "no_speech_prob": 5.302133649820462e-05}, {"id": 252, "seek": 177084, "start": 1791.8, "end": 1798.24, "text": " In contrast, reasoning is efficient. It is most useful when you are stuck in a novel", "tokens": [682, 8712, 11, 21577, 307, 7148, 13, 467, 307, 881, 4420, 562, 291, 366, 5541, 294, 257, 7613], "temperature": 0.0, "avg_logprob": -0.15628545078230494, "compression_ratio": 1.5281385281385282, "no_speech_prob": 5.302133649820462e-05}, {"id": 253, "seek": 179824, "start": 1798.24, "end": 1805.48, "text": " situation or experience and understanding doesn't help you. Or perhaps you need to plan ahead", "tokens": [2590, 420, 1752, 293, 3701, 1177, 380, 854, 291, 13, 1610, 4317, 291, 643, 281, 1393, 2286], "temperature": 0.0, "avg_logprob": -0.15431605445014107, "compression_ratio": 1.5268817204301075, "no_speech_prob": 0.00010758668940979987}, {"id": 254, "seek": 179824, "start": 1805.48, "end": 1812.6, "text": " or need to find reasons for why something happened after the fact. It is used at a formal level", "tokens": [420, 643, 281, 915, 4112, 337, 983, 746, 2011, 934, 264, 1186, 13, 467, 307, 1143, 412, 257, 9860, 1496], "temperature": 0.0, "avg_logprob": -0.15431605445014107, "compression_ratio": 1.5268817204301075, "no_speech_prob": 0.00010758668940979987}, {"id": 255, "seek": 179824, "start": 1812.6, "end": 1820.96, "text": " in the sciences. Reasoning is important but just rarely needed or used. Finally, understanding", "tokens": [294, 264, 17677, 13, 39693, 278, 307, 1021, 457, 445, 13752, 2978, 420, 1143, 13, 6288, 11, 3701], "temperature": 0.0, "avg_logprob": -0.15431605445014107, "compression_ratio": 1.5268817204301075, "no_speech_prob": 0.00010758668940979987}, {"id": 256, "seek": 182096, "start": 1820.96, "end": 1828.48, "text": " is model-free and reasoning is model-based. This is likely the most important distinction", "tokens": [307, 2316, 12, 10792, 293, 21577, 307, 2316, 12, 6032, 13, 639, 307, 3700, 264, 881, 1021, 16844], "temperature": 0.0, "avg_logprob": -0.09871673583984375, "compression_ratio": 1.631336405529954, "no_speech_prob": 0.00010110117000294849}, {"id": 257, "seek": 182096, "start": 1828.48, "end": 1833.88, "text": " to people who are implementing intelligent systems since it provides a way to keep the", "tokens": [281, 561, 567, 366, 18114, 13232, 3652, 1670, 309, 6417, 257, 636, 281, 1066, 264], "temperature": 0.0, "avg_logprob": -0.09871673583984375, "compression_ratio": 1.631336405529954, "no_speech_prob": 0.00010110117000294849}, {"id": 258, "seek": 182096, "start": 1833.88, "end": 1840.08, "text": " implementation on the correct path when the going gets rough. We cannot discuss these", "tokens": [11420, 322, 264, 3006, 3100, 562, 264, 516, 2170, 5903, 13, 492, 2644, 2248, 613], "temperature": 0.0, "avg_logprob": -0.09871673583984375, "compression_ratio": 1.631336405529954, "no_speech_prob": 0.00010110117000294849}, {"id": 259, "seek": 182096, "start": 1840.08, "end": 1847.44, "text": " issues quite yet but if you are curious you can watch the videos at Vimeo.com which discuss", "tokens": [2663, 1596, 1939, 457, 498, 291, 366, 6369, 291, 393, 1159, 264, 2145, 412, 691, 1312, 78, 13, 1112, 597, 2248], "temperature": 0.0, "avg_logprob": -0.09871673583984375, "compression_ratio": 1.631336405529954, "no_speech_prob": 0.00010110117000294849}, {"id": 260, "seek": 184744, "start": 1847.44, "end": 1853.92, "text": " this distinction at length. Think of the appearance in this table as a kind of foreshadowing.", "tokens": [341, 16844, 412, 4641, 13, 6557, 295, 264, 8967, 294, 341, 3199, 382, 257, 733, 295, 2091, 2716, 345, 9637, 13], "temperature": 0.0, "avg_logprob": -0.1366082373119536, "compression_ratio": 1.605263157894737, "no_speech_prob": 0.00010573417966952547}, {"id": 261, "seek": 184744, "start": 1853.92, "end": 1860.68, "text": " All of this groundwork allows me to state the main point of this section. We have known", "tokens": [1057, 295, 341, 2727, 1902, 4045, 385, 281, 1785, 264, 2135, 935, 295, 341, 3541, 13, 492, 362, 2570], "temperature": 0.0, "avg_logprob": -0.1366082373119536, "compression_ratio": 1.605263157894737, "no_speech_prob": 0.00010573417966952547}, {"id": 262, "seek": 184744, "start": 1860.68, "end": 1867.88, "text": " for a long time that brains use these two modes. But the AI research community has been spending", "tokens": [337, 257, 938, 565, 300, 15442, 764, 613, 732, 14068, 13, 583, 264, 7318, 2132, 1768, 575, 668, 6434], "temperature": 0.0, "avg_logprob": -0.1366082373119536, "compression_ratio": 1.605263157894737, "no_speech_prob": 0.00010573417966952547}, {"id": 263, "seek": 184744, "start": 1867.88, "end": 1873.56, "text": " over much effort on the reasoning part and has been ignoring the understanding part for", "tokens": [670, 709, 4630, 322, 264, 21577, 644, 293, 575, 668, 26258, 264, 3701, 644, 337], "temperature": 0.0, "avg_logprob": -0.1366082373119536, "compression_ratio": 1.605263157894737, "no_speech_prob": 0.00010573417966952547}, {"id": 264, "seek": 187356, "start": 1873.56, "end": 1881.3999999999999, "text": " 60 years. We had several good reasons for this. Until quite recently, our machines were too", "tokens": [4060, 924, 13, 492, 632, 2940, 665, 4112, 337, 341, 13, 9088, 1596, 3938, 11, 527, 8379, 645, 886], "temperature": 0.0, "avg_logprob": -0.11689110545368937, "compression_ratio": 1.4811715481171548, "no_speech_prob": 9.275216871174052e-05}, {"id": 265, "seek": 187356, "start": 1881.3999999999999, "end": 1888.32, "text": " small to run any useful sized neural network. And also, we didn't have a clue about how", "tokens": [1359, 281, 1190, 604, 4420, 20004, 18161, 3209, 13, 400, 611, 11, 321, 994, 380, 362, 257, 13602, 466, 577], "temperature": 0.0, "avg_logprob": -0.11689110545368937, "compression_ratio": 1.4811715481171548, "no_speech_prob": 9.275216871174052e-05}, {"id": 266, "seek": 187356, "start": 1888.32, "end": 1895.44, "text": " to implement this understanding. But that is exactly what changed in 2012 when a group", "tokens": [281, 4445, 341, 3701, 13, 583, 300, 307, 2293, 437, 3105, 294, 9125, 562, 257, 1594], "temperature": 0.0, "avg_logprob": -0.11689110545368937, "compression_ratio": 1.4811715481171548, "no_speech_prob": 9.275216871174052e-05}, {"id": 267, "seek": 187356, "start": 1895.44, "end": 1900.96, "text": " of AI researchers from Toronto effectively demonstrated that deep neural networks could", "tokens": [295, 7318, 10309, 490, 14140, 8659, 18772, 300, 2452, 18161, 9590, 727], "temperature": 0.0, "avg_logprob": -0.11689110545368937, "compression_ratio": 1.4811715481171548, "no_speech_prob": 9.275216871174052e-05}, {"id": 268, "seek": 190096, "start": 1900.96, "end": 1907.32, "text": " provide a simple kind of shallow and hollow proto-understanding. Well, they didn't call", "tokens": [2893, 257, 2199, 733, 295, 20488, 293, 23972, 47896, 12, 6617, 8618, 13, 1042, 11, 436, 994, 380, 818], "temperature": 0.0, "avg_logprob": -0.1366394603953642, "compression_ratio": 1.5955555555555556, "no_speech_prob": 8.413616160396487e-05}, {"id": 269, "seek": 190096, "start": 1907.32, "end": 1914.48, "text": " it that, but I do. I will look just a little into the future and overstate this just a", "tokens": [309, 300, 11, 457, 286, 360, 13, 286, 486, 574, 445, 257, 707, 666, 264, 2027, 293, 670, 15406, 341, 445, 257], "temperature": 0.0, "avg_logprob": -0.1366394603953642, "compression_ratio": 1.5955555555555556, "no_speech_prob": 8.413616160396487e-05}, {"id": 270, "seek": 190096, "start": 1914.48, "end": 1922.44, "text": " little in order to make it more memorable. Deep neural networks can provide understanding.", "tokens": [707, 294, 1668, 281, 652, 309, 544, 20723, 13, 14895, 18161, 9590, 393, 2893, 3701, 13], "temperature": 0.0, "avg_logprob": -0.1366394603953642, "compression_ratio": 1.5955555555555556, "no_speech_prob": 8.413616160396487e-05}, {"id": 271, "seek": 190096, "start": 1922.44, "end": 1928.48, "text": " This new phase of AI took decades to develop, but it would never have happened without people", "tokens": [639, 777, 5574, 295, 7318, 1890, 7878, 281, 1499, 11, 457, 309, 576, 1128, 362, 2011, 1553, 561], "temperature": 0.0, "avg_logprob": -0.1366394603953642, "compression_ratio": 1.5955555555555556, "no_speech_prob": 8.413616160396487e-05}, {"id": 272, "seek": 192848, "start": 1928.48, "end": 1935.6, "text": " like the group led by Jeffrey Hinton at the University of Toronto, who spent 34 plus years", "tokens": [411, 264, 1594, 4684, 538, 28721, 389, 12442, 412, 264, 3535, 295, 14140, 11, 567, 4418, 12790, 1804, 924], "temperature": 0.0, "avg_logprob": -0.12408680386013454, "compression_ratio": 1.4798387096774193, "no_speech_prob": 4.843583155889064e-05}, {"id": 273, "seek": 192848, "start": 1935.6, "end": 1942.8, "text": " to develop the deep neural network technology we now call, deep learning. A number of breakthroughs", "tokens": [281, 1499, 264, 2452, 18161, 3209, 2899, 321, 586, 818, 11, 2452, 2539, 13, 316, 1230, 295, 22397, 82], "temperature": 0.0, "avg_logprob": -0.12408680386013454, "compression_ratio": 1.4798387096774193, "no_speech_prob": 4.843583155889064e-05}, {"id": 274, "seek": 192848, "start": 1942.8, "end": 1951.16, "text": " from 1997 to 2006 led to a number of successful demonstrations, including first prizes in", "tokens": [490, 22383, 281, 14062, 4684, 281, 257, 1230, 295, 4406, 34714, 11, 3009, 700, 27350, 294], "temperature": 0.0, "avg_logprob": -0.12408680386013454, "compression_ratio": 1.4798387096774193, "no_speech_prob": 4.843583155889064e-05}, {"id": 275, "seek": 192848, "start": 1951.16, "end": 1958.16, "text": " AI competitions in 2012. And we therefore count this year as the birth year of machine", "tokens": [7318, 26185, 294, 9125, 13, 400, 321, 4412, 1207, 341, 1064, 382, 264, 3965, 1064, 295, 3479], "temperature": 0.0, "avg_logprob": -0.12408680386013454, "compression_ratio": 1.4798387096774193, "no_speech_prob": 4.843583155889064e-05}, {"id": 276, "seek": 195816, "start": 1958.16, "end": 1965.16, "text": " understanding. To an outsider, it may look like an older program or phone app might be", "tokens": [3701, 13, 1407, 364, 40484, 11, 309, 815, 574, 411, 364, 4906, 1461, 420, 2593, 724, 1062, 312], "temperature": 0.0, "avg_logprob": -0.1550267709268106, "compression_ratio": 1.7889447236180904, "no_speech_prob": 9.684959513833746e-05}, {"id": 277, "seek": 195816, "start": 1965.16, "end": 1971.2, "text": " understanding whatever the app is doing, but that understanding really only happened in", "tokens": [3701, 2035, 264, 724, 307, 884, 11, 457, 300, 3701, 534, 787, 2011, 294], "temperature": 0.0, "avg_logprob": -0.1550267709268106, "compression_ratio": 1.7889447236180904, "no_speech_prob": 9.684959513833746e-05}, {"id": 278, "seek": 195816, "start": 1971.2, "end": 1977.48, "text": " the mind of the programmer creating the app. The programmer first simplified the problem", "tokens": [264, 1575, 295, 264, 32116, 4084, 264, 724, 13, 440, 32116, 700, 26335, 264, 1154], "temperature": 0.0, "avg_logprob": -0.1550267709268106, "compression_ratio": 1.7889447236180904, "no_speech_prob": 9.684959513833746e-05}, {"id": 279, "seek": 195816, "start": 1977.48, "end": 1984.64, "text": " in their own head by discarding a lot of irrelevant detail using programmer's understanding.", "tokens": [294, 641, 1065, 1378, 538, 31597, 278, 257, 688, 295, 28682, 2607, 1228, 32116, 311, 3701, 13], "temperature": 0.0, "avg_logprob": -0.1550267709268106, "compression_ratio": 1.7889447236180904, "no_speech_prob": 9.684959513833746e-05}, {"id": 280, "seek": 198464, "start": 1984.64, "end": 1990.64, "text": " The simplified mental model of the problem domain could then be explained to a computer", "tokens": [440, 26335, 4973, 2316, 295, 264, 1154, 9274, 727, 550, 312, 8825, 281, 257, 3820], "temperature": 0.0, "avg_logprob": -0.10452082422044542, "compression_ratio": 1.551111111111111, "no_speech_prob": 4.560558591037989e-05}, {"id": 281, "seek": 198464, "start": 1990.64, "end": 1996.72, "text": " in the form of a computer program. What is changing is that computers are now making", "tokens": [294, 264, 1254, 295, 257, 3820, 1461, 13, 708, 307, 4473, 307, 300, 10807, 366, 586, 1455], "temperature": 0.0, "avg_logprob": -0.10452082422044542, "compression_ratio": 1.551111111111111, "no_speech_prob": 4.560558591037989e-05}, {"id": 282, "seek": 198464, "start": 1996.72, "end": 2004.5200000000002, "text": " these models themselves. The first bullet point describes regular programming, including", "tokens": [613, 5245, 2969, 13, 440, 700, 11632, 935, 15626, 3890, 9410, 11, 3009], "temperature": 0.0, "avg_logprob": -0.10452082422044542, "compression_ratio": 1.551111111111111, "no_speech_prob": 4.560558591037989e-05}, {"id": 283, "seek": 198464, "start": 2004.5200000000002, "end": 2014.1200000000001, "text": " old style AI programs. AI has, since 1955, provided many novel and brilliant algorithms", "tokens": [1331, 3758, 7318, 4268, 13, 7318, 575, 11, 1670, 46881, 11, 5649, 867, 7613, 293, 10248, 14642], "temperature": 0.0, "avg_logprob": -0.10452082422044542, "compression_ratio": 1.551111111111111, "no_speech_prob": 4.560558591037989e-05}, {"id": 284, "seek": 201412, "start": 2014.12, "end": 2020.8, "text": " that we now use in programs everywhere. But when you contrast old style AI to understanding", "tokens": [300, 321, 586, 764, 294, 4268, 5315, 13, 583, 562, 291, 8712, 1331, 3758, 7318, 281, 3701], "temperature": 0.0, "avg_logprob": -0.1101226806640625, "compression_ratio": 1.5862068965517242, "no_speech_prob": 6.98865478625521e-05}, {"id": 285, "seek": 201412, "start": 2020.8, "end": 2027.9599999999998, "text": " systems, the old kind of AI is basically indistinguishable from any other kind of programming we do", "tokens": [3652, 11, 264, 1331, 733, 295, 7318, 307, 1936, 1016, 468, 7050, 742, 712, 490, 604, 661, 733, 295, 9410, 321, 360], "temperature": 0.0, "avg_logprob": -0.1101226806640625, "compression_ratio": 1.5862068965517242, "no_speech_prob": 6.98865478625521e-05}, {"id": 286, "seek": 201412, "start": 2027.9599999999998, "end": 2034.9599999999998, "text": " nowadays. The second bullet point describes the recent developments. Deep neural networks", "tokens": [13434, 13, 440, 1150, 11632, 935, 15626, 264, 5162, 20862, 13, 14895, 18161, 9590], "temperature": 0.0, "avg_logprob": -0.1101226806640625, "compression_ratio": 1.5862068965517242, "no_speech_prob": 6.98865478625521e-05}, {"id": 287, "seek": 201412, "start": 2034.9599999999998, "end": 2040.1599999999999, "text": " are so different from regular programs that we have to acknowledge them as a different", "tokens": [366, 370, 819, 490, 3890, 4268, 300, 321, 362, 281, 10692, 552, 382, 257, 819], "temperature": 0.0, "avg_logprob": -0.1101226806640625, "compression_ratio": 1.5862068965517242, "no_speech_prob": 6.98865478625521e-05}, {"id": 288, "seek": 204016, "start": 2040.16, "end": 2047.16, "text": " computational paradigm. This is why they took almost four decades to develop. And the", "tokens": [28270, 24709, 13, 639, 307, 983, 436, 1890, 1920, 1451, 7878, 281, 1499, 13, 400, 264], "temperature": 0.0, "avg_logprob": -0.10022452940423805, "compression_ratio": 1.540084388185654, "no_speech_prob": 5.974941814201884e-05}, {"id": 289, "seek": 204016, "start": 2047.16, "end": 2054.44, "text": " paradigm, being pre-scientific and model-free, is difficult to grasp if you receive a solid", "tokens": [24709, 11, 885, 659, 12, 82, 5412, 1089, 293, 2316, 12, 10792, 11, 307, 2252, 281, 21743, 498, 291, 4774, 257, 5100], "temperature": 0.0, "avg_logprob": -0.10022452940423805, "compression_ratio": 1.540084388185654, "no_speech_prob": 5.974941814201884e-05}, {"id": 290, "seek": 204016, "start": 2054.44, "end": 2061.44, "text": " reductionist and model-based education. It takes a long time for an established AI practitioners", "tokens": [11004, 468, 293, 2316, 12, 6032, 3309, 13, 467, 2516, 257, 938, 565, 337, 364, 7545, 7318, 25742], "temperature": 0.0, "avg_logprob": -0.10022452940423805, "compression_ratio": 1.540084388185654, "no_speech_prob": 5.974941814201884e-05}, {"id": 291, "seek": 204016, "start": 2061.44, "end": 2067.84, "text": " or experienced programmer to switch. People who are just starting out in AI have an easier", "tokens": [420, 6751, 32116, 281, 3679, 13, 3432, 567, 366, 445, 2891, 484, 294, 7318, 362, 364, 3571], "temperature": 0.0, "avg_logprob": -0.10022452940423805, "compression_ratio": 1.540084388185654, "no_speech_prob": 5.974941814201884e-05}, {"id": 292, "seek": 206784, "start": 2067.84, "end": 2073.0, "text": " time assimilating this new paradigm since they haven't had a full career's worth of", "tokens": [565, 8249, 388, 990, 341, 777, 24709, 1670, 436, 2378, 380, 632, 257, 1577, 3988, 311, 3163, 295], "temperature": 0.0, "avg_logprob": -0.16370659125478645, "compression_ratio": 1.5333333333333334, "no_speech_prob": 3.983965143561363e-05}, {"id": 293, "seek": 206784, "start": 2073.0, "end": 2079.32, "text": " experience and success using old style AI techniques. The amount of work we have to", "tokens": [1752, 293, 2245, 1228, 1331, 3758, 7318, 7512, 13, 440, 2372, 295, 589, 321, 362, 281], "temperature": 0.0, "avg_logprob": -0.16370659125478645, "compression_ratio": 1.5333333333333334, "no_speech_prob": 3.983965143561363e-05}, {"id": 294, "seek": 206784, "start": 2079.32, "end": 2085.7200000000003, "text": " do to get a deep neural network to understand is surprisingly small, and companies like", "tokens": [360, 281, 483, 257, 2452, 18161, 3209, 281, 1223, 307, 17600, 1359, 11, 293, 3431, 411], "temperature": 0.0, "avg_logprob": -0.16370659125478645, "compression_ratio": 1.5333333333333334, "no_speech_prob": 3.983965143561363e-05}, {"id": 295, "seek": 206784, "start": 2085.7200000000003, "end": 2091.0, "text": " Google and Cintiens are working on eliminating the remaining effort of programming neural", "tokens": [3329, 293, 383, 686, 72, 694, 366, 1364, 322, 31203, 264, 8877, 4630, 295, 9410, 18161], "temperature": 0.0, "avg_logprob": -0.16370659125478645, "compression_ratio": 1.5333333333333334, "no_speech_prob": 3.983965143561363e-05}, {"id": 296, "seek": 209100, "start": 2091.0, "end": 2098.68, "text": " networks. This is where things will get really weird. When the deep neural network, DNN,", "tokens": [9590, 13, 639, 307, 689, 721, 486, 483, 534, 3657, 13, 1133, 264, 2452, 18161, 3209, 11, 21500, 45, 11], "temperature": 0.0, "avg_logprob": -0.09394696431282239, "compression_ratio": 1.5676855895196506, "no_speech_prob": 0.00010059009218821302}, {"id": 297, "seek": 209100, "start": 2098.68, "end": 2104.04, "text": " understands enough about the world and about the problem it is faced with, then we no longer", "tokens": [15146, 1547, 466, 264, 1002, 293, 466, 264, 1154, 309, 307, 11446, 365, 11, 550, 321, 572, 2854], "temperature": 0.0, "avg_logprob": -0.09394696431282239, "compression_ratio": 1.5676855895196506, "no_speech_prob": 0.00010059009218821302}, {"id": 298, "seek": 209100, "start": 2104.04, "end": 2111.56, "text": " need a programmer to acquire this understanding. Let me elaborate. Programmers are employed", "tokens": [643, 257, 32116, 281, 20001, 341, 3701, 13, 961, 385, 20945, 13, 8338, 18552, 366, 20115], "temperature": 0.0, "avg_logprob": -0.09394696431282239, "compression_ratio": 1.5676855895196506, "no_speech_prob": 0.00010059009218821302}, {"id": 299, "seek": 209100, "start": 2111.56, "end": 2117.76, "text": " to bridge two different domains. They first have to study whatever application domain", "tokens": [281, 7283, 732, 819, 25514, 13, 814, 700, 362, 281, 2979, 2035, 3861, 9274], "temperature": 0.0, "avg_logprob": -0.09394696431282239, "compression_ratio": 1.5676855895196506, "no_speech_prob": 0.00010059009218821302}, {"id": 300, "seek": 211776, "start": 2117.76, "end": 2123.7200000000003, "text": " they are working on. For instance, if they are writing an airline ticket reservation", "tokens": [436, 366, 1364, 322, 13, 1171, 5197, 11, 498, 436, 366, 3579, 364, 29528, 10550, 28922], "temperature": 0.0, "avg_logprob": -0.1290941363886783, "compression_ratio": 1.6759259259259258, "no_speech_prob": 6.251510058064014e-05}, {"id": 301, "seek": 211776, "start": 2123.7200000000003, "end": 2130.5200000000004, "text": " system they will have to learn a lot of detailed information about airlines, airline tickets,", "tokens": [1185, 436, 486, 362, 281, 1466, 257, 688, 295, 9942, 1589, 466, 37147, 11, 29528, 12628, 11], "temperature": 0.0, "avg_logprob": -0.1290941363886783, "compression_ratio": 1.6759259259259258, "no_speech_prob": 6.251510058064014e-05}, {"id": 302, "seek": 211776, "start": 2130.5200000000004, "end": 2138.4, "text": " flights, luggage, etc. and then know to provide features for unusual cases such as cancelled", "tokens": [21089, 11, 27744, 11, 5183, 13, 293, 550, 458, 281, 2893, 4122, 337, 10901, 3331, 1270, 382, 25103], "temperature": 0.0, "avg_logprob": -0.1290941363886783, "compression_ratio": 1.6759259259259258, "no_speech_prob": 6.251510058064014e-05}, {"id": 303, "seek": 211776, "start": 2138.4, "end": 2144.7200000000003, "text": " flights. And then the programmer uses their understanding of the problem domain to explain", "tokens": [21089, 13, 400, 550, 264, 32116, 4960, 641, 3701, 295, 264, 1154, 9274, 281, 2903], "temperature": 0.0, "avg_logprob": -0.1290941363886783, "compression_ratio": 1.6759259259259258, "no_speech_prob": 6.251510058064014e-05}, {"id": 304, "seek": 214472, "start": 2144.72, "end": 2150.4399999999996, "text": " to a computer how it can reason about these things, but the programmer cannot make the", "tokens": [281, 257, 3820, 577, 309, 393, 1778, 466, 613, 721, 11, 457, 264, 32116, 2644, 652, 264], "temperature": 0.0, "avg_logprob": -0.1605386257171631, "compression_ratio": 1.7439613526570048, "no_speech_prob": 6.387219036696479e-05}, {"id": 305, "seek": 214472, "start": 2150.4399999999996, "end": 2156.64, "text": " system understand, it can only put in the hollow and fragile kind of reasoning, as a", "tokens": [1185, 1223, 11, 309, 393, 787, 829, 294, 264, 23972, 293, 23847, 733, 295, 21577, 11, 382, 257], "temperature": 0.0, "avg_logprob": -0.1605386257171631, "compression_ratio": 1.7439613526570048, "no_speech_prob": 6.387219036696479e-05}, {"id": 306, "seek": 214472, "start": 2156.64, "end": 2164.08, "text": " program with many of thin cases, and any misunderstandings the programmer has about the problem domain", "tokens": [1461, 365, 867, 295, 5862, 3331, 11, 293, 604, 35736, 1109, 264, 32116, 575, 466, 264, 1154, 9274], "temperature": 0.0, "avg_logprob": -0.1605386257171631, "compression_ratio": 1.7439613526570048, "no_speech_prob": 6.387219036696479e-05}, {"id": 307, "seek": 214472, "start": 2164.08, "end": 2172.04, "text": " will become bugs in the computer program. Notice the shift in terminology. More later.", "tokens": [486, 1813, 15120, 294, 264, 3820, 1461, 13, 13428, 264, 5513, 294, 27575, 13, 5048, 1780, 13], "temperature": 0.0, "avg_logprob": -0.1605386257171631, "compression_ratio": 1.7439613526570048, "no_speech_prob": 6.387219036696479e-05}, {"id": 308, "seek": 217204, "start": 2172.04, "end": 2180.04, "text": " But today, for certain classes of moderately complex problems, we can use a DNN to automatically", "tokens": [583, 965, 11, 337, 1629, 5359, 295, 10494, 1592, 3997, 2740, 11, 321, 393, 764, 257, 21500, 45, 281, 6772], "temperature": 0.0, "avg_logprob": -0.11406540590174058, "compression_ratio": 1.605263157894737, "no_speech_prob": 0.00011135583190480247}, {"id": 309, "seek": 217204, "start": 2180.04, "end": 2186.7599999999998, "text": " learn for itself how to understand the problem, which means we no longer need a programmer", "tokens": [1466, 337, 2564, 577, 281, 1223, 264, 1154, 11, 597, 1355, 321, 572, 2854, 643, 257, 32116], "temperature": 0.0, "avg_logprob": -0.11406540590174058, "compression_ratio": 1.605263157894737, "no_speech_prob": 0.00011135583190480247}, {"id": 310, "seek": 217204, "start": 2186.7599999999998, "end": 2193.52, "text": " to understand the problem. We have delegated our understanding to a machine, and if you", "tokens": [281, 1223, 264, 1154, 13, 492, 362, 15824, 770, 527, 3701, 281, 257, 3479, 11, 293, 498, 291], "temperature": 0.0, "avg_logprob": -0.11406540590174058, "compression_ratio": 1.605263157894737, "no_speech_prob": 0.00011135583190480247}, {"id": 311, "seek": 217204, "start": 2193.52, "end": 2199.88, "text": " think about that for a minute you will see that that's exactly what an AI should be doing.", "tokens": [519, 466, 300, 337, 257, 3456, 291, 486, 536, 300, 300, 311, 2293, 437, 364, 7318, 820, 312, 884, 13], "temperature": 0.0, "avg_logprob": -0.11406540590174058, "compression_ratio": 1.605263157894737, "no_speech_prob": 0.00011135583190480247}, {"id": 312, "seek": 219988, "start": 2199.88, "end": 2206.0, "text": " It should understand all kinds of things, so that we humans won't have to. And there", "tokens": [467, 820, 1223, 439, 3685, 295, 721, 11, 370, 300, 321, 6255, 1582, 380, 362, 281, 13, 400, 456], "temperature": 0.0, "avg_logprob": -0.1367770771921417, "compression_ratio": 1.619718309859155, "no_speech_prob": 0.00015723728574812412}, {"id": 313, "seek": 219988, "start": 2206.0, "end": 2212.0, "text": " are two common situations where this will be a really good idea. One is when we have", "tokens": [366, 732, 2689, 6851, 689, 341, 486, 312, 257, 534, 665, 1558, 13, 1485, 307, 562, 321, 362], "temperature": 0.0, "avg_logprob": -0.1367770771921417, "compression_ratio": 1.619718309859155, "no_speech_prob": 0.00015723728574812412}, {"id": 314, "seek": 219988, "start": 2212.0, "end": 2218.96, "text": " a problem we cannot understand ourselves. We know a lot of those, starting with cellular", "tokens": [257, 1154, 321, 2644, 1223, 4175, 13, 492, 458, 257, 688, 295, 729, 11, 2891, 365, 29267], "temperature": 0.0, "avg_logprob": -0.1367770771921417, "compression_ratio": 1.619718309859155, "no_speech_prob": 0.00015723728574812412}, {"id": 315, "seek": 219988, "start": 2218.96, "end": 2225.44, "text": " biology. The other common case will be when we understand the problem well, but making", "tokens": [14956, 13, 440, 661, 2689, 1389, 486, 312, 562, 321, 1223, 264, 1154, 731, 11, 457, 1455], "temperature": 0.0, "avg_logprob": -0.1367770771921417, "compression_ratio": 1.619718309859155, "no_speech_prob": 0.00015723728574812412}, {"id": 316, "seek": 222544, "start": 2225.44, "end": 2230.92, "text": " the machine understand it well enough to get the job done is cheaper and easier than any", "tokens": [264, 3479, 1223, 309, 731, 1547, 281, 483, 264, 1691, 1096, 307, 12284, 293, 3571, 813, 604], "temperature": 0.0, "avg_logprob": -0.19831786155700684, "compression_ratio": 1.550660792951542, "no_speech_prob": 2.8763288355548866e-05}, {"id": 317, "seek": 222544, "start": 2230.92, "end": 2237.8, "text": " alternative. MoonBoss accomplish this level of using old style AI methods, but I predict", "tokens": [8535, 13, 10714, 33, 772, 9021, 341, 1496, 295, 1228, 1331, 3758, 7318, 7150, 11, 457, 286, 6069], "temperature": 0.0, "avg_logprob": -0.19831786155700684, "compression_ratio": 1.550660792951542, "no_speech_prob": 2.8763288355548866e-05}, {"id": 318, "seek": 222544, "start": 2237.8, "end": 2243.84, "text": " we will one day be flooded with similar, but DNN based devices that understand several", "tokens": [321, 486, 472, 786, 312, 31594, 365, 2531, 11, 457, 21500, 45, 2361, 5759, 300, 1223, 2940], "temperature": 0.0, "avg_logprob": -0.19831786155700684, "compression_ratio": 1.550660792951542, "no_speech_prob": 2.8763288355548866e-05}, {"id": 319, "seek": 222544, "start": 2243.84, "end": 2251.4, "text": " aspects of domestic maintenance, as well as we do. Do machines really understand? If we", "tokens": [7270, 295, 10939, 11258, 11, 382, 731, 382, 321, 360, 13, 1144, 8379, 534, 1223, 30, 759, 321], "temperature": 0.0, "avg_logprob": -0.19831786155700684, "compression_ratio": 1.550660792951542, "no_speech_prob": 2.8763288355548866e-05}, {"id": 320, "seek": 225140, "start": 2251.4, "end": 2257.84, "text": " give a picture like this to a DNN trained on images it will identify the important objects", "tokens": [976, 257, 3036, 411, 341, 281, 257, 21500, 45, 8895, 322, 5267, 309, 486, 5876, 264, 1021, 6565], "temperature": 0.0, "avg_logprob": -0.11843734068029067, "compression_ratio": 1.5627705627705628, "no_speech_prob": 4.7834353608777747e-05}, {"id": 321, "seek": 225140, "start": 2257.84, "end": 2264.6800000000003, "text": " in the image and provide the rectangles, called, owning boxes as approximations to where the", "tokens": [294, 264, 3256, 293, 2893, 264, 24077, 904, 11, 1219, 11, 29820, 9002, 382, 8542, 763, 281, 689, 264], "temperature": 0.0, "avg_logprob": -0.11843734068029067, "compression_ratio": 1.5627705627705628, "no_speech_prob": 4.7834353608777747e-05}, {"id": 322, "seek": 225140, "start": 2264.6800000000003, "end": 2271.32, "text": " objects are. The text on the right says, woman in white dress standing with tennis racket", "tokens": [6565, 366, 13, 440, 2487, 322, 264, 558, 1619, 11, 3059, 294, 2418, 5231, 4877, 365, 18118, 41130], "temperature": 0.0, "avg_logprob": -0.11843734068029067, "compression_ratio": 1.5627705627705628, "no_speech_prob": 4.7834353608777747e-05}, {"id": 323, "seek": 225140, "start": 2271.32, "end": 2278.28, "text": " to people in green behind her, which is not a bad description of the image. It could be", "tokens": [281, 561, 294, 3092, 2261, 720, 11, 597, 307, 406, 257, 1578, 3855, 295, 264, 3256, 13, 467, 727, 312], "temperature": 0.0, "avg_logprob": -0.11843734068029067, "compression_ratio": 1.5627705627705628, "no_speech_prob": 4.7834353608777747e-05}, {"id": 324, "seek": 227828, "start": 2278.28, "end": 2285.1600000000003, "text": " used as the basis for a test for English skill level for adult education placement, for all", "tokens": [1143, 382, 264, 5143, 337, 257, 1500, 337, 3669, 5389, 1496, 337, 5075, 3309, 17257, 11, 337, 439], "temperature": 0.0, "avg_logprob": -0.15965251317099918, "compression_ratio": 1.4565217391304348, "no_speech_prob": 0.00012473028618842363}, {"id": 325, "seek": 227828, "start": 2285.1600000000003, "end": 2292.48, "text": " practical purposes. This is understanding. We had no idea how to make our computers do", "tokens": [8496, 9932, 13, 639, 307, 3701, 13, 492, 632, 572, 1558, 577, 281, 652, 527, 10807, 360], "temperature": 0.0, "avg_logprob": -0.15965251317099918, "compression_ratio": 1.4565217391304348, "no_speech_prob": 0.00012473028618842363}, {"id": 326, "seek": 227828, "start": 2292.48, "end": 2301.4, "text": " this before 2012. This is a really big deal. This feat requires not only a new algorithm,", "tokens": [341, 949, 9125, 13, 639, 307, 257, 534, 955, 2028, 13, 639, 15425, 7029, 406, 787, 257, 777, 9284, 11], "temperature": 0.0, "avg_logprob": -0.15965251317099918, "compression_ratio": 1.4565217391304348, "no_speech_prob": 0.00012473028618842363}, {"id": 327, "seek": 230140, "start": 2301.4, "end": 2308.84, "text": " it requires a new computational paradigm and images to a computer, a single long sequence", "tokens": [309, 7029, 257, 777, 28270, 24709, 293, 5267, 281, 257, 3820, 11, 257, 2167, 938, 8310], "temperature": 0.0, "avg_logprob": -0.14913731529599145, "compression_ratio": 1.5638766519823788, "no_speech_prob": 1.853354297054466e-05}, {"id": 328, "seek": 230140, "start": 2308.84, "end": 2317.56, "text": " of numbers denoting values for red, blue and green colors and values from 0 to 255. It", "tokens": [295, 3547, 1441, 17001, 4190, 337, 2182, 11, 3344, 293, 3092, 4577, 293, 4190, 490, 1958, 281, 3552, 20, 13, 467], "temperature": 0.0, "avg_logprob": -0.14913731529599145, "compression_ratio": 1.5638766519823788, "no_speech_prob": 1.853354297054466e-05}, {"id": 329, "seek": 230140, "start": 2317.56, "end": 2324.48, "text": " also knows how wide the image is. How does it get from this very low level representation", "tokens": [611, 3255, 577, 4874, 264, 3256, 307, 13, 1012, 775, 309, 483, 490, 341, 588, 2295, 1496, 10290], "temperature": 0.0, "avg_logprob": -0.14913731529599145, "compression_ratio": 1.5638766519823788, "no_speech_prob": 1.853354297054466e-05}, {"id": 330, "seek": 230140, "start": 2324.48, "end": 2329.8, "text": " to knowing that there is a woman with a tennis racket in the image? This is what William", "tokens": [281, 5276, 300, 456, 307, 257, 3059, 365, 257, 18118, 41130, 294, 264, 3256, 30, 639, 307, 437, 6740], "temperature": 0.0, "avg_logprob": -0.14913731529599145, "compression_ratio": 1.5638766519823788, "no_speech_prob": 1.853354297054466e-05}, {"id": 331, "seek": 232980, "start": 2329.8, "end": 2336.2400000000002, "text": " Calvin has called, a river that flows uphill. There are very few mechanisms that can go", "tokens": [28025, 575, 1219, 11, 257, 6810, 300, 12867, 39132, 13, 821, 366, 588, 1326, 15902, 300, 393, 352], "temperature": 0.0, "avg_logprob": -0.1453831872822326, "compression_ratio": 1.7083333333333333, "no_speech_prob": 6.1041719163768e-05}, {"id": 332, "seek": 232980, "start": 2336.2400000000002, "end": 2343.6800000000003, "text": " in this direction, from low levels to high levels. Calvin used the term to describe evolution,", "tokens": [294, 341, 3513, 11, 490, 2295, 4358, 281, 1090, 4358, 13, 28025, 1143, 264, 1433, 281, 6786, 9303, 11], "temperature": 0.0, "avg_logprob": -0.1453831872822326, "compression_ratio": 1.7083333333333333, "no_speech_prob": 6.1041719163768e-05}, {"id": 333, "seek": 232980, "start": 2343.6800000000003, "end": 2351.1200000000003, "text": " and I can use this quote to describe understanding. I like to think of evolution as, nature's", "tokens": [293, 286, 393, 764, 341, 6513, 281, 6786, 3701, 13, 286, 411, 281, 519, 295, 9303, 382, 11, 3687, 311], "temperature": 0.0, "avg_logprob": -0.1453831872822326, "compression_ratio": 1.7083333333333333, "no_speech_prob": 6.1041719163768e-05}, {"id": 334, "seek": 232980, "start": 2351.1200000000003, "end": 2357.88, "text": " understanding because the phenomena are very similar at several levels. Evolution of species", "tokens": [3701, 570, 264, 22004, 366, 588, 2531, 412, 2940, 4358, 13, 40800, 295, 6172], "temperature": 0.0, "avg_logprob": -0.1453831872822326, "compression_ratio": 1.7083333333333333, "no_speech_prob": 6.1041719163768e-05}, {"id": 335, "seek": 235788, "start": 2357.88, "end": 2363.48, "text": " can bring forth advanced species starting from simpler species in the same manner that", "tokens": [393, 1565, 5220, 7339, 6172, 2891, 490, 18587, 6172, 294, 264, 912, 9060, 300], "temperature": 0.0, "avg_logprob": -0.11988818018059981, "compression_ratio": 1.669767441860465, "no_speech_prob": 5.7999652199214324e-05}, {"id": 336, "seek": 235788, "start": 2363.48, "end": 2369.48, "text": " understanding is the discovery and reuse of high level concepts and low level input.", "tokens": [3701, 307, 264, 12114, 293, 26225, 295, 1090, 1496, 10392, 293, 2295, 1496, 4846, 13], "temperature": 0.0, "avg_logprob": -0.11988818018059981, "compression_ratio": 1.669767441860465, "no_speech_prob": 5.7999652199214324e-05}, {"id": 337, "seek": 235788, "start": 2369.48, "end": 2376.6400000000003, "text": " In contrast, reasoning proceeds by breaking problems into sub-problems and solving those,", "tokens": [682, 8712, 11, 21577, 32280, 538, 7697, 2740, 666, 1422, 12, 47419, 82, 293, 12606, 729, 11], "temperature": 0.0, "avg_logprob": -0.11988818018059981, "compression_ratio": 1.669767441860465, "no_speech_prob": 5.7999652199214324e-05}, {"id": 338, "seek": 235788, "start": 2376.6400000000003, "end": 2384.6, "text": " which is a, flowing downhill, kind of strategy. In mathematics we accept, and many mathematicians", "tokens": [597, 307, 257, 11, 13974, 29929, 11, 733, 295, 5206, 13, 682, 18666, 321, 3241, 11, 293, 867, 32811, 2567], "temperature": 0.0, "avg_logprob": -0.11988818018059981, "compression_ratio": 1.669767441860465, "no_speech_prob": 5.7999652199214324e-05}, {"id": 339, "seek": 238460, "start": 2384.6, "end": 2391.04, "text": " only accept this reluctantly, that we need to use induction to move uphill in abstractions,", "tokens": [787, 3241, 341, 25149, 3627, 11, 300, 321, 643, 281, 764, 33371, 281, 1286, 39132, 294, 12649, 626, 11], "temperature": 0.0, "avg_logprob": -0.13993142951618542, "compression_ratio": 1.5982142857142858, "no_speech_prob": 3.9857648516772315e-05}, {"id": 340, "seek": 238460, "start": 2391.04, "end": 2398.24, "text": " and that's a very limited uphill movement at that. Epistemology allows for much stronger", "tokens": [293, 300, 311, 257, 588, 5567, 39132, 3963, 412, 300, 13, 9970, 43958, 1793, 4045, 337, 709, 7249], "temperature": 0.0, "avg_logprob": -0.13993142951618542, "compression_ratio": 1.5982142857142858, "no_speech_prob": 3.9857648516772315e-05}, {"id": 341, "seek": 238460, "start": 2398.24, "end": 2405.0, "text": " uphill moves. This is known as, jumping to conclusions on scant evidence and it's allowed", "tokens": [39132, 6067, 13, 639, 307, 2570, 382, 11, 11233, 281, 22865, 322, 795, 394, 4467, 293, 309, 311, 4350], "temperature": 0.0, "avg_logprob": -0.13993142951618542, "compression_ratio": 1.5982142857142858, "no_speech_prob": 3.9857648516772315e-05}, {"id": 342, "seek": 238460, "start": 2405.0, "end": 2411.7999999999997, "text": " in epistemology based pre-scientific systems. As an aside, here's a pretty deep related", "tokens": [294, 2388, 43958, 1793, 2361, 659, 12, 82, 5412, 1089, 3652, 13, 1018, 364, 7359, 11, 510, 311, 257, 1238, 2452, 4077], "temperature": 0.0, "avg_logprob": -0.13993142951618542, "compression_ratio": 1.5982142857142858, "no_speech_prob": 3.9857648516772315e-05}, {"id": 343, "seek": 241180, "start": 2411.8, "end": 2419.4, "text": " thought. In nature, evolution reuses anything that works. I like to think that understanding", "tokens": [1194, 13, 682, 3687, 11, 9303, 319, 8355, 1340, 300, 1985, 13, 286, 411, 281, 519, 300, 3701], "temperature": 0.0, "avg_logprob": -0.14482897381449855, "compression_ratio": 1.5084745762711864, "no_speech_prob": 9.665208926890045e-05}, {"id": 344, "seek": 241180, "start": 2419.4, "end": 2426.5600000000004, "text": " is a spandrel of evolution itself. Neural Darwinism certainly straddles this gap. Could", "tokens": [307, 257, 637, 474, 4419, 295, 9303, 2564, 13, 1734, 1807, 30233, 1434, 3297, 1056, 345, 21915, 341, 7417, 13, 7497], "temperature": 0.0, "avg_logprob": -0.14482897381449855, "compression_ratio": 1.5084745762711864, "no_speech_prob": 9.665208926890045e-05}, {"id": 345, "seek": 241180, "start": 2426.5600000000004, "end": 2433.88, "text": " be coincidence, or the only answer that will work at all. More later, we doubled our AI", "tokens": [312, 22137, 11, 420, 264, 787, 1867, 300, 486, 589, 412, 439, 13, 5048, 1780, 11, 321, 24405, 527, 7318], "temperature": 0.0, "avg_logprob": -0.14482897381449855, "compression_ratio": 1.5084745762711864, "no_speech_prob": 9.665208926890045e-05}, {"id": 346, "seek": 241180, "start": 2433.88, "end": 2441.0800000000004, "text": " toolkit in 2012. We can now use these deep neural networks as components in our systems", "tokens": [40167, 294, 9125, 13, 492, 393, 586, 764, 613, 2452, 18161, 9590, 382, 6677, 294, 527, 3652], "temperature": 0.0, "avg_logprob": -0.14482897381449855, "compression_ratio": 1.5084745762711864, "no_speech_prob": 9.665208926890045e-05}, {"id": 347, "seek": 244108, "start": 2441.08, "end": 2447.16, "text": " to provide understanding of certain things like vision, speech, and other problems that", "tokens": [281, 2893, 3701, 295, 1629, 721, 411, 5201, 11, 6218, 11, 293, 661, 2740, 300], "temperature": 0.0, "avg_logprob": -0.10335118093608338, "compression_ratio": 1.565217391304348, "no_speech_prob": 5.244992644293234e-05}, {"id": 348, "seek": 244108, "start": 2447.16, "end": 2454.24, "text": " require that we discover high level concepts and low level data. The technical, epistemology", "tokens": [3651, 300, 321, 4411, 1090, 1496, 10392, 293, 2295, 1496, 1412, 13, 440, 6191, 11, 2388, 43958, 1793], "temperature": 0.0, "avg_logprob": -0.10335118093608338, "compression_ratio": 1.565217391304348, "no_speech_prob": 5.244992644293234e-05}, {"id": 349, "seek": 244108, "start": 2454.24, "end": 2460.84, "text": " level name for this uphill flow in processes, reduction, and we'll be using that term later", "tokens": [1496, 1315, 337, 341, 39132, 3095, 294, 7555, 11, 11004, 11, 293, 321, 603, 312, 1228, 300, 1433, 1780], "temperature": 0.0, "avg_logprob": -0.10335118093608338, "compression_ratio": 1.565217391304348, "no_speech_prob": 5.244992644293234e-05}, {"id": 350, "seek": 244108, "start": 2460.84, "end": 2466.36, "text": " after we explain what it means. Let's look at what the industry is doing with their new", "tokens": [934, 321, 2903, 437, 309, 1355, 13, 961, 311, 574, 412, 437, 264, 3518, 307, 884, 365, 641, 777], "temperature": 0.0, "avg_logprob": -0.10335118093608338, "compression_ratio": 1.565217391304348, "no_speech_prob": 5.244992644293234e-05}, {"id": 351, "seek": 246636, "start": 2466.36, "end": 2474.08, "text": " found toys. This is my view of what I think Tesla is doing, based on public sources in", "tokens": [1352, 13753, 13, 639, 307, 452, 1910, 295, 437, 286, 519, 13666, 307, 884, 11, 2361, 322, 1908, 7139, 294], "temperature": 0.0, "avg_logprob": -0.11178952891652177, "compression_ratio": 1.5874439461883407, "no_speech_prob": 0.00012729078298434615}, {"id": 352, "seek": 246636, "start": 2474.08, "end": 2481.04, "text": " their self-driving, autopilot, cars, cameras feed vision understanding components based", "tokens": [641, 2698, 12, 47094, 11, 31090, 31516, 11, 5163, 11, 8622, 3154, 5201, 3701, 6677, 2361], "temperature": 0.0, "avg_logprob": -0.11178952891652177, "compression_ratio": 1.5874439461883407, "no_speech_prob": 0.00012729078298434615}, {"id": 353, "seek": 246636, "start": 2481.04, "end": 2487.48, "text": " on deep learning, and radar feeds to radar understanding components. These supply bounding", "tokens": [322, 2452, 2539, 11, 293, 16544, 23712, 281, 16544, 3701, 6677, 13, 1981, 5847, 5472, 278], "temperature": 0.0, "avg_logprob": -0.11178952891652177, "compression_ratio": 1.5874439461883407, "no_speech_prob": 0.00012729078298434615}, {"id": 354, "seek": 246636, "start": 2487.48, "end": 2493.88, "text": " boxes in 2D or 3D with additional information like, there's a woman with a tennis racket", "tokens": [9002, 294, 568, 35, 420, 805, 35, 365, 4497, 1589, 411, 11, 456, 311, 257, 3059, 365, 257, 18118, 41130], "temperature": 0.0, "avg_logprob": -0.11178952891652177, "compression_ratio": 1.5874439461883407, "no_speech_prob": 0.00012729078298434615}, {"id": 355, "seek": 249388, "start": 2493.88, "end": 2499.8, "text": " ahead to a traffic reasoning component that uses regular programming, or some old style", "tokens": [2286, 281, 257, 6419, 21577, 6542, 300, 4960, 3890, 9410, 11, 420, 512, 1331, 3758], "temperature": 0.0, "avg_logprob": -0.2133616054759306, "compression_ratio": 1.5063829787234042, "no_speech_prob": 4.350066228653304e-05}, {"id": 356, "seek": 249388, "start": 2499.8, "end": 2506.6400000000003, "text": " AI like a rule based system to actually control the car based on the vision and radar inputs,", "tokens": [7318, 411, 257, 4978, 2361, 1185, 281, 767, 1969, 264, 1032, 2361, 322, 264, 5201, 293, 16544, 15743, 11], "temperature": 0.0, "avg_logprob": -0.2133616054759306, "compression_ratio": 1.5063829787234042, "no_speech_prob": 4.350066228653304e-05}, {"id": 357, "seek": 249388, "start": 2506.6400000000003, "end": 2514.12, "text": " and the driver's desires. But this is not the only possible configuration. George Hopps", "tokens": [293, 264, 6787, 311, 18005, 13, 583, 341, 307, 406, 264, 787, 1944, 11694, 13, 7136, 13438, 1878], "temperature": 0.0, "avg_logprob": -0.2133616054759306, "compression_ratio": 1.5063829787234042, "no_speech_prob": 4.350066228653304e-05}, {"id": 358, "seek": 249388, "start": 2514.12, "end": 2521.48, "text": " at Comma.ai, a team at NVIDIA Corporation, and the deep Tesla class at MIT are using", "tokens": [412, 3046, 64, 13, 1301, 11, 257, 1469, 412, 426, 3958, 6914, 26464, 11, 293, 264, 2452, 13666, 1508, 412, 13100, 366, 1228], "temperature": 0.0, "avg_logprob": -0.2133616054759306, "compression_ratio": 1.5063829787234042, "no_speech_prob": 4.350066228653304e-05}, {"id": 359, "seek": 252148, "start": 2521.48, "end": 2526.84, "text": " a simpler architecture with just a neural network that implements lane following and", "tokens": [257, 18587, 9482, 365, 445, 257, 18161, 3209, 300, 704, 17988, 12705, 3480, 293], "temperature": 0.0, "avg_logprob": -0.1461985641055637, "compression_ratio": 1.6824644549763033, "no_speech_prob": 5.018683441448957e-05}, {"id": 360, "seek": 252148, "start": 2526.84, "end": 2532.84, "text": " other simple driving behaviors directly in one single deep neural network. There's room", "tokens": [661, 2199, 4840, 15501, 3838, 294, 472, 2167, 2452, 18161, 3209, 13, 821, 311, 1808], "temperature": 0.0, "avg_logprob": -0.1461985641055637, "compression_ratio": 1.6824644549763033, "no_speech_prob": 5.018683441448957e-05}, {"id": 361, "seek": 252148, "start": 2532.84, "end": 2540.36, "text": " for improvement, but there a big step in the direction we want to move in. Future automotive", "tokens": [337, 10444, 11, 457, 456, 257, 955, 1823, 294, 264, 3513, 321, 528, 281, 1286, 294, 13, 20805, 32866], "temperature": 0.0, "avg_logprob": -0.1461985641055637, "compression_ratio": 1.6824644549763033, "no_speech_prob": 5.018683441448957e-05}, {"id": 362, "seek": 252148, "start": 2540.36, "end": 2546.4, "text": " systems will likely integrate everything about driving into one single neural network, or", "tokens": [3652, 486, 3700, 13365, 1203, 466, 4840, 666, 472, 2167, 18161, 3209, 11, 420], "temperature": 0.0, "avg_logprob": -0.1461985641055637, "compression_ratio": 1.6824644549763033, "no_speech_prob": 5.018683441448957e-05}, {"id": 363, "seek": 254640, "start": 2546.4, "end": 2553.2400000000002, "text": " something that effectively behaves as one. Vision, traffic, the car itself including", "tokens": [746, 300, 8659, 36896, 382, 472, 13, 25170, 11, 6419, 11, 264, 1032, 2564, 3009], "temperature": 0.0, "avg_logprob": -0.14566373825073242, "compression_ratio": 1.5818181818181818, "no_speech_prob": 3.764067150768824e-05}, {"id": 364, "seek": 254640, "start": 2553.2400000000002, "end": 2560.08, "text": " various functionality like windscreen wipers, lights, and entertainment, how to drive in", "tokens": [3683, 14980, 411, 2468, 12439, 15887, 433, 11, 5811, 11, 293, 12393, 11, 577, 281, 3332, 294], "temperature": 0.0, "avg_logprob": -0.14566373825073242, "compression_ratio": 1.5818181818181818, "no_speech_prob": 3.764067150768824e-05}, {"id": 365, "seek": 254640, "start": 2560.08, "end": 2567.7200000000003, "text": " a safe and polite manner, and to understand also the drivers or car owners desires. And", "tokens": [257, 3273, 293, 25171, 9060, 11, 293, 281, 1223, 611, 264, 11590, 420, 1032, 7710, 18005, 13, 400], "temperature": 0.0, "avg_logprob": -0.14566373825073242, "compression_ratio": 1.5818181818181818, "no_speech_prob": 3.764067150768824e-05}, {"id": 366, "seek": 254640, "start": 2567.7200000000003, "end": 2573.76, "text": " if we've gotten that far, then it is a given that we will have speech input and output", "tokens": [498, 321, 600, 5768, 300, 1400, 11, 550, 309, 307, 257, 2212, 300, 321, 486, 362, 6218, 4846, 293, 5598], "temperature": 0.0, "avg_logprob": -0.14566373825073242, "compression_ratio": 1.5818181818181818, "no_speech_prob": 3.764067150768824e-05}, {"id": 367, "seek": 257376, "start": 2573.76, "end": 2579.32, "text": " so that the driver can have a conversation with the car while driving, and can just", "tokens": [370, 300, 264, 6787, 393, 362, 257, 3761, 365, 264, 1032, 1339, 4840, 11, 293, 393, 445], "temperature": 0.0, "avg_logprob": -0.18940938877154, "compression_ratio": 1.5344827586206897, "no_speech_prob": 2.411929381196387e-05}, {"id": 368, "seek": 257376, "start": 2579.32, "end": 2585.92, "text": " advise it in case it does something wrong. We are nowhere close to this today. But after", "tokens": [18312, 309, 294, 1389, 309, 775, 746, 2085, 13, 492, 366, 11159, 1998, 281, 341, 965, 13, 583, 934], "temperature": 0.0, "avg_logprob": -0.18940938877154, "compression_ratio": 1.5344827586206897, "no_speech_prob": 2.411929381196387e-05}, {"id": 369, "seek": 257376, "start": 2585.92, "end": 2593.2400000000002, "text": " a DNM breakthrough or two, who knows how quickly these kinds of systems become available. We", "tokens": [257, 21500, 44, 22397, 420, 732, 11, 567, 3255, 577, 2661, 613, 3685, 295, 3652, 1813, 2435, 13, 492], "temperature": 0.0, "avg_logprob": -0.18940938877154, "compression_ratio": 1.5344827586206897, "no_speech_prob": 2.411929381196387e-05}, {"id": 370, "seek": 257376, "start": 2593.2400000000002, "end": 2600.5200000000004, "text": " can already see an increasing stream of new features built using understanding components.", "tokens": [393, 1217, 536, 364, 5662, 4309, 295, 777, 4122, 3094, 1228, 3701, 6677, 13], "temperature": 0.0, "avg_logprob": -0.18940938877154, "compression_ratio": 1.5344827586206897, "no_speech_prob": 2.411929381196387e-05}, {"id": 371, "seek": 260052, "start": 2600.52, "end": 2607.52, "text": " This article, and the next, are expansions of a talk given on June 10, 2017 at the San", "tokens": [639, 7222, 11, 293, 264, 958, 11, 366, 9672, 626, 295, 257, 751, 2212, 322, 6928, 1266, 11, 6591, 412, 264, 5271], "temperature": 0.0, "avg_logprob": -0.11437339782714843, "compression_ratio": 1.482905982905983, "no_speech_prob": 0.00043241234379820526}, {"id": 372, "seek": 260052, "start": 2607.52, "end": 2615.28, "text": " Francisco Bill Conference. A decade ago I created artificialintuition.com. I now have", "tokens": [12279, 5477, 22131, 13, 316, 10378, 2057, 286, 2942, 11677, 686, 19080, 13, 1112, 13, 286, 586, 362], "temperature": 0.0, "avg_logprob": -0.11437339782714843, "compression_ratio": 1.482905982905983, "no_speech_prob": 0.00043241234379820526}, {"id": 373, "seek": 260052, "start": 2615.28, "end": 2622.12, "text": " a lot more to say, but I need to split this meme package into digestible chunks. This", "tokens": [257, 688, 544, 281, 584, 11, 457, 286, 643, 281, 7472, 341, 21701, 7372, 666, 13884, 964, 24004, 13, 639], "temperature": 0.0, "avg_logprob": -0.11437339782714843, "compression_ratio": 1.482905982905983, "no_speech_prob": 0.00043241234379820526}, {"id": 374, "seek": 260052, "start": 2622.12, "end": 2628.12, "text": " takes a lot of effort to get right. If you liked this article and would like to see more", "tokens": [2516, 257, 688, 295, 4630, 281, 483, 558, 13, 759, 291, 4501, 341, 7222, 293, 576, 411, 281, 536, 544], "temperature": 0.0, "avg_logprob": -0.11437339782714843, "compression_ratio": 1.482905982905983, "no_speech_prob": 0.00043241234379820526}, {"id": 375, "seek": 262812, "start": 2628.12, "end": 2635.12, "text": " like it then you can support my writing and my research in many ways, small to large,", "tokens": [411, 309, 550, 291, 393, 1406, 452, 3579, 293, 452, 2132, 294, 867, 2098, 11, 1359, 281, 2416, 11], "temperature": 0.0, "avg_logprob": -0.14019322395324707, "compression_ratio": 1.59375, "no_speech_prob": 0.00015045475447550416}, {"id": 376, "seek": 262812, "start": 2635.12, "end": 2640.7599999999998, "text": " like and share these ideas with someone who might want to invest in sentience incorporated", "tokens": [411, 293, 2073, 613, 3487, 365, 1580, 567, 1062, 528, 281, 1963, 294, 2279, 1182, 21654], "temperature": 0.0, "avg_logprob": -0.14019322395324707, "compression_ratio": 1.59375, "no_speech_prob": 0.00015045475447550416}, {"id": 377, "seek": 262812, "start": 2640.7599999999998, "end": 2646.2, "text": " or might be otherwise interested in my research on a novel language understanding technology", "tokens": [420, 1062, 312, 5911, 3102, 294, 452, 2132, 322, 257, 7613, 2856, 3701, 2899], "temperature": 0.0, "avg_logprob": -0.14019322395324707, "compression_ratio": 1.59375, "no_speech_prob": 0.00015045475447550416}, {"id": 378, "seek": 262812, "start": 2646.2, "end": 2653.72, "text": " called organic learning. More on that later. I do not receive external funding from any", "tokens": [1219, 10220, 2539, 13, 5048, 322, 300, 1780, 13, 286, 360, 406, 4774, 8320, 6137, 490, 604], "temperature": 0.0, "avg_logprob": -0.14019322395324707, "compression_ratio": 1.59375, "no_speech_prob": 0.00015045475447550416}, {"id": 379, "seek": 265372, "start": 2653.72, "end": 2660.22, "text": " investors for this research. You can support my research and writing directly at the donation", "tokens": [11519, 337, 341, 2132, 13, 509, 393, 1406, 452, 2132, 293, 3579, 3838, 412, 264, 19724], "temperature": 0.0, "avg_logprob": -0.21078565385606554, "compression_ratio": 1.3959390862944163, "no_speech_prob": 9.401034185430035e-05}, {"id": 380, "seek": 265372, "start": 2660.22, "end": 2669.3999999999996, "text": " section at artificialintuition.com. Chapter 2. Our Greatest Invention, Model Based Problem", "tokens": [3541, 412, 11677, 686, 19080, 13, 1112, 13, 18874, 568, 13, 2621, 3769, 377, 682, 6411, 11, 17105, 18785, 11676], "temperature": 0.0, "avg_logprob": -0.21078565385606554, "compression_ratio": 1.3959390862944163, "no_speech_prob": 9.401034185430035e-05}, {"id": 381, "seek": 265372, "start": 2669.3999999999996, "end": 2677.56, "text": " Solving. The first chapter, why AI works, provided the big picture of AI and understanding", "tokens": [7026, 798, 13, 440, 700, 7187, 11, 983, 7318, 1985, 11, 5649, 264, 955, 3036, 295, 7318, 293, 3701], "temperature": 0.0, "avg_logprob": -0.21078565385606554, "compression_ratio": 1.3959390862944163, "no_speech_prob": 9.401034185430035e-05}, {"id": 382, "seek": 267756, "start": 2677.56, "end": 2684.64, "text": " machines. Next we will focus on how to actually implement understanding in a computer. But", "tokens": [8379, 13, 3087, 321, 486, 1879, 322, 577, 281, 767, 4445, 3701, 294, 257, 3820, 13, 583], "temperature": 0.0, "avg_logprob": -0.11972240122353159, "compression_ratio": 1.5726872246696035, "no_speech_prob": 0.00011513527715578675}, {"id": 383, "seek": 267756, "start": 2684.64, "end": 2690.6, "text": " before we can attack that core issue, we need to simplify the journey a bit by defining", "tokens": [949, 321, 393, 2690, 300, 4965, 2734, 11, 321, 643, 281, 20460, 264, 4671, 257, 857, 538, 17827], "temperature": 0.0, "avg_logprob": -0.11972240122353159, "compression_ratio": 1.5726872246696035, "no_speech_prob": 0.00011513527715578675}, {"id": 384, "seek": 267756, "start": 2690.6, "end": 2698.12, "text": " four important words and concepts. I'll define one in this section, two in the next, and", "tokens": [1451, 1021, 2283, 293, 10392, 13, 286, 603, 6964, 472, 294, 341, 3541, 11, 732, 294, 264, 958, 11, 293], "temperature": 0.0, "avg_logprob": -0.11972240122353159, "compression_ratio": 1.5726872246696035, "no_speech_prob": 0.00011513527715578675}, {"id": 385, "seek": 267756, "start": 2698.12, "end": 2704.4, "text": " the concept of reduction after that. We can then discuss the epistemology level algorithm", "tokens": [264, 3410, 295, 11004, 934, 300, 13, 492, 393, 550, 2248, 264, 2388, 43958, 1793, 1496, 9284], "temperature": 0.0, "avg_logprob": -0.11972240122353159, "compression_ratio": 1.5726872246696035, "no_speech_prob": 0.00011513527715578675}, {"id": 386, "seek": 270440, "start": 2704.4, "end": 2711.04, "text": " for understanding itself. If you are already familiar with these concepts, just check the", "tokens": [337, 3701, 2564, 13, 759, 291, 366, 1217, 4963, 365, 613, 10392, 11, 445, 1520, 264], "temperature": 0.0, "avg_logprob": -0.144115746739399, "compression_ratio": 1.5682819383259912, "no_speech_prob": 7.854723662603647e-05}, {"id": 387, "seek": 270440, "start": 2711.04, "end": 2717.12, "text": " headings and definitions that follow in order to ensure we are using these words roughly", "tokens": [1378, 1109, 293, 21988, 300, 1524, 294, 1668, 281, 5586, 321, 366, 1228, 613, 2283, 9810], "temperature": 0.0, "avg_logprob": -0.144115746739399, "compression_ratio": 1.5682819383259912, "no_speech_prob": 7.854723662603647e-05}, {"id": 388, "seek": 270440, "start": 2717.12, "end": 2724.4, "text": " the way you use them. You may have noticed I write certain, sometimes common words,", "tokens": [264, 636, 291, 764, 552, 13, 509, 815, 362, 5694, 286, 2464, 1629, 11, 2171, 2689, 2283, 11], "temperature": 0.0, "avg_logprob": -0.144115746739399, "compression_ratio": 1.5682819383259912, "no_speech_prob": 7.854723662603647e-05}, {"id": 389, "seek": 270440, "start": 2724.4, "end": 2731.6, "text": " such as model, with an uppercase first letter. This means I am using the word in a technical,", "tokens": [1270, 382, 2316, 11, 365, 364, 11775, 2869, 651, 700, 5063, 13, 639, 1355, 286, 669, 1228, 264, 1349, 294, 257, 6191, 11], "temperature": 0.0, "avg_logprob": -0.144115746739399, "compression_ratio": 1.5682819383259912, "no_speech_prob": 7.854723662603647e-05}, {"id": 390, "seek": 273160, "start": 2731.6, "end": 2738.92, "text": " well-defined, unchanging sense. I will define all such technical terms over time and I will", "tokens": [731, 12, 37716, 11, 517, 27123, 2020, 13, 286, 486, 6964, 439, 1270, 6191, 2115, 670, 565, 293, 286, 486], "temperature": 0.0, "avg_logprob": -0.10964321268015895, "compression_ratio": 1.7361111111111112, "no_speech_prob": 5.796168989036232e-05}, {"id": 391, "seek": 273160, "start": 2738.92, "end": 2746.04, "text": " try not to use these terms until I have defined them. We define 11 such terms in the first", "tokens": [853, 406, 281, 764, 613, 2115, 1826, 286, 362, 7642, 552, 13, 492, 6964, 2975, 1270, 2115, 294, 264, 700], "temperature": 0.0, "avg_logprob": -0.10964321268015895, "compression_ratio": 1.7361111111111112, "no_speech_prob": 5.796168989036232e-05}, {"id": 392, "seek": 273160, "start": 2746.04, "end": 2754.56, "text": " chapter, starting with understanding and reasoning. A dictionary of defined terms is in the works.", "tokens": [7187, 11, 2891, 365, 3701, 293, 21577, 13, 316, 25890, 295, 7642, 2115, 307, 294, 264, 1985, 13], "temperature": 0.0, "avg_logprob": -0.10964321268015895, "compression_ratio": 1.7361111111111112, "no_speech_prob": 5.796168989036232e-05}, {"id": 393, "seek": 273160, "start": 2754.56, "end": 2761.0, "text": " Models are simplifications of reality in epistemology and science. Models are simplifications", "tokens": [6583, 1625, 366, 6883, 7833, 295, 4103, 294, 2388, 43958, 1793, 293, 3497, 13, 6583, 1625, 366, 6883, 7833], "temperature": 0.0, "avg_logprob": -0.10964321268015895, "compression_ratio": 1.7361111111111112, "no_speech_prob": 5.796168989036232e-05}, {"id": 394, "seek": 276100, "start": 2761.0, "end": 2768.64, "text": " of reality. A rich mundane reality is too complex to land itself directly to computation.", "tokens": [295, 4103, 13, 316, 4593, 43497, 4103, 307, 886, 3997, 281, 2117, 2564, 3838, 281, 24903, 13], "temperature": 0.0, "avg_logprob": -0.19122951214130107, "compression_ratio": 1.4673913043478262, "no_speech_prob": 0.00010429003305034712}, {"id": 395, "seek": 276100, "start": 2768.64, "end": 2775.92, "text": " In OTB science fiction shows, we would sometimes hear. And then we fed all the information", "tokens": [682, 38617, 33, 3497, 13266, 3110, 11, 321, 576, 2171, 1568, 13, 400, 550, 321, 4636, 439, 264, 1589], "temperature": 0.0, "avg_logprob": -0.19122951214130107, "compression_ratio": 1.4673913043478262, "no_speech_prob": 0.00010429003305034712}, {"id": 396, "seek": 276100, "start": 2775.92, "end": 2783.4, "text": " into the computer and this is what came out. Well, not anymore. Audiences now know that's", "tokens": [666, 264, 3820, 293, 341, 307, 437, 1361, 484, 13, 1042, 11, 406, 3602, 13, 8821, 14004, 586, 458, 300, 311], "temperature": 0.0, "avg_logprob": -0.19122951214130107, "compression_ratio": 1.4673913043478262, "no_speech_prob": 0.00010429003305034712}, {"id": 397, "seek": 278340, "start": 2783.4, "end": 2792.2000000000003, "text": " not how regular computers work. Consider an automobile. It consists of thousands of parts,", "tokens": [406, 577, 3890, 10807, 589, 13, 17416, 364, 38809, 13, 467, 14689, 295, 5383, 295, 3166, 11], "temperature": 0.0, "avg_logprob": -0.11199135780334472, "compression_ratio": 1.481283422459893, "no_speech_prob": 5.376655826694332e-05}, {"id": 398, "seek": 278340, "start": 2792.2000000000003, "end": 2799.8, "text": " each with properties like materials, size, color, function, and sometimes complex interactions", "tokens": [1184, 365, 7221, 411, 5319, 11, 2744, 11, 2017, 11, 2445, 11, 293, 2171, 3997, 13280], "temperature": 0.0, "avg_logprob": -0.11199135780334472, "compression_ratio": 1.481283422459893, "no_speech_prob": 5.376655826694332e-05}, {"id": 399, "seek": 278340, "start": 2799.8, "end": 2807.36, "text": " with other parts. What's all the information here? We can just feed all of those properties", "tokens": [365, 661, 3166, 13, 708, 311, 439, 264, 1589, 510, 30, 492, 393, 445, 3154, 439, 295, 729, 7221], "temperature": 0.0, "avg_logprob": -0.11199135780334472, "compression_ratio": 1.481283422459893, "no_speech_prob": 5.376655826694332e-05}, {"id": 400, "seek": 280736, "start": 2807.36, "end": 2814.2400000000002, "text": " and measurements and facts into a computer and expect to get an answer. We need to ask", "tokens": [293, 15383, 293, 9130, 666, 257, 3820, 293, 2066, 281, 483, 364, 1867, 13, 492, 643, 281, 1029], "temperature": 0.0, "avg_logprob": -0.09199422387515797, "compression_ratio": 1.6603773584905661, "no_speech_prob": 4.818686284124851e-05}, {"id": 401, "seek": 280736, "start": 2814.2400000000002, "end": 2821.2000000000003, "text": " a question and we also need to simplify the problem so that we can feed in just the facts", "tokens": [257, 1168, 293, 321, 611, 643, 281, 20460, 264, 1154, 370, 300, 321, 393, 3154, 294, 445, 264, 9130], "temperature": 0.0, "avg_logprob": -0.09199422387515797, "compression_ratio": 1.6603773584905661, "no_speech_prob": 4.818686284124851e-05}, {"id": 402, "seek": 280736, "start": 2821.2000000000003, "end": 2827.7200000000003, "text": " or numbers that matter so that our question can be answered with minimum effort. How do", "tokens": [420, 3547, 300, 1871, 370, 300, 527, 1168, 393, 312, 10103, 365, 7285, 4630, 13, 1012, 360], "temperature": 0.0, "avg_logprob": -0.09199422387515797, "compression_ratio": 1.6603773584905661, "no_speech_prob": 4.818686284124851e-05}, {"id": 403, "seek": 280736, "start": 2827.7200000000003, "end": 2835.5, "text": " we do that? We must identify or create, first in our minds, a very simple model of some", "tokens": [321, 360, 300, 30, 492, 1633, 5876, 420, 1884, 11, 700, 294, 527, 9634, 11, 257, 588, 2199, 2316, 295, 512], "temperature": 0.0, "avg_logprob": -0.09199422387515797, "compression_ratio": 1.6603773584905661, "no_speech_prob": 4.818686284124851e-05}, {"id": 404, "seek": 283550, "start": 2835.5, "end": 2842.24, "text": " sort of a generic automobile, and use that model for our computation. After we get the", "tokens": [1333, 295, 257, 19577, 38809, 11, 293, 764, 300, 2316, 337, 527, 24903, 13, 2381, 321, 483, 264], "temperature": 0.0, "avg_logprob": -0.10883893285478864, "compression_ratio": 1.6572769953051643, "no_speech_prob": 8.618291758466512e-05}, {"id": 405, "seek": 283550, "start": 2842.24, "end": 2849.16, "text": " answer for the pure and simple model case, we apply the answer, with some care, back", "tokens": [1867, 337, 264, 6075, 293, 2199, 2316, 1389, 11, 321, 3079, 264, 1867, 11, 365, 512, 1127, 11, 646], "temperature": 0.0, "avg_logprob": -0.10883893285478864, "compression_ratio": 1.6572769953051643, "no_speech_prob": 8.618291758466512e-05}, {"id": 406, "seek": 283550, "start": 2849.16, "end": 2856.4, "text": " to our complex reality where the real automobile and the problem exists. What kind of model", "tokens": [281, 527, 3997, 4103, 689, 264, 957, 38809, 293, 264, 1154, 8198, 13, 708, 733, 295, 2316], "temperature": 0.0, "avg_logprob": -0.10883893285478864, "compression_ratio": 1.6572769953051643, "no_speech_prob": 8.618291758466512e-05}, {"id": 407, "seek": 283550, "start": 2856.4, "end": 2863.52, "text": " we choose depends on our goals. As an example of a model, Newton's second law states that", "tokens": [321, 2826, 5946, 322, 527, 5493, 13, 1018, 364, 1365, 295, 257, 2316, 11, 19541, 311, 1150, 2101, 4368, 300], "temperature": 0.0, "avg_logprob": -0.10883893285478864, "compression_ratio": 1.6572769953051643, "no_speech_prob": 8.618291758466512e-05}, {"id": 408, "seek": 286352, "start": 2863.52, "end": 2871.56, "text": " force equals mass times acceleration, f equals ma. This equation is a classical scientific", "tokens": [3464, 6915, 2758, 1413, 17162, 11, 283, 6915, 463, 13, 639, 5367, 307, 257, 13735, 8134], "temperature": 0.0, "avg_logprob": -0.12489039174626383, "compression_ratio": 1.7242990654205608, "no_speech_prob": 0.00014096977247390896}, {"id": 409, "seek": 286352, "start": 2871.56, "end": 2878.4, "text": " model. If we measure mass and acceleration of a car, then we can estimate how many horsepower", "tokens": [2316, 13, 759, 321, 3481, 2758, 293, 17162, 295, 257, 1032, 11, 550, 321, 393, 12539, 577, 867, 25250], "temperature": 0.0, "avg_logprob": -0.12489039174626383, "compression_ratio": 1.7242990654205608, "no_speech_prob": 0.00014096977247390896}, {"id": 410, "seek": 286352, "start": 2878.4, "end": 2886.36, "text": " the engine has. To use this equation, we engineers would model, in our minds, the car as a single", "tokens": [264, 2848, 575, 13, 1407, 764, 341, 5367, 11, 321, 11955, 576, 2316, 11, 294, 527, 9634, 11, 264, 1032, 382, 257, 2167], "temperature": 0.0, "avg_logprob": -0.12489039174626383, "compression_ratio": 1.7242990654205608, "no_speech_prob": 0.00014096977247390896}, {"id": 411, "seek": 286352, "start": 2886.36, "end": 2893.2, "text": " small point mass with all the mass of the car in that point. Because if we don't, then", "tokens": [1359, 935, 2758, 365, 439, 264, 2758, 295, 264, 1032, 294, 300, 935, 13, 1436, 498, 321, 500, 380, 11, 550], "temperature": 0.0, "avg_logprob": -0.12489039174626383, "compression_ratio": 1.7242990654205608, "no_speech_prob": 0.00014096977247390896}, {"id": 412, "seek": 289320, "start": 2893.2, "end": 2899.4399999999996, "text": " we'd have to worry about the car rotating and other problems. This is how model-based", "tokens": [321, 1116, 362, 281, 3292, 466, 264, 1032, 19627, 293, 661, 2740, 13, 639, 307, 577, 2316, 12, 6032], "temperature": 0.0, "avg_logprob": -0.09524309182468849, "compression_ratio": 1.6073059360730593, "no_speech_prob": 3.3030730264727026e-05}, {"id": 413, "seek": 289320, "start": 2899.4399999999996, "end": 2907.08, "text": " science works. One or more scientists somehow derive a model for some phenomenon. The model", "tokens": [3497, 1985, 13, 1485, 420, 544, 7708, 6063, 28446, 257, 2316, 337, 512, 14029, 13, 440, 2316], "temperature": 0.0, "avg_logprob": -0.09524309182468849, "compression_ratio": 1.6073059360730593, "no_speech_prob": 3.3030730264727026e-05}, {"id": 414, "seek": 289320, "start": 2907.08, "end": 2914.6, "text": " is published as an equation, a formula, or a computer program. Scientists and engineers", "tokens": [307, 6572, 382, 364, 5367, 11, 257, 8513, 11, 420, 257, 3820, 1461, 13, 32958, 293, 11955], "temperature": 0.0, "avg_logprob": -0.09524309182468849, "compression_ratio": 1.6073059360730593, "no_speech_prob": 3.3030730264727026e-05}, {"id": 415, "seek": 289320, "start": 2914.6, "end": 2920.6, "text": " anywhere can now use this equation program model, treating it as a quick shortcut that", "tokens": [4992, 393, 586, 764, 341, 5367, 1461, 2316, 11, 15083, 309, 382, 257, 1702, 24822, 300], "temperature": 0.0, "avg_logprob": -0.09524309182468849, "compression_ratio": 1.6073059360730593, "no_speech_prob": 3.3030730264727026e-05}, {"id": 416, "seek": 292060, "start": 2920.6, "end": 2926.72, "text": " works every time, as long as they have correct input data and are confidently applying the", "tokens": [1985, 633, 565, 11, 382, 938, 382, 436, 362, 3006, 4846, 1412, 293, 366, 41956, 9275, 264], "temperature": 0.0, "avg_logprob": -0.11025945763838918, "compression_ratio": 1.640552995391705, "no_speech_prob": 4.1300412704003975e-05}, {"id": 417, "seek": 292060, "start": 2926.72, "end": 2934.2799999999997, "text": " formula to a suitable problem in their reality. Our greatest invention, model-based problem", "tokens": [8513, 281, 257, 12873, 1154, 294, 641, 4103, 13, 2621, 6636, 22265, 11, 2316, 12, 6032, 1154], "temperature": 0.0, "avg_logprob": -0.11025945763838918, "compression_ratio": 1.640552995391705, "no_speech_prob": 4.1300412704003975e-05}, {"id": 418, "seek": 292060, "start": 2934.2799999999997, "end": 2941.2, "text": " solving, aka reductionism, is the greatest invention our species has ever made. The", "tokens": [12606, 11, 28042, 11004, 1434, 11, 307, 264, 6636, 22265, 527, 6172, 575, 1562, 1027, 13, 440], "temperature": 0.0, "avg_logprob": -0.11025945763838918, "compression_ratio": 1.640552995391705, "no_speech_prob": 4.1300412704003975e-05}, {"id": 419, "seek": 292060, "start": 2941.2, "end": 2947.08, "text": " general strategy of simplifying problems before solving them must be tens of thousands of", "tokens": [2674, 5206, 295, 6883, 5489, 2740, 949, 12606, 552, 1633, 312, 10688, 295, 5383, 295], "temperature": 0.0, "avg_logprob": -0.11025945763838918, "compression_ratio": 1.640552995391705, "no_speech_prob": 4.1300412704003975e-05}, {"id": 420, "seek": 294708, "start": 2947.08, "end": 2954.2799999999997, "text": " years old. In some sense, it is a prerequisite for all other inventions, including the use", "tokens": [924, 1331, 13, 682, 512, 2020, 11, 309, 307, 257, 38333, 34152, 337, 439, 661, 43748, 11, 3009, 264, 764], "temperature": 0.0, "avg_logprob": -0.12011844810398145, "compression_ratio": 1.6108597285067874, "no_speech_prob": 0.00014530224143527448}, {"id": 421, "seek": 294708, "start": 2954.2799999999997, "end": 2961.52, "text": " of fire. If you see a forest fire then you need to first imagine the utility of fire.", "tokens": [295, 2610, 13, 759, 291, 536, 257, 6719, 2610, 550, 291, 643, 281, 700, 3811, 264, 14877, 295, 2610, 13], "temperature": 0.0, "avg_logprob": -0.12011844810398145, "compression_ratio": 1.6108597285067874, "no_speech_prob": 0.00014530224143527448}, {"id": 422, "seek": 294708, "start": 2961.52, "end": 2967.3199999999997, "text": " As a model, before you can figure out that it might be useful to carry home a burning", "tokens": [1018, 257, 2316, 11, 949, 291, 393, 2573, 484, 300, 309, 1062, 312, 4420, 281, 3985, 1280, 257, 9488], "temperature": 0.0, "avg_logprob": -0.12011844810398145, "compression_ratio": 1.6108597285067874, "no_speech_prob": 0.00014530224143527448}, {"id": 423, "seek": 294708, "start": 2967.3199999999997, "end": 2973.84, "text": " branch, we don't think of this problem solving strategy as an invention because it is already", "tokens": [9819, 11, 321, 500, 380, 519, 295, 341, 1154, 12606, 5206, 382, 364, 22265, 570, 309, 307, 1217], "temperature": 0.0, "avg_logprob": -0.12011844810398145, "compression_ratio": 1.6108597285067874, "no_speech_prob": 0.00014530224143527448}, {"id": 424, "seek": 297384, "start": 2973.84, "end": 2980.08, "text": " ubiquitous in our lives. We are all taught how to use model-based problem solving in", "tokens": [43868, 39831, 294, 527, 2909, 13, 492, 366, 439, 5928, 577, 281, 764, 2316, 12, 6032, 1154, 12606, 294], "temperature": 0.0, "avg_logprob": -0.115968409879708, "compression_ratio": 1.579646017699115, "no_speech_prob": 4.844820796279237e-05}, {"id": 425, "seek": 297384, "start": 2980.08, "end": 2986.32, "text": " school when we start solving story problems in math class, but most people never learn", "tokens": [1395, 562, 321, 722, 12606, 1657, 2740, 294, 5221, 1508, 11, 457, 881, 561, 1128, 1466], "temperature": 0.0, "avg_logprob": -0.115968409879708, "compression_ratio": 1.579646017699115, "no_speech_prob": 4.844820796279237e-05}, {"id": 426, "seek": 297384, "start": 2986.32, "end": 2992.6400000000003, "text": " the names of these strategies and are missing the big epistemology level picture. This rarely", "tokens": [264, 5288, 295, 613, 9029, 293, 366, 5361, 264, 955, 2388, 43958, 1793, 1496, 3036, 13, 639, 13752], "temperature": 0.0, "avg_logprob": -0.115968409879708, "compression_ratio": 1.579646017699115, "no_speech_prob": 4.844820796279237e-05}, {"id": 427, "seek": 297384, "start": 2992.6400000000003, "end": 2999.44, "text": " matters until you start working with AI, where lack of an epistemological drowning may lead", "tokens": [7001, 1826, 291, 722, 1364, 365, 7318, 11, 689, 5011, 295, 364, 2388, 43958, 4383, 37198, 815, 1477], "temperature": 0.0, "avg_logprob": -0.115968409879708, "compression_ratio": 1.579646017699115, "no_speech_prob": 4.844820796279237e-05}, {"id": 428, "seek": 299944, "start": 2999.44, "end": 3006.36, "text": " you astray into failing strategies. These little pills are an attempt to remedy that.", "tokens": [291, 5357, 3458, 666, 18223, 9029, 13, 1981, 707, 23871, 366, 364, 5217, 281, 31648, 300, 13], "temperature": 0.0, "avg_logprob": -0.16615236309212697, "compression_ratio": 1.5530973451327434, "no_speech_prob": 3.1461990147363394e-05}, {"id": 429, "seek": 299944, "start": 3006.36, "end": 3013.36, "text": " Model-based methods were examined and refined into scientific methods over the past 450", "tokens": [17105, 12, 6032, 7150, 645, 30972, 293, 26201, 666, 8134, 7150, 670, 264, 1791, 26034], "temperature": 0.0, "avg_logprob": -0.16615236309212697, "compression_ratio": 1.5530973451327434, "no_speech_prob": 3.1461990147363394e-05}, {"id": 430, "seek": 299944, "start": 3013.36, "end": 3019.92, "text": " years. Science is now a collection of thousands of models that taken together allow science", "tokens": [924, 13, 8976, 307, 586, 257, 5765, 295, 5383, 295, 5245, 300, 2726, 1214, 2089, 3497], "temperature": 0.0, "avg_logprob": -0.16615236309212697, "compression_ratio": 1.5530973451327434, "no_speech_prob": 3.1461990147363394e-05}, {"id": 431, "seek": 299944, "start": 3019.92, "end": 3025.84, "text": " competent people to solve problems quickly and efficiently without having to redo all", "tokens": [29998, 561, 281, 5039, 2740, 2661, 293, 19621, 1553, 1419, 281, 29956, 439], "temperature": 0.0, "avg_logprob": -0.16615236309212697, "compression_ratio": 1.5530973451327434, "no_speech_prob": 3.1461990147363394e-05}, {"id": 432, "seek": 302584, "start": 3025.84, "end": 3031.84, "text": " the work that scientists, like Newton, put into creating these models in the first place.", "tokens": [264, 589, 300, 7708, 11, 411, 19541, 11, 829, 666, 4084, 613, 5245, 294, 264, 700, 1081, 13], "temperature": 0.0, "avg_logprob": -0.14891983823078433, "compression_ratio": 1.5682819383259912, "no_speech_prob": 7.005194493103772e-05}, {"id": 433, "seek": 302584, "start": 3031.84, "end": 3038.6400000000003, "text": " And the sum total of those models covers many problems we want to solve scientifically,", "tokens": [400, 264, 2408, 3217, 295, 729, 5245, 10538, 867, 2740, 321, 528, 281, 5039, 39719, 11], "temperature": 0.0, "avg_logprob": -0.14891983823078433, "compression_ratio": 1.5682819383259912, "no_speech_prob": 7.005194493103772e-05}, {"id": 434, "seek": 302584, "start": 3038.6400000000003, "end": 3044.84, "text": " such as how to build a bridge or travel to the moon. This reuse is what makes science", "tokens": [1270, 382, 577, 281, 1322, 257, 7283, 420, 3147, 281, 264, 7135, 13, 639, 26225, 307, 437, 1669, 3497], "temperature": 0.0, "avg_logprob": -0.14891983823078433, "compression_ratio": 1.5682819383259912, "no_speech_prob": 7.005194493103772e-05}, {"id": 435, "seek": 302584, "start": 3044.84, "end": 3052.04, "text": " so effective, but not all sciences can benefit equally from this model-making. It works well", "tokens": [370, 4942, 11, 457, 406, 439, 17677, 393, 5121, 12309, 490, 341, 2316, 12, 12402, 13, 467, 1985, 731], "temperature": 0.0, "avg_logprob": -0.14891983823078433, "compression_ratio": 1.5682819383259912, "no_speech_prob": 7.005194493103772e-05}, {"id": 436, "seek": 305204, "start": 3052.04, "end": 3060.44, "text": " for physics, chemistry, and most of biochemistry. As I'm fond of saying, physics is for simple", "tokens": [337, 10649, 11, 12558, 11, 293, 881, 295, 12198, 48353, 13, 1018, 286, 478, 9557, 295, 1566, 11, 10649, 307, 337, 2199], "temperature": 0.0, "avg_logprob": -0.09031557469140916, "compression_ratio": 1.6966824644549763, "no_speech_prob": 6.268658034969121e-05}, {"id": 437, "seek": 305204, "start": 3060.44, "end": 3066.96, "text": " problems, but as you get to more and more complex sciences, as you get further away", "tokens": [2740, 11, 457, 382, 291, 483, 281, 544, 293, 544, 3997, 17677, 11, 382, 291, 483, 3052, 1314], "temperature": 0.0, "avg_logprob": -0.09031557469140916, "compression_ratio": 1.6966824644549763, "no_speech_prob": 6.268658034969121e-05}, {"id": 438, "seek": 305204, "start": 3066.96, "end": 3074.32, "text": " from physics and closer to life, it gets harder to make decent models. The models used by", "tokens": [490, 10649, 293, 4966, 281, 993, 11, 309, 2170, 6081, 281, 652, 8681, 5245, 13, 440, 5245, 1143, 538], "temperature": 0.0, "avg_logprob": -0.09031557469140916, "compression_ratio": 1.6966824644549763, "no_speech_prob": 6.268658034969121e-05}, {"id": 439, "seek": 305204, "start": 3074.32, "end": 3081.56, "text": " for instance psychology, ecology, physiology, and medicine are generally more complex but", "tokens": [337, 5197, 15105, 11, 39683, 11, 43585, 11, 293, 7195, 366, 5101, 544, 3997, 457], "temperature": 0.0, "avg_logprob": -0.09031557469140916, "compression_ratio": 1.6966824644549763, "no_speech_prob": 6.268658034969121e-05}, {"id": 440, "seek": 308156, "start": 3081.56, "end": 3088.96, "text": " also less powerful than models in physics. Given some solid data, a physicist can compute", "tokens": [611, 1570, 4005, 813, 5245, 294, 10649, 13, 18600, 512, 5100, 1412, 11, 257, 42466, 393, 14722], "temperature": 0.0, "avg_logprob": -0.13420708974202475, "compression_ratio": 1.5502183406113537, "no_speech_prob": 3.830161222140305e-05}, {"id": 441, "seek": 308156, "start": 3088.96, "end": 3094.92, "text": " the mass of the proton to six decimal places, but we would have a harder time predicting", "tokens": [264, 2758, 295, 264, 31728, 281, 2309, 26601, 3190, 11, 457, 321, 576, 362, 257, 6081, 565, 32884], "temperature": 0.0, "avg_logprob": -0.13420708974202475, "compression_ratio": 1.5502183406113537, "no_speech_prob": 3.830161222140305e-05}, {"id": 442, "seek": 308156, "start": 3094.92, "end": 3100.48, "text": " the number of muskrats in New England next summer because that outcome depends on millions", "tokens": [264, 1230, 295, 1038, 74, 25869, 294, 1873, 8196, 958, 4266, 570, 300, 9700, 5946, 322, 6803], "temperature": 0.0, "avg_logprob": -0.13420708974202475, "compression_ratio": 1.5502183406113537, "no_speech_prob": 3.830161222140305e-05}, {"id": 443, "seek": 308156, "start": 3100.48, "end": 3107.4, "text": " of parameters. The life sciences base many of their models on statistics. Statistical", "tokens": [295, 9834, 13, 440, 993, 17677, 3096, 867, 295, 641, 5245, 322, 12523, 13, 16249, 42686], "temperature": 0.0, "avg_logprob": -0.13420708974202475, "compression_ratio": 1.5502183406113537, "no_speech_prob": 3.830161222140305e-05}, {"id": 444, "seek": 310740, "start": 3107.4, "end": 3113.6800000000003, "text": " models are among the weakest models used in science. These statistical models when more", "tokens": [5245, 366, 3654, 264, 44001, 5245, 1143, 294, 3497, 13, 1981, 22820, 5245, 562, 544], "temperature": 0.0, "avg_logprob": -0.2617423576221131, "compression_ratio": 1.590643274853801, "no_speech_prob": 2.3850145225878805e-05}, {"id": 445, "seek": 310740, "start": 3113.6800000000003, "end": 3120.88, "text": " powerful models with better predictive capabilities cannot be used for complexity reasons. Models", "tokens": [4005, 5245, 365, 1101, 35521, 10862, 2644, 312, 1143, 337, 14024, 4112, 13, 6583, 1625], "temperature": 0.0, "avg_logprob": -0.2617423576221131, "compression_ratio": 1.590643274853801, "no_speech_prob": 2.3850145225878805e-05}, {"id": 446, "seek": 310740, "start": 3120.88, "end": 3131.96, "text": " are apothesis, unverified models, scientific theories, models verified by peer review,", "tokens": [366, 1882, 4624, 271, 11, 517, 331, 2587, 5245, 11, 8134, 13667, 11, 5245, 31197, 538, 15108, 3131, 11], "temperature": 0.0, "avg_logprob": -0.2617423576221131, "compression_ratio": 1.590643274853801, "no_speech_prob": 2.3850145225878805e-05}, {"id": 447, "seek": 313196, "start": 3131.96, "end": 3144.6, "text": " equations, formulas, complex scientific models, simulations of climate, weather, etc. Naive", "tokens": [11787, 11, 30546, 11, 3997, 8134, 5245, 11, 35138, 295, 5659, 11, 5503, 11, 5183, 13, 6056, 488], "temperature": 0.0, "avg_logprob": -0.1955730838160361, "compression_ratio": 1.5195530726256983, "no_speech_prob": 6.705621490254998e-05}, {"id": 448, "seek": 313196, "start": 3144.6, "end": 3153.4, "text": " models that we create to simplify our own lives. Computer programs, and what is mathematics?", "tokens": [5245, 300, 321, 1884, 281, 20460, 527, 1065, 2909, 13, 22289, 4268, 11, 293, 437, 307, 18666, 30], "temperature": 0.0, "avg_logprob": -0.1955730838160361, "compression_ratio": 1.5195530726256983, "no_speech_prob": 6.705621490254998e-05}, {"id": 449, "seek": 313196, "start": 3153.4, "end": 3160.7200000000003, "text": " It is a system that allows us to manipulate our models to cover more cases. Mathematics", "tokens": [467, 307, 257, 1185, 300, 4045, 505, 281, 20459, 527, 5245, 281, 2060, 544, 3331, 13, 15776, 37541], "temperature": 0.0, "avg_logprob": -0.1955730838160361, "compression_ratio": 1.5195530726256983, "no_speech_prob": 6.705621490254998e-05}, {"id": 450, "seek": 316072, "start": 3160.72, "end": 3168.72, "text": " is the purest, most context free of all scientific disciplines. As such, its greatest value to", "tokens": [307, 264, 6075, 372, 11, 881, 4319, 1737, 295, 439, 8134, 21919, 13, 1018, 1270, 11, 1080, 6636, 2158, 281], "temperature": 0.0, "avg_logprob": -0.13379808476096705, "compression_ratio": 1.6744186046511629, "no_speech_prob": 8.938927203416824e-05}, {"id": 451, "seek": 316072, "start": 3168.72, "end": 3175.08, "text": " humanity is in its role as a help discipline to all other disciplines. Einstein's famous", "tokens": [10243, 307, 294, 1080, 3090, 382, 257, 854, 13635, 281, 439, 661, 21919, 13, 23486, 311, 4618], "temperature": 0.0, "avg_logprob": -0.13379808476096705, "compression_ratio": 1.6744186046511629, "no_speech_prob": 8.938927203416824e-05}, {"id": 452, "seek": 316072, "start": 3175.08, "end": 3181.4399999999996, "text": " equals MC squared model was derived using mathematical manipulation of other models", "tokens": [6915, 8797, 8889, 2316, 390, 18949, 1228, 18894, 26475, 295, 661, 5245], "temperature": 0.0, "avg_logprob": -0.13379808476096705, "compression_ratio": 1.6744186046511629, "no_speech_prob": 8.938927203416824e-05}, {"id": 453, "seek": 316072, "start": 3181.4399999999996, "end": 3188.24, "text": " known to Einstein at the time. But perhaps mathematics isn't as much a scientific discipline", "tokens": [2570, 281, 23486, 412, 264, 565, 13, 583, 4317, 18666, 1943, 380, 382, 709, 257, 8134, 13635], "temperature": 0.0, "avg_logprob": -0.13379808476096705, "compression_ratio": 1.6744186046511629, "no_speech_prob": 8.938927203416824e-05}, {"id": 454, "seek": 318824, "start": 3188.24, "end": 3196.24, "text": " as an epistemological one. I may explore this aside later. Model use requires understanding.", "tokens": [382, 364, 2388, 43958, 4383, 472, 13, 286, 815, 6839, 341, 7359, 1780, 13, 17105, 764, 7029, 3701, 13], "temperature": 0.0, "avg_logprob": -0.23417691216952558, "compression_ratio": 1.404040404040404, "no_speech_prob": 3.736012877197936e-05}, {"id": 455, "seek": 318824, "start": 3196.24, "end": 3204.0, "text": " A good model is context free, since it maximizes the number of contexts it can be applied in.", "tokens": [316, 665, 2316, 307, 4319, 1737, 11, 1670, 309, 5138, 5660, 264, 1230, 295, 30628, 309, 393, 312, 6456, 294, 13], "temperature": 0.0, "avg_logprob": -0.23417691216952558, "compression_ratio": 1.404040404040404, "no_speech_prob": 3.736012877197936e-05}, {"id": 456, "seek": 318824, "start": 3204.0, "end": 3212.24, "text": " Newton's second law, F equals MA, works pretty much everywhere. We have forces, masses, and", "tokens": [19541, 311, 1150, 2101, 11, 479, 6915, 12191, 11, 1985, 1238, 709, 5315, 13, 492, 362, 5874, 11, 23935, 11, 293], "temperature": 0.0, "avg_logprob": -0.23417691216952558, "compression_ratio": 1.404040404040404, "no_speech_prob": 3.736012877197936e-05}, {"id": 457, "seek": 321224, "start": 3212.24, "end": 3219.2, "text": " accelerations. The trade-off for this flexibility is that we ourselves need to understand the", "tokens": [10172, 763, 13, 440, 4923, 12, 4506, 337, 341, 12635, 307, 300, 321, 4175, 643, 281, 1223, 264], "temperature": 0.0, "avg_logprob": -0.09037603333938954, "compression_ratio": 1.5914893617021277, "no_speech_prob": 0.00010087516420753673}, {"id": 458, "seek": 321224, "start": 3219.2, "end": 3227.8399999999997, "text": " problem domain. In rocket science, when maneuvering in space, F equals MA will often work perfectly,", "tokens": [1154, 9274, 13, 682, 13012, 3497, 11, 562, 25976, 278, 294, 1901, 11, 479, 6915, 12191, 486, 2049, 589, 6239, 11], "temperature": 0.0, "avg_logprob": -0.09037603333938954, "compression_ratio": 1.5914893617021277, "no_speech_prob": 0.00010087516420753673}, {"id": 459, "seek": 321224, "start": 3227.8399999999997, "end": 3233.68, "text": " but when you are applying it to the acceleration of your car, you need to account for lots of", "tokens": [457, 562, 291, 366, 9275, 309, 281, 264, 17162, 295, 428, 1032, 11, 291, 643, 281, 2696, 337, 3195, 295], "temperature": 0.0, "avg_logprob": -0.09037603333938954, "compression_ratio": 1.5914893617021277, "no_speech_prob": 0.00010087516420753673}, {"id": 460, "seek": 321224, "start": 3233.68, "end": 3240.3199999999997, "text": " effects like friction between the road and the wheels, wind resistance, and the like.", "tokens": [5065, 411, 17710, 1296, 264, 3060, 293, 264, 10046, 11, 2468, 7335, 11, 293, 264, 411, 13], "temperature": 0.0, "avg_logprob": -0.09037603333938954, "compression_ratio": 1.5914893617021277, "no_speech_prob": 0.00010087516420753673}, {"id": 461, "seek": 324032, "start": 3240.32, "end": 3248.0800000000004, "text": " So, F equals MA, applied naively would give you the wrong answer if friction is involved.", "tokens": [407, 11, 479, 6915, 12191, 11, 6456, 1667, 3413, 576, 976, 291, 264, 2085, 1867, 498, 17710, 307, 3288, 13], "temperature": 0.0, "avg_logprob": -0.10973251013108241, "compression_ratio": 1.5677966101694916, "no_speech_prob": 4.157218791078776e-05}, {"id": 462, "seek": 324032, "start": 3248.0800000000004, "end": 3254.96, "text": " This demonstrates the main disadvantage with models. They require that both the model maker,", "tokens": [639, 31034, 264, 2135, 24292, 365, 5245, 13, 814, 3651, 300, 1293, 264, 2316, 17127, 11], "temperature": 0.0, "avg_logprob": -0.10973251013108241, "compression_ratio": 1.5677966101694916, "no_speech_prob": 4.157218791078776e-05}, {"id": 463, "seek": 324032, "start": 3254.96, "end": 3261.6800000000003, "text": " scientists like Newton and the model users, STEM competent people everywhere, understand", "tokens": [7708, 411, 19541, 293, 264, 2316, 5022, 11, 25043, 29998, 561, 5315, 11, 1223], "temperature": 0.0, "avg_logprob": -0.10973251013108241, "compression_ratio": 1.5677966101694916, "no_speech_prob": 4.157218791078776e-05}, {"id": 464, "seek": 324032, "start": 3261.6800000000003, "end": 3268.7200000000003, "text": " enough about the problem domain to know whether the model is applicable or not, and how to use it.", "tokens": [1547, 466, 264, 1154, 9274, 281, 458, 1968, 264, 2316, 307, 21142, 420, 406, 11, 293, 577, 281, 764, 309, 13], "temperature": 0.0, "avg_logprob": -0.10973251013108241, "compression_ratio": 1.5677966101694916, "no_speech_prob": 4.157218791078776e-05}, {"id": 465, "seek": 326872, "start": 3268.72, "end": 3274.56, "text": " This understanding is the expensive part of science, since using science requires first", "tokens": [639, 3701, 307, 264, 5124, 644, 295, 3497, 11, 1670, 1228, 3497, 7029, 700], "temperature": 0.0, "avg_logprob": -0.08728794008493423, "compression_ratio": 1.6157894736842104, "no_speech_prob": 5.533987859962508e-05}, {"id": 466, "seek": 326872, "start": 3274.56, "end": 3279.7599999999998, "text": " getting a solid science education in order to avoid mistakes when using models.", "tokens": [1242, 257, 5100, 3497, 3309, 294, 1668, 281, 5042, 8038, 562, 1228, 5245, 13], "temperature": 0.0, "avg_logprob": -0.08728794008493423, "compression_ratio": 1.6157894736842104, "no_speech_prob": 5.533987859962508e-05}, {"id": 467, "seek": 326872, "start": 3280.3999999999996, "end": 3285.9199999999996, "text": " And since models require understanding, they cannot be used to create understanding.", "tokens": [400, 1670, 5245, 3651, 3701, 11, 436, 2644, 312, 1143, 281, 1884, 3701, 13], "temperature": 0.0, "avg_logprob": -0.08728794008493423, "compression_ratio": 1.6157894736842104, "no_speech_prob": 5.533987859962508e-05}, {"id": 468, "seek": 326872, "start": 3286.64, "end": 3292.08, "text": " This is a major problem for AI implementers. Chapter 3", "tokens": [639, 307, 257, 2563, 1154, 337, 7318, 4445, 433, 13, 18874, 805], "temperature": 0.0, "avg_logprob": -0.08728794008493423, "compression_ratio": 1.6157894736842104, "no_speech_prob": 5.533987859962508e-05}, {"id": 469, "seek": 329208, "start": 3292.08, "end": 3297.12, "text": " 2 Dirty Words Reductionism is the use of models.", "tokens": [568, 413, 9340, 32857, 4477, 27549, 1434, 307, 264, 764, 295, 5245, 13], "temperature": 0.0, "avg_logprob": -0.18738849742992505, "compression_ratio": 1.5621621621621622, "no_speech_prob": 0.00021832658967468888}, {"id": 470, "seek": 329208, "start": 3297.84, "end": 3304.24, "text": " Holism is the avoidance of models. Matters are scientific models, theories,", "tokens": [11086, 1434, 307, 264, 5042, 719, 295, 5245, 13, 20285, 82, 366, 8134, 5245, 11, 13667, 11], "temperature": 0.0, "avg_logprob": -0.18738849742992505, "compression_ratio": 1.5621621621621622, "no_speech_prob": 0.00021832658967468888}, {"id": 471, "seek": 329208, "start": 3304.24, "end": 3311.7599999999998, "text": " hypotheses, formulas, equations, superstitions, and most computer programs.", "tokens": [49969, 11, 30546, 11, 11787, 11, 29423, 2451, 11, 293, 881, 3820, 4268, 13], "temperature": 0.0, "avg_logprob": -0.18738849742992505, "compression_ratio": 1.5621621621621622, "no_speech_prob": 0.00021832658967468888}, {"id": 472, "seek": 329208, "start": 3313.04, "end": 3320.0, "text": " Reductionism and Holism. After having sorted out what models are, we can now discuss two", "tokens": [4477, 27549, 1434, 293, 11086, 1434, 13, 2381, 1419, 25462, 484, 437, 5245, 366, 11, 321, 393, 586, 2248, 732], "temperature": 0.0, "avg_logprob": -0.18738849742992505, "compression_ratio": 1.5621621621621622, "no_speech_prob": 0.00021832658967468888}, {"id": 473, "seek": 332000, "start": 3320.0, "end": 3327.28, "text": " complementary problem-solving strategies, or perhaps meta-strategies. There are in many ways", "tokens": [40705, 1154, 12, 30926, 798, 9029, 11, 420, 4317, 19616, 12, 9733, 2968, 530, 13, 821, 366, 294, 867, 2098], "temperature": 0.0, "avg_logprob": -0.0815060859502748, "compression_ratio": 1.625531914893617, "no_speech_prob": 4.31858679803554e-05}, {"id": 474, "seek": 332000, "start": 3327.28, "end": 3333.36, "text": " each other's opposites, but the classification can become an argument about novel levels and", "tokens": [1184, 661, 311, 4665, 3324, 11, 457, 264, 21538, 393, 1813, 364, 6770, 466, 7613, 4358, 293], "temperature": 0.0, "avg_logprob": -0.0815060859502748, "compression_ratio": 1.625531914893617, "no_speech_prob": 4.31858679803554e-05}, {"id": 475, "seek": 332000, "start": 3333.36, "end": 3340.56, "text": " definitions. I will initially pretend the division is clear and obvious, and will elaborate later.", "tokens": [21988, 13, 286, 486, 9105, 11865, 264, 10044, 307, 1850, 293, 6322, 11, 293, 486, 20945, 1780, 13], "temperature": 0.0, "avg_logprob": -0.0815060859502748, "compression_ratio": 1.625531914893617, "no_speech_prob": 4.31858679803554e-05}, {"id": 476, "seek": 332000, "start": 3341.2, "end": 3348.32, "text": " Reductionism is the use of models. In this series we will use exactly the above definition of the", "tokens": [4477, 27549, 1434, 307, 264, 764, 295, 5245, 13, 682, 341, 2638, 321, 486, 764, 2293, 264, 3673, 7123, 295, 264], "temperature": 0.0, "avg_logprob": -0.0815060859502748, "compression_ratio": 1.625531914893617, "no_speech_prob": 4.31858679803554e-05}, {"id": 477, "seek": 334832, "start": 3348.32, "end": 3355.6000000000004, "text": " word, reductionism. If you look up the definition elsewhere you may find that some sources divide", "tokens": [1349, 11, 11004, 1434, 13, 759, 291, 574, 493, 264, 7123, 14517, 291, 815, 915, 300, 512, 7139, 9845], "temperature": 0.0, "avg_logprob": -0.08767505125565962, "compression_ratio": 1.625, "no_speech_prob": 7.002339407335967e-05}, {"id": 478, "seek": 334832, "start": 3355.6000000000004, "end": 3362.6400000000003, "text": " the strategy into sub-strategies. They also seem to miss the most important sub-strategy,", "tokens": [264, 5206, 666, 1422, 12, 9733, 2968, 530, 13, 814, 611, 1643, 281, 1713, 264, 881, 1021, 1422, 12, 372, 37464, 11], "temperature": 0.0, "avg_logprob": -0.08767505125565962, "compression_ratio": 1.625, "no_speech_prob": 7.002339407335967e-05}, {"id": 479, "seek": 334832, "start": 3362.6400000000003, "end": 3368.7200000000003, "text": " which we'll discuss later. But what all these sub-strategies have in common is that they all", "tokens": [597, 321, 603, 2248, 1780, 13, 583, 437, 439, 613, 1422, 12, 9733, 2968, 530, 362, 294, 2689, 307, 300, 436, 439], "temperature": 0.0, "avg_logprob": -0.08767505125565962, "compression_ratio": 1.625, "no_speech_prob": 7.002339407335967e-05}, {"id": 480, "seek": 334832, "start": 3368.7200000000003, "end": 3375.44, "text": " provide ways to simplify observations of fragments of our rich mundane reality into much simpler", "tokens": [2893, 2098, 281, 20460, 18163, 295, 29197, 295, 527, 4593, 43497, 4103, 666, 709, 18587], "temperature": 0.0, "avg_logprob": -0.08767505125565962, "compression_ratio": 1.625, "no_speech_prob": 7.002339407335967e-05}, {"id": 481, "seek": 337544, "start": 3375.44, "end": 3383.84, "text": " models, which we use for reasoning, computation, and sharing. Reductionism is so central to how", "tokens": [5245, 11, 597, 321, 764, 337, 21577, 11, 24903, 11, 293, 5414, 13, 4477, 27549, 1434, 307, 370, 5777, 281, 577], "temperature": 0.0, "avg_logprob": -0.07950066818910487, "compression_ratio": 1.5706214689265536, "no_speech_prob": 6.622759974561632e-05}, {"id": 482, "seek": 337544, "start": 3383.84, "end": 3391.44, "text": " we do science, the heavy reliance on models, such as theories, equations and formulas,", "tokens": [321, 360, 3497, 11, 264, 4676, 1039, 6276, 322, 5245, 11, 1270, 382, 13667, 11, 11787, 293, 30546, 11], "temperature": 0.0, "avg_logprob": -0.07950066818910487, "compression_ratio": 1.5706214689265536, "no_speech_prob": 6.622759974561632e-05}, {"id": 483, "seek": 337544, "start": 3391.44, "end": 3401.04, "text": " and physics, chemistry, etc. That we can speak of model-based sciences or reductionist sciences", "tokens": [293, 10649, 11, 12558, 11, 5183, 13, 663, 321, 393, 1710, 295, 2316, 12, 6032, 17677, 420, 11004, 468, 17677], "temperature": 0.0, "avg_logprob": -0.07950066818910487, "compression_ratio": 1.5706214689265536, "no_speech_prob": 6.622759974561632e-05}, {"id": 484, "seek": 340104, "start": 3401.04, "end": 3408.32, "text": " where such model-making is easy and effective, and this classification excludes those sciences,", "tokens": [689, 1270, 2316, 12, 12402, 307, 1858, 293, 4942, 11, 293, 341, 21538, 16269, 279, 729, 17677, 11], "temperature": 0.0, "avg_logprob": -0.058932212682870716, "compression_ratio": 1.6383928571428572, "no_speech_prob": 3.480337545624934e-05}, {"id": 485, "seek": 340104, "start": 3408.32, "end": 3414.48, "text": " like psychology, where such model-making is difficult and less often rewarded with reliable", "tokens": [411, 15105, 11, 689, 1270, 2316, 12, 12402, 307, 2252, 293, 1570, 2049, 29105, 365, 12924], "temperature": 0.0, "avg_logprob": -0.058932212682870716, "compression_ratio": 1.6383928571428572, "no_speech_prob": 3.480337545624934e-05}, {"id": 486, "seek": 340104, "start": 3414.48, "end": 3421.7599999999998, "text": " results. After considering all the advantages of models we might wonder why we even bother", "tokens": [3542, 13, 2381, 8079, 439, 264, 14906, 295, 5245, 321, 1062, 2441, 983, 321, 754, 8677], "temperature": 0.0, "avg_logprob": -0.058932212682870716, "compression_ratio": 1.6383928571428572, "no_speech_prob": 3.480337545624934e-05}, {"id": 487, "seek": 340104, "start": 3421.7599999999998, "end": 3429.36, "text": " discussing it. Too many people, especially those with a solid stem, science, technology,", "tokens": [10850, 309, 13, 11395, 867, 561, 11, 2318, 729, 365, 257, 5100, 12312, 11, 3497, 11, 2899, 11], "temperature": 0.0, "avg_logprob": -0.058932212682870716, "compression_ratio": 1.6383928571428572, "no_speech_prob": 3.480337545624934e-05}, {"id": 488, "seek": 342936, "start": 3429.36, "end": 3435.36, "text": " engineering, and mathematics education, it may well look like the only choice,", "tokens": [7043, 11, 293, 18666, 3309, 11, 309, 815, 731, 574, 411, 264, 787, 3922, 11], "temperature": 0.0, "avg_logprob": -0.09009452563960378, "compression_ratio": 1.640552995391705, "no_speech_prob": 4.4988180889049545e-05}, {"id": 489, "seek": 342936, "start": 3436.08, "end": 3443.04, "text": " but there's also the other strategy. Homism is the avoidance of models. This is where the", "tokens": [457, 456, 311, 611, 264, 661, 5206, 13, 20903, 1434, 307, 264, 5042, 719, 295, 5245, 13, 639, 307, 689, 264], "temperature": 0.0, "avg_logprob": -0.09009452563960378, "compression_ratio": 1.640552995391705, "no_speech_prob": 4.4988180889049545e-05}, {"id": 490, "seek": 342936, "start": 3443.04, "end": 3450.0, "text": " questions start. This is where the paradox is surface. This is where your worldview may get", "tokens": [1651, 722, 13, 639, 307, 689, 264, 26221, 307, 3753, 13, 639, 307, 689, 428, 41141, 815, 483], "temperature": 0.0, "avg_logprob": -0.09009452563960378, "compression_ratio": 1.640552995391705, "no_speech_prob": 4.4988180889049545e-05}, {"id": 491, "seek": 342936, "start": 3450.0, "end": 3458.0, "text": " shaken up. Seriously, especially if you are a scientist or engineer with a solid stem education", "tokens": [40971, 493, 13, 14063, 11, 2318, 498, 291, 366, 257, 12662, 420, 11403, 365, 257, 5100, 12312, 3309], "temperature": 0.0, "avg_logprob": -0.09009452563960378, "compression_ratio": 1.640552995391705, "no_speech_prob": 4.4988180889049545e-05}, {"id": 492, "seek": 345800, "start": 3458.0, "end": 3466.24, "text": " and decades of professional success using science and models. In some sense, the goal of this entire", "tokens": [293, 7878, 295, 4843, 2245, 1228, 3497, 293, 5245, 13, 682, 512, 2020, 11, 264, 3387, 295, 341, 2302], "temperature": 0.0, "avg_logprob": -0.0538670413465385, "compression_ratio": 1.6696428571428572, "no_speech_prob": 7.26499711163342e-05}, {"id": 493, "seek": 345800, "start": 3466.24, "end": 3472.08, "text": " series is to demonstrate that we need to use both problem-solving strategies when creating", "tokens": [2638, 307, 281, 11698, 300, 321, 643, 281, 764, 1293, 1154, 12, 30926, 798, 9029, 562, 4084], "temperature": 0.0, "avg_logprob": -0.0538670413465385, "compression_ratio": 1.6696428571428572, "no_speech_prob": 7.26499711163342e-05}, {"id": 494, "seek": 345800, "start": 3472.08, "end": 3478.64, "text": " our artificial intelligences, because that is what it is going to take. We need holistic", "tokens": [527, 11677, 5613, 2667, 11, 570, 300, 307, 437, 309, 307, 516, 281, 747, 13, 492, 643, 30334], "temperature": 0.0, "avg_logprob": -0.0538670413465385, "compression_ratio": 1.6696428571428572, "no_speech_prob": 7.26499711163342e-05}, {"id": 495, "seek": 345800, "start": 3478.64, "end": 3485.76, "text": " understanding. We established that in the first chapter, as a sample of the new ideas that we", "tokens": [3701, 13, 492, 7545, 300, 294, 264, 700, 7187, 11, 382, 257, 6889, 295, 264, 777, 3487, 300, 321], "temperature": 0.0, "avg_logprob": -0.0538670413465385, "compression_ratio": 1.6696428571428572, "no_speech_prob": 7.26499711163342e-05}, {"id": 496, "seek": 348576, "start": 3485.76, "end": 3494.48, "text": " will have to deal with I will just mention, reasoning is reductionist. Understanding is holistic.", "tokens": [486, 362, 281, 2028, 365, 286, 486, 445, 2152, 11, 21577, 307, 11004, 468, 13, 36858, 307, 30334, 13], "temperature": 0.0, "avg_logprob": -0.10861043606774282, "compression_ratio": 1.5864197530864197, "no_speech_prob": 0.0001363566261716187}, {"id": 497, "seek": 348576, "start": 3495.76, "end": 3503.1200000000003, "text": " Newer networks are holistic. Holistic systems can jump to conclusions on scant evidence.", "tokens": [1873, 260, 9590, 366, 30334, 13, 11086, 3142, 3652, 393, 3012, 281, 22865, 322, 795, 394, 4467, 13], "temperature": 0.0, "avg_logprob": -0.10861043606774282, "compression_ratio": 1.5864197530864197, "no_speech_prob": 0.0001363566261716187}, {"id": 498, "seek": 348576, "start": 3504.4, "end": 3508.96, "text": " Holistic systems can themselves know what is important and what isn't.", "tokens": [11086, 3142, 3652, 393, 2969, 458, 437, 307, 1021, 293, 437, 1943, 380, 13], "temperature": 0.0, "avg_logprob": -0.10861043606774282, "compression_ratio": 1.5864197530864197, "no_speech_prob": 0.0001363566261716187}, {"id": 499, "seek": 350896, "start": 3508.96, "end": 3516.7200000000003, "text": " Holistic systems can solve problems we ourselves cannot or don't care to understand.", "tokens": [11086, 3142, 3652, 393, 5039, 2740, 321, 4175, 2644, 420, 500, 380, 1127, 281, 1223, 13], "temperature": 0.0, "avg_logprob": -0.18465158972941653, "compression_ratio": 1.8674698795180722, "no_speech_prob": 9.535118442727253e-05}, {"id": 500, "seek": 350896, "start": 3516.7200000000003, "end": 3525.36, "text": " Holistic systems are model-free. We do not use any a priori models of any problem domain.", "tokens": [11086, 3142, 3652, 366, 2316, 12, 10792, 13, 492, 360, 406, 764, 604, 257, 4059, 72, 5245, 295, 604, 1154, 9274, 13], "temperature": 0.0, "avg_logprob": -0.18465158972941653, "compression_ratio": 1.8674698795180722, "no_speech_prob": 9.535118442727253e-05}, {"id": 501, "seek": 350896, "start": 3526.56, "end": 3531.2, "text": " Reasoning systems inherit all problems and benefits of reductionism.", "tokens": [39693, 278, 3652, 21389, 439, 2740, 293, 5311, 295, 11004, 1434, 13], "temperature": 0.0, "avg_logprob": -0.18465158972941653, "compression_ratio": 1.8674698795180722, "no_speech_prob": 9.535118442727253e-05}, {"id": 502, "seek": 350896, "start": 3532.48, "end": 3537.12, "text": " Understanding systems inherit all problems and benefits of holism.", "tokens": [36858, 3652, 21389, 439, 2740, 293, 5311, 295, 4091, 1434, 13], "temperature": 0.0, "avg_logprob": -0.18465158972941653, "compression_ratio": 1.8674698795180722, "no_speech_prob": 9.535118442727253e-05}, {"id": 503, "seek": 353712, "start": 3537.12, "end": 3544.0, "text": " Humans are born holistic. Humans each solve thousands of little", "tokens": [35809, 366, 4232, 30334, 13, 35809, 1184, 5039, 5383, 295, 707], "temperature": 0.0, "avg_logprob": -0.12015280516251274, "compression_ratio": 1.6119402985074627, "no_speech_prob": 7.035883754724637e-05}, {"id": 504, "seek": 353712, "start": 3544.0, "end": 3551.8399999999997, "text": " problems every day, and we are solving almost all these problems holistically, using understanding,", "tokens": [2740, 633, 786, 11, 293, 321, 366, 12606, 1920, 439, 613, 2740, 4091, 20458, 11, 1228, 3701, 11], "temperature": 0.0, "avg_logprob": -0.12015280516251274, "compression_ratio": 1.6119402985074627, "no_speech_prob": 7.035883754724637e-05}, {"id": 505, "seek": 353712, "start": 3551.8399999999997, "end": 3556.96, "text": " and without a need to reason at all. This includes fluent language use.", "tokens": [293, 1553, 257, 643, 281, 1778, 412, 439, 13, 639, 5974, 40799, 2856, 764, 13], "temperature": 0.0, "avg_logprob": -0.12015280516251274, "compression_ratio": 1.6119402985074627, "no_speech_prob": 7.035883754724637e-05}, {"id": 506, "seek": 353712, "start": 3558.16, "end": 3564.0, "text": " A stem education instills a strict reductionist discipline in order to mitigate problems", "tokens": [316, 12312, 3309, 1058, 2565, 257, 10910, 11004, 468, 13635, 294, 1668, 281, 27336, 2740], "temperature": 0.0, "avg_logprob": -0.12015280516251274, "compression_ratio": 1.6119402985074627, "no_speech_prob": 7.035883754724637e-05}, {"id": 507, "seek": 356400, "start": 3564.0, "end": 3570.0, "text": " with fallibility of holistic human minds. Our intelligences are fallible.", "tokens": [365, 2100, 2841, 295, 30334, 1952, 9634, 13, 2621, 5613, 2667, 366, 2100, 964, 13], "temperature": 0.0, "avg_logprob": -0.1008435151515863, "compression_ratio": 1.5772727272727274, "no_speech_prob": 6.542080518556759e-05}, {"id": 508, "seek": 356400, "start": 3570.8, "end": 3576.88, "text": " These claims all deserve individual treatments, and we'll get to all of them in later sections.", "tokens": [1981, 9441, 439, 9948, 2609, 15795, 11, 293, 321, 603, 483, 281, 439, 295, 552, 294, 1780, 10863, 13], "temperature": 0.0, "avg_logprob": -0.1008435151515863, "compression_ratio": 1.5772727272727274, "no_speech_prob": 6.542080518556759e-05}, {"id": 509, "seek": 356400, "start": 3577.6, "end": 3583.36, "text": " But the major theme is clear. Humans are mainly holistic problem solvers.", "tokens": [583, 264, 2563, 6314, 307, 1850, 13, 35809, 366, 8704, 30334, 1154, 1404, 840, 13], "temperature": 0.0, "avg_logprob": -0.1008435151515863, "compression_ratio": 1.5772727272727274, "no_speech_prob": 6.542080518556759e-05}, {"id": 510, "seek": 356400, "start": 3584.0, "end": 3591.84, "text": " This must be true for our artificial intelligences. We had several reasons for focusing on reductionist", "tokens": [639, 1633, 312, 2074, 337, 527, 11677, 5613, 2667, 13, 492, 632, 2940, 4112, 337, 8416, 322, 11004, 468], "temperature": 0.0, "avg_logprob": -0.1008435151515863, "compression_ratio": 1.5772727272727274, "no_speech_prob": 6.542080518556759e-05}, {"id": 511, "seek": 359184, "start": 3591.84, "end": 3599.28, "text": " methods, models, and reasoning during the first 60 years of AI. Our computers were too small to", "tokens": [7150, 11, 5245, 11, 293, 21577, 1830, 264, 700, 4060, 924, 295, 7318, 13, 2621, 10807, 645, 886, 1359, 281], "temperature": 0.0, "avg_logprob": -0.05869612061833761, "compression_ratio": 1.6111111111111112, "no_speech_prob": 3.642205774667673e-05}, {"id": 512, "seek": 359184, "start": 3599.28, "end": 3607.2000000000003, "text": " make neural networks work at all. But there were also ideological reasons. AI was born out of the", "tokens": [652, 18161, 9590, 589, 412, 439, 13, 583, 456, 645, 611, 35341, 4112, 13, 7318, 390, 4232, 484, 295, 264], "temperature": 0.0, "avg_logprob": -0.05869612061833761, "compression_ratio": 1.6111111111111112, "no_speech_prob": 3.642205774667673e-05}, {"id": 513, "seek": 359184, "start": 3607.2000000000003, "end": 3613.52, "text": " math and computer science departments of our universities, and therefore most of the people", "tokens": [5221, 293, 3820, 3497, 15326, 295, 527, 11779, 11, 293, 4412, 881, 295, 264, 561], "temperature": 0.0, "avg_logprob": -0.05869612061833761, "compression_ratio": 1.6111111111111112, "no_speech_prob": 3.642205774667673e-05}, {"id": 514, "seek": 359184, "start": 3613.52, "end": 3619.44, "text": " working on AI were solidly oriented towards the goal of creating a logic-based reductionist", "tokens": [1364, 322, 7318, 645, 5100, 356, 21841, 3030, 264, 3387, 295, 4084, 257, 9952, 12, 6032, 11004, 468], "temperature": 0.0, "avg_logprob": -0.05869612061833761, "compression_ratio": 1.6111111111111112, "no_speech_prob": 3.642205774667673e-05}, {"id": 515, "seek": 361944, "start": 3619.44, "end": 3626.64, "text": " infallible artificial mind. To build early AIs, like expert systems, we entered rules", "tokens": [1536, 336, 964, 11677, 1575, 13, 1407, 1322, 2440, 316, 6802, 11, 411, 5844, 3652, 11, 321, 9065, 4474], "temperature": 0.0, "avg_logprob": -0.0957108906337193, "compression_ratio": 1.5145228215767634, "no_speech_prob": 4.228143734508194e-05}, {"id": 516, "seek": 361944, "start": 3626.64, "end": 3633.52, "text": " or programmed in lots of facts to reason about. But this was budding reductionist castles in the", "tokens": [420, 31092, 294, 3195, 295, 9130, 281, 1778, 466, 13, 583, 341, 390, 3265, 3584, 11004, 468, 4193, 904, 294, 264], "temperature": 0.0, "avg_logprob": -0.0957108906337193, "compression_ratio": 1.5145228215767634, "no_speech_prob": 4.228143734508194e-05}, {"id": 517, "seek": 361944, "start": 3633.52, "end": 3640.7200000000003, "text": " air, comprised of unanchored facts that didn't tie to any understanding whatsoever. The troubles", "tokens": [1988, 11, 38062, 295, 517, 4778, 2769, 9130, 300, 994, 380, 7582, 281, 604, 3701, 17076, 13, 440, 15379], "temperature": 0.0, "avg_logprob": -0.0957108906337193, "compression_ratio": 1.5145228215767634, "no_speech_prob": 4.228143734508194e-05}, {"id": 518, "seek": 361944, "start": 3640.7200000000003, "end": 3647.28, "text": " with classical AI, such as bitterness, the tendency to make spectacular and expensive", "tokens": [365, 13735, 7318, 11, 1270, 382, 44224, 11, 264, 18187, 281, 652, 18149, 293, 5124], "temperature": 0.0, "avg_logprob": -0.0957108906337193, "compression_ratio": 1.5145228215767634, "no_speech_prob": 4.228143734508194e-05}, {"id": 519, "seek": 364728, "start": 3647.28, "end": 3653.52, "text": " mistakes at the edges of their competence, can be directly traced to the lack of foundational", "tokens": [8038, 412, 264, 8819, 295, 641, 39965, 11, 393, 312, 3838, 38141, 281, 264, 5011, 295, 32195], "temperature": 0.0, "avg_logprob": -0.0693868166440493, "compression_ratio": 1.7463414634146341, "no_speech_prob": 4.415252624312416e-05}, {"id": 520, "seek": 364728, "start": 3653.52, "end": 3659.36, "text": " understanding to support these attempts at reasoning. Understanding machines will not", "tokens": [3701, 281, 1406, 613, 15257, 412, 21577, 13, 36858, 8379, 486, 406], "temperature": 0.0, "avg_logprob": -0.0693868166440493, "compression_ratio": 1.7463414634146341, "no_speech_prob": 4.415252624312416e-05}, {"id": 521, "seek": 364728, "start": 3659.36, "end": 3665.1200000000003, "text": " suffer from this brittleness, but will fail gracefully at the edges of their competence,", "tokens": [9753, 490, 341, 738, 593, 45887, 11, 457, 486, 3061, 10042, 2277, 412, 264, 8819, 295, 641, 39965, 11], "temperature": 0.0, "avg_logprob": -0.0693868166440493, "compression_ratio": 1.7463414634146341, "no_speech_prob": 4.415252624312416e-05}, {"id": 522, "seek": 364728, "start": 3665.1200000000003, "end": 3672.2400000000002, "text": " much like humans. Most of the time they will know the answer beyond that they will guess,", "tokens": [709, 411, 6255, 13, 4534, 295, 264, 565, 436, 486, 458, 264, 1867, 4399, 300, 436, 486, 2041, 11], "temperature": 0.0, "avg_logprob": -0.0693868166440493, "compression_ratio": 1.7463414634146341, "no_speech_prob": 4.415252624312416e-05}, {"id": 523, "seek": 367224, "start": 3672.24, "end": 3678.56, "text": " and the guesses they make are based on a lifetime of experience, gained through learning from a", "tokens": [293, 264, 42703, 436, 652, 366, 2361, 322, 257, 11364, 295, 1752, 11, 12634, 807, 2539, 490, 257], "temperature": 0.0, "avg_logprob": -0.0861971208027431, "compression_ratio": 1.6311111111111112, "no_speech_prob": 6.047928400221281e-05}, {"id": 524, "seek": 367224, "start": 3678.56, "end": 3685.2799999999997, "text": " large corpus and so they have a good chance of being at least a workable choice, if not perfect.", "tokens": [2416, 1181, 31624, 293, 370, 436, 362, 257, 665, 2931, 295, 885, 412, 1935, 257, 589, 712, 3922, 11, 498, 406, 2176, 13], "temperature": 0.0, "avg_logprob": -0.0861971208027431, "compression_ratio": 1.6311111111111112, "no_speech_prob": 6.047928400221281e-05}, {"id": 525, "seek": 367224, "start": 3685.9199999999996, "end": 3692.64, "text": " How can anyone solve problems without using models? A lot of people coming from a STEM", "tokens": [1012, 393, 2878, 5039, 2740, 1553, 1228, 5245, 30, 316, 688, 295, 561, 1348, 490, 257, 25043], "temperature": 0.0, "avg_logprob": -0.0861971208027431, "compression_ratio": 1.6311111111111112, "no_speech_prob": 6.047928400221281e-05}, {"id": 526, "seek": 367224, "start": 3692.64, "end": 3698.9599999999996, "text": " background cannot even imagine how to solve problems without using models. But it's not", "tokens": [3678, 2644, 754, 3811, 577, 281, 5039, 2740, 1553, 1228, 5245, 13, 583, 309, 311, 406], "temperature": 0.0, "avg_logprob": -0.0861971208027431, "compression_ratio": 1.6311111111111112, "no_speech_prob": 6.047928400221281e-05}, {"id": 527, "seek": 369896, "start": 3698.96, "end": 3705.76, "text": " hard, once you understand the difference, mostly it's a matter of doing what worked last time.", "tokens": [1152, 11, 1564, 291, 1223, 264, 2649, 11, 5240, 309, 311, 257, 1871, 295, 884, 437, 2732, 1036, 565, 13], "temperature": 0.0, "avg_logprob": -0.08054533223996217, "compression_ratio": 1.6059322033898304, "no_speech_prob": 0.00010459290206199512}, {"id": 528, "seek": 369896, "start": 3706.48, "end": 3712.4, "text": " The problem is now figuring out whether we are in a situation that's similar enough that it will", "tokens": [440, 1154, 307, 586, 15213, 484, 1968, 321, 366, 294, 257, 2590, 300, 311, 2531, 1547, 300, 309, 486], "temperature": 0.0, "avg_logprob": -0.08054533223996217, "compression_ratio": 1.6059322033898304, "no_speech_prob": 0.00010459290206199512}, {"id": 529, "seek": 369896, "start": 3712.4, "end": 3720.4, "text": " work again. This is mostly a pattern matching problem. More later, what's the result? The", "tokens": [589, 797, 13, 639, 307, 5240, 257, 5102, 14324, 1154, 13, 5048, 1780, 11, 437, 311, 264, 1874, 30, 440], "temperature": 0.0, "avg_logprob": -0.08054533223996217, "compression_ratio": 1.6059322033898304, "no_speech_prob": 0.00010459290206199512}, {"id": 530, "seek": 369896, "start": 3720.4, "end": 3727.12, "text": " holistic answer is a quick guess at the best action, based on experience with similar situations.", "tokens": [30334, 1867, 307, 257, 1702, 2041, 412, 264, 1151, 3069, 11, 2361, 322, 1752, 365, 2531, 6851, 13], "temperature": 0.0, "avg_logprob": -0.08054533223996217, "compression_ratio": 1.6059322033898304, "no_speech_prob": 0.00010459290206199512}, {"id": 531, "seek": 372712, "start": 3727.12, "end": 3733.3599999999997, "text": " Most of the time it's correct, sometimes it's a little wrong, and every now and then,", "tokens": [4534, 295, 264, 565, 309, 311, 3006, 11, 2171, 309, 311, 257, 707, 2085, 11, 293, 633, 586, 293, 550, 11], "temperature": 0.0, "avg_logprob": -0.12653144923123447, "compression_ratio": 1.7129186602870814, "no_speech_prob": 0.00015545389032922685}, {"id": 532, "seek": 372712, "start": 3733.3599999999997, "end": 3740.08, "text": " there's a noticeable mistake. And if we get things a little wrong, we may notice the outcome", "tokens": [456, 311, 257, 26041, 6146, 13, 400, 498, 321, 483, 721, 257, 707, 2085, 11, 321, 815, 3449, 264, 9700], "temperature": 0.0, "avg_logprob": -0.12653144923123447, "compression_ratio": 1.7129186602870814, "no_speech_prob": 0.00015545389032922685}, {"id": 533, "seek": 372712, "start": 3740.08, "end": 3746.56, "text": " and correct the action. We learn from our mistakes. If we practice something a lot,", "tokens": [293, 3006, 264, 3069, 13, 492, 1466, 490, 527, 8038, 13, 759, 321, 3124, 746, 257, 688, 11], "temperature": 0.0, "avg_logprob": -0.12653144923123447, "compression_ratio": 1.7129186602870814, "no_speech_prob": 0.00015545389032922685}, {"id": 534, "seek": 372712, "start": 3746.56, "end": 3753.52, "text": " we will start doing it effectively and perfectly every time. Do we learn faster if we make more", "tokens": [321, 486, 722, 884, 309, 8659, 293, 6239, 633, 565, 13, 1144, 321, 1466, 4663, 498, 321, 652, 544], "temperature": 0.0, "avg_logprob": -0.12653144923123447, "compression_ratio": 1.7129186602870814, "no_speech_prob": 0.00015545389032922685}, {"id": 535, "seek": 375352, "start": 3753.52, "end": 3761.92, "text": " mistakes? Should we make mistakes on purpose? More later, in situations where you cannot use", "tokens": [8038, 30, 6454, 321, 652, 8038, 322, 4334, 30, 5048, 1780, 11, 294, 6851, 689, 291, 2644, 764], "temperature": 0.0, "avg_logprob": -0.08340196333069733, "compression_ratio": 1.4545454545454546, "no_speech_prob": 4.380390964797698e-05}, {"id": 536, "seek": 375352, "start": 3761.92, "end": 3768.8, "text": " models, which are more common than many realize, the holistic guess may also be your only option.", "tokens": [5245, 11, 597, 366, 544, 2689, 813, 867, 4325, 11, 264, 30334, 2041, 815, 611, 312, 428, 787, 3614, 13], "temperature": 0.0, "avg_logprob": -0.08340196333069733, "compression_ratio": 1.4545454545454546, "no_speech_prob": 4.380390964797698e-05}, {"id": 537, "seek": 375352, "start": 3769.6, "end": 3777.84, "text": " Conversely, if you have an adequately well-working model-based solution, just use that. My video,", "tokens": [33247, 736, 11, 498, 291, 362, 364, 41822, 731, 12, 22475, 2316, 12, 6032, 3827, 11, 445, 764, 300, 13, 1222, 960, 11], "temperature": 0.0, "avg_logprob": -0.08340196333069733, "compression_ratio": 1.4545454545454546, "no_speech_prob": 4.380390964797698e-05}, {"id": 538, "seek": 377784, "start": 3777.84, "end": 3783.92, "text": " Model-Free Methods Workshop demonstrates how the group solves four different problems", "tokens": [17105, 12, 45479, 25285, 82, 48366, 31034, 577, 264, 1594, 39890, 1451, 819, 2740], "temperature": 0.0, "avg_logprob": -0.07995951312711869, "compression_ratio": 1.6096491228070176, "no_speech_prob": 5.815807162434794e-05}, {"id": 539, "seek": 377784, "start": 3783.92, "end": 3791.2000000000003, "text": " at a high level, using both reductionist and holistic methods. Why are these dirty words?", "tokens": [412, 257, 1090, 1496, 11, 1228, 1293, 11004, 468, 293, 30334, 7150, 13, 1545, 366, 613, 9360, 2283, 30], "temperature": 0.0, "avg_logprob": -0.07995951312711869, "compression_ratio": 1.6096491228070176, "no_speech_prob": 5.815807162434794e-05}, {"id": 540, "seek": 377784, "start": 3791.92, "end": 3799.04, "text": " Well, they are not dirty to epistemologists. Reductionism has been the default problem-solving", "tokens": [1042, 11, 436, 366, 406, 9360, 281, 2388, 43958, 12256, 13, 4477, 27549, 1434, 575, 668, 264, 7576, 1154, 12, 30926, 798], "temperature": 0.0, "avg_logprob": -0.07995951312711869, "compression_ratio": 1.6096491228070176, "no_speech_prob": 5.815807162434794e-05}, {"id": 541, "seek": 377784, "start": 3799.04, "end": 3805.6000000000004, "text": " paradigm because it's the one that has to be taught. We are born with a holistic problem-solving", "tokens": [24709, 570, 309, 311, 264, 472, 300, 575, 281, 312, 5928, 13, 492, 366, 4232, 365, 257, 30334, 1154, 12, 30926, 798], "temperature": 0.0, "avg_logprob": -0.07995951312711869, "compression_ratio": 1.6096491228070176, "no_speech_prob": 5.815807162434794e-05}, {"id": 542, "seek": 380560, "start": 3805.6, "end": 3812.48, "text": " apparatus. But reductionist science doesn't come naturally. Therefore, it has to be taught in", "tokens": [38573, 13, 583, 11004, 468, 3497, 1177, 380, 808, 8195, 13, 7504, 11, 309, 575, 281, 312, 5928, 294], "temperature": 0.0, "avg_logprob": -0.08735861449406065, "compression_ratio": 1.4867724867724867, "no_speech_prob": 0.00011580379214137793}, {"id": 543, "seek": 380560, "start": 3812.48, "end": 3819.7599999999998, "text": " schools, practiced, and carried out according to certain rules. Perhaps that's why the sciences", "tokens": [4656, 11, 19268, 11, 293, 9094, 484, 4650, 281, 1629, 4474, 13, 10517, 300, 311, 983, 264, 17677], "temperature": 0.0, "avg_logprob": -0.08735861449406065, "compression_ratio": 1.4867724867724867, "no_speech_prob": 0.00011580379214137793}, {"id": 544, "seek": 380560, "start": 3819.7599999999998, "end": 3826.4, "text": " are called disciplines, because following the ideal scientific method requires practice and", "tokens": [366, 1219, 21919, 11, 570, 3480, 264, 7157, 8134, 3170, 7029, 3124, 293], "temperature": 0.0, "avg_logprob": -0.08735861449406065, "compression_ratio": 1.4867724867724867, "no_speech_prob": 0.00011580379214137793}, {"id": 545, "seek": 382640, "start": 3826.4, "end": 3835.76, "text": " constant vigilance. J. C. Smutsbrook, Holism and Evolution, 1926 established the terminology in the", "tokens": [5754, 39093, 719, 13, 508, 13, 383, 13, 3915, 3648, 9120, 453, 11, 11086, 1434, 293, 40800, 11, 1294, 10880, 7545, 264, 27575, 294, 264], "temperature": 0.0, "avg_logprob": -0.2179926300048828, "compression_ratio": 1.3971291866028708, "no_speech_prob": 9.121523180510849e-05}, {"id": 546, "seek": 382640, "start": 3835.76, "end": 3844.64, "text": " epistemological literature. And no inchrodinger wrote, what is life, 1944, questioning the power", "tokens": [2388, 43958, 4383, 10394, 13, 400, 572, 7227, 340, 3584, 260, 4114, 11, 437, 307, 993, 11, 35133, 11, 21257, 264, 1347], "temperature": 0.0, "avg_logprob": -0.2179926300048828, "compression_ratio": 1.3971291866028708, "no_speech_prob": 9.121523180510849e-05}, {"id": 547, "seek": 382640, "start": 3844.64, "end": 3851.84, "text": " of physics to provide useful explanations to the life sciences. Percy Grote, zen and the art of", "tokens": [295, 10649, 281, 2893, 4420, 28708, 281, 264, 993, 17677, 13, 46216, 2606, 1370, 11, 37097, 293, 264, 1523, 295], "temperature": 0.0, "avg_logprob": -0.2179926300048828, "compression_ratio": 1.3971291866028708, "no_speech_prob": 9.121523180510849e-05}, {"id": 548, "seek": 385184, "start": 3851.84, "end": 3859.84, "text": " motorcycle maintenance, 1974, had contrast something very holistic, zen Buddhism, with", "tokens": [20554, 11258, 11, 33422, 11, 632, 8712, 746, 588, 30334, 11, 37097, 24744, 11, 365], "temperature": 0.0, "avg_logprob": -0.12190997763855817, "compression_ratio": 1.5026178010471205, "no_speech_prob": 7.161141547840089e-05}, {"id": 549, "seek": 385184, "start": 3859.84, "end": 3867.92, "text": " something very reductionist, motorcycle maintenance. So the chasm between the strategies was identified", "tokens": [746, 588, 11004, 468, 11, 20554, 11258, 13, 407, 264, 417, 14774, 1296, 264, 9029, 390, 9234], "temperature": 0.0, "avg_logprob": -0.12190997763855817, "compression_ratio": 1.5026178010471205, "no_speech_prob": 7.161141547840089e-05}, {"id": 550, "seek": 385184, "start": 3867.92, "end": 3876.2400000000002, "text": " a long time ago. The strategies are each other's opposites. H-O-L-E-L-I-S-M-based strategies for", "tokens": [257, 938, 565, 2057, 13, 440, 9029, 366, 1184, 661, 311, 4665, 3324, 13, 389, 12, 46, 12, 43, 12, 36, 12, 43, 12, 40, 12, 50, 12, 44, 12, 6032, 9029, 337], "temperature": 0.0, "avg_logprob": -0.12190997763855817, "compression_ratio": 1.5026178010471205, "no_speech_prob": 7.161141547840089e-05}, {"id": 551, "seek": 387624, "start": 3876.24, "end": 3882.64, "text": " understanding can handle many important kinds of complexity and can quickly provide a guest", "tokens": [3701, 393, 4813, 867, 1021, 3685, 295, 14024, 293, 393, 2661, 2893, 257, 8341], "temperature": 0.0, "avg_logprob": -0.07446338761020714, "compression_ratio": 1.5964125560538116, "no_speech_prob": 9.087567013921216e-05}, {"id": 552, "seek": 387624, "start": 3882.64, "end": 3889.8399999999997, "text": " answer. But these guesses are fallible, and often more expensive to compute. Reductionist", "tokens": [1867, 13, 583, 613, 42703, 366, 2100, 964, 11, 293, 2049, 544, 5124, 281, 14722, 13, 4477, 27549, 468], "temperature": 0.0, "avg_logprob": -0.07446338761020714, "compression_ratio": 1.5964125560538116, "no_speech_prob": 9.087567013921216e-05}, {"id": 553, "seek": 387624, "start": 3889.8399999999997, "end": 3895.8399999999997, "text": " education and strategies brought benefits of cheap model reuse and formal rigor to improve", "tokens": [3309, 293, 9029, 3038, 5311, 295, 7084, 2316, 26225, 293, 9860, 42191, 281, 3470], "temperature": 0.0, "avg_logprob": -0.07446338761020714, "compression_ratio": 1.5964125560538116, "no_speech_prob": 9.087567013921216e-05}, {"id": 554, "seek": 387624, "start": 3895.8399999999997, "end": 3901.4399999999996, "text": " correctness, but cannot handle complexity and is therefore dependent on an external", "tokens": [3006, 1287, 11, 457, 2644, 4813, 14024, 293, 307, 4412, 12334, 322, 364, 8320], "temperature": 0.0, "avg_logprob": -0.07446338761020714, "compression_ratio": 1.5964125560538116, "no_speech_prob": 9.087567013921216e-05}, {"id": 555, "seek": 390144, "start": 3901.44, "end": 3906.7200000000003, "text": " understander to determine applicability in real-world complexity rich situations.", "tokens": [833, 1115, 260, 281, 6997, 2580, 2310, 294, 957, 12, 13217, 14024, 4593, 6851, 13], "temperature": 0.0, "avg_logprob": -0.10286951653751326, "compression_ratio": 1.6278026905829597, "no_speech_prob": 7.759676373098046e-05}, {"id": 556, "seek": 390144, "start": 3907.44, "end": 3914.8, "text": " And as part of that education, we are told that holistic methods, such as jumping to conclusions", "tokens": [400, 382, 644, 295, 300, 3309, 11, 321, 366, 1907, 300, 30334, 7150, 11, 1270, 382, 11233, 281, 22865], "temperature": 0.0, "avg_logprob": -0.10286951653751326, "compression_ratio": 1.6278026905829597, "no_speech_prob": 7.759676373098046e-05}, {"id": 557, "seek": 390144, "start": 3914.8, "end": 3921.6, "text": " unscanned evidence, are bad, in spite of the fact that our brains use holistic methods thousands", "tokens": [2693, 7035, 9232, 4467, 11, 366, 1578, 11, 294, 22794, 295, 264, 1186, 300, 527, 15442, 764, 30334, 7150, 5383], "temperature": 0.0, "avg_logprob": -0.10286951653751326, "compression_ratio": 1.6278026905829597, "no_speech_prob": 7.759676373098046e-05}, {"id": 558, "seek": 390144, "start": 3921.6, "end": 3928.2400000000002, "text": " of times each day to successfully understand the environment we live in. We can all use", "tokens": [295, 1413, 1184, 786, 281, 10727, 1223, 264, 2823, 321, 1621, 294, 13, 492, 393, 439, 764], "temperature": 0.0, "avg_logprob": -0.10286951653751326, "compression_ratio": 1.6278026905829597, "no_speech_prob": 7.759676373098046e-05}, {"id": 559, "seek": 392824, "start": 3928.24, "end": 3935.04, "text": " either strategy as appropriate. If we don't have a STEM education, we will still sometimes make", "tokens": [2139, 5206, 382, 6854, 13, 759, 321, 500, 380, 362, 257, 25043, 3309, 11, 321, 486, 920, 2171, 652], "temperature": 0.0, "avg_logprob": -0.08087870052882604, "compression_ratio": 1.5889830508474576, "no_speech_prob": 4.705207538791001e-05}, {"id": 560, "seek": 392824, "start": 3935.04, "end": 3941.52, "text": " naive models. But sometimes there is a choice and different people may prefer one or the other.", "tokens": [29052, 5245, 13, 583, 2171, 456, 307, 257, 3922, 293, 819, 561, 815, 4382, 472, 420, 264, 661, 13], "temperature": 0.0, "avg_logprob": -0.08087870052882604, "compression_ratio": 1.5889830508474576, "no_speech_prob": 4.705207538791001e-05}, {"id": 561, "seek": 392824, "start": 3942.16, "end": 3948.64, "text": " When playing pool, some people estimate and compute bouncing angles and some people shoot", "tokens": [1133, 2433, 7005, 11, 512, 561, 12539, 293, 14722, 27380, 14708, 293, 512, 561, 3076], "temperature": 0.0, "avg_logprob": -0.08087870052882604, "compression_ratio": 1.5889830508474576, "no_speech_prob": 4.705207538791001e-05}, {"id": 562, "seek": 392824, "start": 3948.64, "end": 3955.9199999999996, "text": " by feel. But we have our preferences, and it may be tempting to label a person with an overly", "tokens": [538, 841, 13, 583, 321, 362, 527, 21910, 11, 293, 309, 815, 312, 37900, 281, 7645, 257, 954, 365, 364, 24324], "temperature": 0.0, "avg_logprob": -0.08087870052882604, "compression_ratio": 1.5889830508474576, "no_speech_prob": 4.705207538791001e-05}, {"id": 563, "seek": 395592, "start": 3955.92, "end": 3964.16, "text": " strong preference as a holistic or a reductionist. This is sometimes received badly, if perceived", "tokens": [2068, 17502, 382, 257, 30334, 420, 257, 11004, 468, 13, 639, 307, 2171, 4613, 13425, 11, 498, 19049], "temperature": 0.0, "avg_logprob": -0.07634800427580533, "compression_ratio": 1.5265957446808511, "no_speech_prob": 4.3819214624818414e-05}, {"id": 564, "seek": 395592, "start": 3964.16, "end": 3972.88, "text": " as a limitation. Some dictionaries even flag reductionist as derogatory. And yet, some people", "tokens": [382, 257, 27432, 13, 2188, 22352, 4889, 754, 7166, 11004, 468, 382, 1163, 664, 4745, 13, 400, 1939, 11, 512, 561], "temperature": 0.0, "avg_logprob": -0.07634800427580533, "compression_ratio": 1.5265957446808511, "no_speech_prob": 4.3819214624818414e-05}, {"id": 565, "seek": 395592, "start": 3972.88, "end": 3980.64, "text": " use it as a self-assigned label. I try to use these terms only as shorthand for a person with a", "tokens": [764, 309, 382, 257, 2698, 12, 640, 16690, 7645, 13, 286, 853, 281, 764, 613, 2115, 787, 382, 402, 2652, 474, 337, 257, 954, 365, 257], "temperature": 0.0, "avg_logprob": -0.07634800427580533, "compression_ratio": 1.5265957446808511, "no_speech_prob": 4.3819214624818414e-05}, {"id": 566, "seek": 398064, "start": 3980.64, "end": 3988.24, "text": " stated strong preference for holistic or reductionist methods. The two terms were very useful in", "tokens": [11323, 2068, 17502, 337, 30334, 420, 11004, 468, 7150, 13, 440, 732, 2115, 645, 588, 4420, 294], "temperature": 0.0, "avg_logprob": -0.07349166562480311, "compression_ratio": 1.485, "no_speech_prob": 3.253981049056165e-05}, {"id": 567, "seek": 398064, "start": 3988.24, "end": 3996.4, "text": " epistemology. But then someone invented the concept of holistic medicine. Instead of just shooting", "tokens": [2388, 43958, 1793, 13, 583, 550, 1580, 14479, 264, 3410, 295, 30334, 7195, 13, 7156, 295, 445, 5942], "temperature": 0.0, "avg_logprob": -0.07349166562480311, "compression_ratio": 1.485, "no_speech_prob": 3.253981049056165e-05}, {"id": 568, "seek": 398064, "start": 3996.4, "end": 4004.4, "text": " a single medical problem, you analyze the patient's entire situation, attempting to account for diet,", "tokens": [257, 2167, 4625, 1154, 11, 291, 12477, 264, 4537, 311, 2302, 2590, 11, 22001, 281, 2696, 337, 6339, 11], "temperature": 0.0, "avg_logprob": -0.07349166562480311, "compression_ratio": 1.485, "no_speech_prob": 3.253981049056165e-05}, {"id": 569, "seek": 400440, "start": 4004.4, "end": 4013.6800000000003, "text": " exercise, sleep, work, habits, stress levels, allergies, family, friends, and environmental", "tokens": [5380, 11, 2817, 11, 589, 11, 14100, 11, 4244, 4358, 11, 37007, 11, 1605, 11, 1855, 11, 293, 8303], "temperature": 0.0, "avg_logprob": -0.11346352801603429, "compression_ratio": 1.489247311827957, "no_speech_prob": 4.723789606941864e-05}, {"id": 570, "seek": 400440, "start": 4013.6800000000003, "end": 4021.92, "text": " poisons. A good idea, in general. But the wide scope was unmanageable by the, traditionally", "tokens": [714, 23886, 13, 316, 665, 1558, 11, 294, 2674, 13, 583, 264, 4874, 11923, 390, 517, 1601, 609, 712, 538, 264, 11, 19067], "temperature": 0.0, "avg_logprob": -0.11346352801603429, "compression_ratio": 1.489247311827957, "no_speech_prob": 4.723789606941864e-05}, {"id": 571, "seek": 400440, "start": 4021.92, "end": 4029.76, "text": " reductionist medical establishment and the idea faded away. Instead, the whole idea of holism", "tokens": [11004, 468, 4625, 20971, 293, 264, 1558, 36352, 1314, 13, 7156, 11, 264, 1379, 1558, 295, 4091, 1434], "temperature": 0.0, "avg_logprob": -0.11346352801603429, "compression_ratio": 1.489247311827957, "no_speech_prob": 4.723789606941864e-05}, {"id": 572, "seek": 402976, "start": 4029.76, "end": 4037.0400000000004, "text": " became tainted as woo-woo in the term, holistic medicine, became associated with woo-woo merchants", "tokens": [3062, 256, 26278, 382, 21657, 12, 17607, 294, 264, 1433, 11, 30334, 7195, 11, 3062, 6615, 365, 21657, 12, 17607, 36253], "temperature": 0.0, "avg_logprob": -0.13145443765740644, "compression_ratio": 1.6327433628318584, "no_speech_prob": 8.53796664159745e-05}, {"id": 573, "seek": 402976, "start": 4037.0400000000004, "end": 4044.5600000000004, "text": " selling crystals and aromatherapy. As explained above, holism is the avoidance of models,", "tokens": [6511, 23772, 293, 594, 298, 1172, 569, 88, 13, 1018, 8825, 3673, 11, 4091, 1434, 307, 264, 5042, 719, 295, 5245, 11], "temperature": 0.0, "avg_logprob": -0.13145443765740644, "compression_ratio": 1.6327433628318584, "no_speech_prob": 8.53796664159745e-05}, {"id": 574, "seek": 402976, "start": 4044.5600000000004, "end": 4051.36, "text": " or better phrased, holism is the metastrategy of avoiding a priori models of the problem domain.", "tokens": [420, 1101, 7636, 1937, 11, 4091, 1434, 307, 264, 1131, 525, 37464, 295, 20220, 257, 4059, 72, 5245, 295, 264, 1154, 9274, 13], "temperature": 0.0, "avg_logprob": -0.13145443765740644, "compression_ratio": 1.6327433628318584, "no_speech_prob": 8.53796664159745e-05}, {"id": 575, "seek": 402976, "start": 4052.0, "end": 4058.7200000000003, "text": " That extra precision rarely matters. There's nothing woo-woo about it. It does say,", "tokens": [663, 2857, 18356, 13752, 7001, 13, 821, 311, 1825, 21657, 12, 17607, 466, 309, 13, 467, 775, 584, 11], "temperature": 0.0, "avg_logprob": -0.13145443765740644, "compression_ratio": 1.6327433628318584, "no_speech_prob": 8.53796664159745e-05}, {"id": 576, "seek": 405872, "start": 4058.72, "end": 4065.3599999999997, "text": " science not required, but, you can make breakfast without reasoning. It is important to note that", "tokens": [3497, 406, 4739, 11, 457, 11, 291, 393, 652, 8201, 1553, 21577, 13, 467, 307, 1021, 281, 3637, 300], "temperature": 0.0, "avg_logprob": -0.06586240213128584, "compression_ratio": 1.6431718061674008, "no_speech_prob": 5.2699848311021924e-05}, {"id": 577, "seek": 405872, "start": 4065.3599999999997, "end": 4071.68, "text": " holistic methods are based on a lifetime of experience, in humans and a training corpus", "tokens": [30334, 7150, 366, 2361, 322, 257, 11364, 295, 1752, 11, 294, 6255, 293, 257, 3097, 1181, 31624], "temperature": 0.0, "avg_logprob": -0.06586240213128584, "compression_ratio": 1.6431718061674008, "no_speech_prob": 5.2699848311021924e-05}, {"id": 578, "seek": 405872, "start": 4071.68, "end": 4078.56, "text": " worth of experience, in neural networks. When you're making breakfast, you are relying on this", "tokens": [3163, 295, 1752, 11, 294, 18161, 9590, 13, 1133, 291, 434, 1455, 8201, 11, 291, 366, 24140, 322, 341], "temperature": 0.0, "avg_logprob": -0.06586240213128584, "compression_ratio": 1.6431718061674008, "no_speech_prob": 5.2699848311021924e-05}, {"id": 579, "seek": 405872, "start": 4078.56, "end": 4085.52, "text": " experience, mostly repeating whatever worked yesterday. Some people claim they use reasoning", "tokens": [1752, 11, 5240, 18617, 2035, 2732, 5186, 13, 2188, 561, 3932, 436, 764, 21577], "temperature": 0.0, "avg_logprob": -0.06586240213128584, "compression_ratio": 1.6431718061674008, "no_speech_prob": 5.2699848311021924e-05}, {"id": 580, "seek": 408552, "start": 4085.52, "end": 4091.36, "text": " while making breakfast, but they can make their breakfast while speaking to someone else on the", "tokens": [1339, 1455, 8201, 11, 457, 436, 393, 652, 641, 8201, 1339, 4124, 281, 1580, 1646, 322, 264], "temperature": 0.0, "avg_logprob": -0.03823339528050916, "compression_ratio": 1.6403508771929824, "no_speech_prob": 5.290682383929379e-05}, {"id": 581, "seek": 408552, "start": 4091.36, "end": 4097.44, "text": " phone. And as they hang up, they find themselves suddenly sitting at the breakfast table with", "tokens": [2593, 13, 400, 382, 436, 3967, 493, 11, 436, 915, 2969, 5800, 3798, 412, 264, 8201, 3199, 365], "temperature": 0.0, "avg_logprob": -0.03823339528050916, "compression_ratio": 1.6403508771929824, "no_speech_prob": 5.290682383929379e-05}, {"id": 582, "seek": 408552, "start": 4097.44, "end": 4104.8, "text": " their coffee and hot oatmeal. Same thing when driving to work. You may get lost in thought,", "tokens": [641, 4982, 293, 2368, 47223, 13, 10635, 551, 562, 4840, 281, 589, 13, 509, 815, 483, 2731, 294, 1194, 11], "temperature": 0.0, "avg_logprob": -0.03823339528050916, "compression_ratio": 1.6403508771929824, "no_speech_prob": 5.290682383929379e-05}, {"id": 583, "seek": 408552, "start": 4104.8, "end": 4111.52, "text": " and then you find yourself parked at work. You didn't need to reason, since all sub-problems", "tokens": [293, 550, 291, 915, 1803, 28491, 412, 589, 13, 509, 994, 380, 643, 281, 1778, 11, 1670, 439, 1422, 12, 47419, 82], "temperature": 0.0, "avg_logprob": -0.03823339528050916, "compression_ratio": 1.6403508771929824, "no_speech_prob": 5.290682383929379e-05}, {"id": 584, "seek": 411152, "start": 4111.52, "end": 4116.4800000000005, "text": " that occur in driving had been solved multiple times, during years of driving.", "tokens": [300, 5160, 294, 4840, 632, 668, 13041, 3866, 1413, 11, 1830, 924, 295, 4840, 13], "temperature": 0.0, "avg_logprob": -0.08860162247058957, "compression_ratio": 1.6355555555555557, "no_speech_prob": 5.508385947905481e-05}, {"id": 585, "seek": 411152, "start": 4117.120000000001, "end": 4123.280000000001, "text": " Sub-conscious understanding is used for simple things like sequencing our leg muscles as we", "tokens": [8511, 12, 19877, 3701, 307, 1143, 337, 2199, 721, 411, 32693, 527, 1676, 9530, 382, 321], "temperature": 0.0, "avg_logprob": -0.08860162247058957, "compression_ratio": 1.6355555555555557, "no_speech_prob": 5.508385947905481e-05}, {"id": 586, "seek": 411152, "start": 4123.280000000001, "end": 4132.0, "text": " walk. You have no idea how you are doing that, it just works. Same thing with vision. You understand", "tokens": [1792, 13, 509, 362, 572, 1558, 577, 291, 366, 884, 300, 11, 309, 445, 1985, 13, 10635, 551, 365, 5201, 13, 509, 1223], "temperature": 0.0, "avg_logprob": -0.08860162247058957, "compression_ratio": 1.6355555555555557, "no_speech_prob": 5.508385947905481e-05}, {"id": 587, "seek": 411152, "start": 4132.0, "end": 4138.64, "text": " that you are looking at a chair, but you do not have conscious access to the 15th rod cone pixel", "tokens": [300, 291, 366, 1237, 412, 257, 6090, 11, 457, 291, 360, 406, 362, 6648, 2105, 281, 264, 2119, 392, 8685, 19749, 19261], "temperature": 0.0, "avg_logprob": -0.08860162247058957, "compression_ratio": 1.6355555555555557, "no_speech_prob": 5.508385947905481e-05}, {"id": 588, "seek": 413864, "start": 4138.64, "end": 4144.320000000001, "text": " to the left of your center of vision, and have no idea how this understanding works.", "tokens": [281, 264, 1411, 295, 428, 3056, 295, 5201, 11, 293, 362, 572, 1558, 577, 341, 3701, 1985, 13], "temperature": 0.0, "avg_logprob": -0.05671563744544983, "compression_ratio": 1.7115384615384615, "no_speech_prob": 3.025719888682943e-05}, {"id": 589, "seek": 413864, "start": 4145.04, "end": 4151.200000000001, "text": " Same thing with understanding and generating language. You do not have any explanation for", "tokens": [10635, 551, 365, 3701, 293, 17746, 2856, 13, 509, 360, 406, 362, 604, 10835, 337], "temperature": 0.0, "avg_logprob": -0.05671563744544983, "compression_ratio": 1.7115384615384615, "no_speech_prob": 3.025719888682943e-05}, {"id": 590, "seek": 413864, "start": 4151.200000000001, "end": 4157.360000000001, "text": " how you are able to understand the meaning of this sentence. Understanding is sub-conscious", "tokens": [577, 291, 366, 1075, 281, 1223, 264, 3620, 295, 341, 8174, 13, 36858, 307, 1422, 12, 19877], "temperature": 0.0, "avg_logprob": -0.05671563744544983, "compression_ratio": 1.7115384615384615, "no_speech_prob": 3.025719888682943e-05}, {"id": 591, "seek": 413864, "start": 4157.360000000001, "end": 4164.240000000001, "text": " and holistic. So for the majority of things we do every day, we do not need reasoning or", "tokens": [293, 30334, 13, 407, 337, 264, 6286, 295, 721, 321, 360, 633, 786, 11, 321, 360, 406, 643, 21577, 420], "temperature": 0.0, "avg_logprob": -0.05671563744544983, "compression_ratio": 1.7115384615384615, "no_speech_prob": 3.025719888682943e-05}, {"id": 592, "seek": 416424, "start": 4164.24, "end": 4171.36, "text": " reductionist methods. Some people would like to think they are, logical thinkers, immune to", "tokens": [11004, 468, 7150, 13, 2188, 561, 576, 411, 281, 519, 436, 366, 11, 14978, 37895, 11, 11992, 281], "temperature": 0.0, "avg_logprob": -0.05718127111109292, "compression_ratio": 1.6607929515418502, "no_speech_prob": 5.910099207540043e-05}, {"id": 593, "seek": 416424, "start": 4171.36, "end": 4178.5599999999995, "text": " most cognitive fallacies, but whether they are or not, at the lower levels, everyone is solving", "tokens": [881, 15605, 2100, 20330, 11, 457, 1968, 436, 366, 420, 406, 11, 412, 264, 3126, 4358, 11, 1518, 307, 12606], "temperature": 0.0, "avg_logprob": -0.05718127111109292, "compression_ratio": 1.6607929515418502, "no_speech_prob": 5.910099207540043e-05}, {"id": 594, "seek": 416424, "start": 4178.5599999999995, "end": 4185.599999999999, "text": " most of their problems holistically. I claim that reductionist reasoning requires holistic", "tokens": [881, 295, 641, 2740, 4091, 20458, 13, 286, 3932, 300, 11004, 468, 21577, 7029, 30334], "temperature": 0.0, "avg_logprob": -0.05718127111109292, "compression_ratio": 1.6607929515418502, "no_speech_prob": 5.910099207540043e-05}, {"id": 595, "seek": 416424, "start": 4185.599999999999, "end": 4193.12, "text": " understanding. In other words, I need to understand the problem domain at hand before I can create", "tokens": [3701, 13, 682, 661, 2283, 11, 286, 643, 281, 1223, 264, 1154, 9274, 412, 1011, 949, 286, 393, 1884], "temperature": 0.0, "avg_logprob": -0.05718127111109292, "compression_ratio": 1.6607929515418502, "no_speech_prob": 5.910099207540043e-05}, {"id": 596, "seek": 419312, "start": 4193.12, "end": 4200.16, "text": " and reuse models to enable me to reason about the domain. So holistic understanding is much", "tokens": [293, 26225, 5245, 281, 9528, 385, 281, 1778, 466, 264, 9274, 13, 407, 30334, 3701, 307, 709], "temperature": 0.0, "avg_logprob": -0.06114761602310907, "compression_ratio": 1.6814159292035398, "no_speech_prob": 7.096826448105276e-05}, {"id": 597, "seek": 419312, "start": 4200.16, "end": 4206.5599999999995, "text": " more important than reductionist reasoning because it is the most used strategy, by far,", "tokens": [544, 1021, 813, 11004, 468, 21577, 570, 309, 307, 264, 881, 1143, 5206, 11, 538, 1400, 11], "temperature": 0.0, "avg_logprob": -0.06114761602310907, "compression_ratio": 1.6814159292035398, "no_speech_prob": 7.096826448105276e-05}, {"id": 598, "seek": 419312, "start": 4206.5599999999995, "end": 4213.599999999999, "text": " and the former is also a prerequisite for the latter. But the fallibility of holistic understanding", "tokens": [293, 264, 5819, 307, 611, 257, 38333, 34152, 337, 264, 18481, 13, 583, 264, 2100, 2841, 295, 30334, 3701], "temperature": 0.0, "avg_logprob": -0.06114761602310907, "compression_ratio": 1.6814159292035398, "no_speech_prob": 7.096826448105276e-05}, {"id": 599, "seek": 419312, "start": 4213.599999999999, "end": 4220.8, "text": " forced us to create reductionist science and to teach it in STEM education. It is as if the purpose", "tokens": [7579, 505, 281, 1884, 11004, 468, 3497, 293, 281, 2924, 309, 294, 25043, 3309, 13, 467, 307, 382, 498, 264, 4334], "temperature": 0.0, "avg_logprob": -0.06114761602310907, "compression_ratio": 1.6814159292035398, "no_speech_prob": 7.096826448105276e-05}, {"id": 600, "seek": 422080, "start": 4220.8, "end": 4227.92, "text": " of science is to keep holistic guessing in check, but this aversion to fallibility has a cost,", "tokens": [295, 3497, 307, 281, 1066, 30334, 17939, 294, 1520, 11, 457, 341, 257, 29153, 281, 2100, 2841, 575, 257, 2063, 11], "temperature": 0.0, "avg_logprob": -0.09321864706570984, "compression_ratio": 1.5, "no_speech_prob": 5.1044717110926285e-05}, {"id": 601, "seek": 422080, "start": 4227.92, "end": 4235.4400000000005, "text": " because it means complexity bound and irreducible problems cannot be solved. Like language", "tokens": [570, 309, 1355, 14024, 5472, 293, 16014, 769, 32128, 2740, 2644, 312, 13041, 13, 1743, 2856], "temperature": 0.0, "avg_logprob": -0.09321864706570984, "compression_ratio": 1.5, "no_speech_prob": 5.1044717110926285e-05}, {"id": 602, "seek": 422080, "start": 4235.4400000000005, "end": 4243.4400000000005, "text": " understanding, global resource allocation, and social interactions, reductionism and model-based", "tokens": [3701, 11, 4338, 7684, 27599, 11, 293, 2093, 13280, 11, 11004, 1434, 293, 2316, 12, 6032], "temperature": 0.0, "avg_logprob": -0.09321864706570984, "compression_ratio": 1.5, "no_speech_prob": 5.1044717110926285e-05}, {"id": 603, "seek": 424344, "start": 4243.44, "end": 4251.759999999999, "text": " science appeared around 1650 after a century of gestation. Excluding minor romantic interludes,", "tokens": [3497, 8516, 926, 3165, 2803, 934, 257, 4901, 295, 7219, 399, 13, 9368, 20626, 6696, 13590, 728, 1471, 279, 11], "temperature": 0.0, "avg_logprob": -0.06975816488265991, "compression_ratio": 1.58008658008658, "no_speech_prob": 2.3658532882109284e-05}, {"id": 604, "seek": 424344, "start": 4251.759999999999, "end": 4258.799999999999, "text": " it has held its position as the dominant paradigm for about 400 years. This is changing.", "tokens": [309, 575, 5167, 1080, 2535, 382, 264, 15657, 24709, 337, 466, 8423, 924, 13, 639, 307, 4473, 13], "temperature": 0.0, "avg_logprob": -0.06975816488265991, "compression_ratio": 1.58008658008658, "no_speech_prob": 2.3658532882109284e-05}, {"id": 605, "seek": 424344, "start": 4259.599999999999, "end": 4266.0, "text": " The reductionist train is running out of track. The remaining hard problems facing humanity", "tokens": [440, 11004, 468, 3847, 307, 2614, 484, 295, 2837, 13, 440, 8877, 1152, 2740, 7170, 10243], "temperature": 0.0, "avg_logprob": -0.06975816488265991, "compression_ratio": 1.58008658008658, "no_speech_prob": 2.3658532882109284e-05}, {"id": 606, "seek": 424344, "start": 4266.0, "end": 4271.919999999999, "text": " are problems of irreducible complexity in domains where reductionist model-based methods", "tokens": [366, 2740, 295, 16014, 769, 32128, 14024, 294, 25514, 689, 11004, 468, 2316, 12, 6032, 7150], "temperature": 0.0, "avg_logprob": -0.06975816488265991, "compression_ratio": 1.58008658008658, "no_speech_prob": 2.3658532882109284e-05}, {"id": 607, "seek": 427192, "start": 4271.92, "end": 4279.36, "text": " simply cannot work. Whether we like the idea or not, we need to accept these holistic methods", "tokens": [2935, 2644, 589, 13, 8503, 321, 411, 264, 1558, 420, 406, 11, 321, 643, 281, 3241, 613, 30334, 7150], "temperature": 0.0, "avg_logprob": -0.09609926663912259, "compression_ratio": 1.5, "no_speech_prob": 0.00010635687067406252}, {"id": 608, "seek": 427192, "start": 4279.36, "end": 4286.4800000000005, "text": " into our AI toolkits. Starting now, we will use these methods either in their raw form,", "tokens": [666, 527, 7318, 2290, 74, 1208, 13, 16217, 586, 11, 321, 486, 764, 613, 7150, 2139, 294, 641, 8936, 1254, 11], "temperature": 0.0, "avg_logprob": -0.09609926663912259, "compression_ratio": 1.5, "no_speech_prob": 0.00010635687067406252}, {"id": 609, "seek": 427192, "start": 4286.4800000000005, "end": 4292.8, "text": " as model-free methods, or as understanding machines at any level from component to robot", "tokens": [382, 2316, 12, 10792, 7150, 11, 420, 382, 3701, 8379, 412, 604, 1496, 490, 6542, 281, 7881], "temperature": 0.0, "avg_logprob": -0.09609926663912259, "compression_ratio": 1.5, "no_speech_prob": 0.00010635687067406252}, {"id": 610, "seek": 429280, "start": 4292.8, "end": 4302.56, "text": " co-worker. Chapter 4. Reduction. Epistemic reduction is a process that discovers higher-level", "tokens": [598, 12, 49402, 13, 18874, 1017, 13, 4477, 27549, 13, 9970, 468, 3438, 11004, 307, 257, 1399, 300, 44522, 2946, 12, 12418], "temperature": 0.0, "avg_logprob": -0.10093668629141415, "compression_ratio": 1.5080213903743316, "no_speech_prob": 0.00011791672295657918}, {"id": 611, "seek": 429280, "start": 4302.56, "end": 4308.8, "text": " abstractions and lower-level data by discarding everything at the lower layer that it recognizes", "tokens": [12649, 626, 293, 3126, 12, 12418, 1412, 538, 31597, 278, 1203, 412, 264, 3126, 4583, 300, 309, 26564], "temperature": 0.0, "avg_logprob": -0.10093668629141415, "compression_ratio": 1.5080213903743316, "no_speech_prob": 0.00011791672295657918}, {"id": 612, "seek": 429280, "start": 4308.8, "end": 4316.56, "text": " as irrelevant. We have seen the power of models. We have introduced the two problem-solving", "tokens": [382, 28682, 13, 492, 362, 1612, 264, 1347, 295, 5245, 13, 492, 362, 7268, 264, 732, 1154, 12, 30926, 798], "temperature": 0.0, "avg_logprob": -0.10093668629141415, "compression_ratio": 1.5080213903743316, "no_speech_prob": 0.00011791672295657918}, {"id": 613, "seek": 431656, "start": 4316.56, "end": 4323.4400000000005, "text": " meta-strategies of reductionism and holism. We also noted that the creation and use of models", "tokens": [19616, 12, 9733, 2968, 530, 295, 11004, 1434, 293, 4091, 1434, 13, 492, 611, 12964, 300, 264, 8016, 293, 764, 295, 5245], "temperature": 0.0, "avg_logprob": -0.0832348236670861, "compression_ratio": 1.5263157894736843, "no_speech_prob": 4.301736771594733e-05}, {"id": 614, "seek": 431656, "start": 4323.4400000000005, "end": 4330.320000000001, "text": " requires an intelligent agent that understands the problem domain. Someone or something has to", "tokens": [7029, 364, 13232, 9461, 300, 15146, 264, 1154, 9274, 13, 8734, 420, 746, 575, 281], "temperature": 0.0, "avg_logprob": -0.0832348236670861, "compression_ratio": 1.5263157894736843, "no_speech_prob": 4.301736771594733e-05}, {"id": 615, "seek": 431656, "start": 4330.320000000001, "end": 4339.76, "text": " perform the reduction. I will now discuss reduction in some detail. Until 2012, only humans and other", "tokens": [2042, 264, 11004, 13, 286, 486, 586, 2248, 11004, 294, 512, 2607, 13, 9088, 9125, 11, 787, 6255, 293, 661], "temperature": 0.0, "avg_logprob": -0.0832348236670861, "compression_ratio": 1.5263157894736843, "no_speech_prob": 4.301736771594733e-05}, {"id": 616, "seek": 433976, "start": 4339.76, "end": 4346.88, "text": " animals with brains could perform reduction. Now our deep neural networks, DNN, can perform", "tokens": [4882, 365, 15442, 727, 2042, 11004, 13, 823, 527, 2452, 18161, 9590, 11, 21500, 45, 11, 393, 2042], "temperature": 0.0, "avg_logprob": -0.08250288529829546, "compression_ratio": 1.5591836734693878, "no_speech_prob": 9.857165423454717e-05}, {"id": 617, "seek": 433976, "start": 4346.88, "end": 4354.64, "text": " limited reduction. How do brains and DNNs accomplish this? And how can we improve these algorithms?", "tokens": [5567, 11004, 13, 1012, 360, 15442, 293, 21500, 45, 82, 9021, 341, 30, 400, 577, 393, 321, 3470, 613, 14642, 30], "temperature": 0.0, "avg_logprob": -0.08250288529829546, "compression_ratio": 1.5591836734693878, "no_speech_prob": 9.857165423454717e-05}, {"id": 618, "seek": 433976, "start": 4355.360000000001, "end": 4362.16, "text": " This may be, to some readers, the most rewarding part of this series, because it provides you", "tokens": [639, 815, 312, 11, 281, 512, 17147, 11, 264, 881, 20063, 644, 295, 341, 2638, 11, 570, 309, 6417, 291], "temperature": 0.0, "avg_logprob": -0.08250288529829546, "compression_ratio": 1.5591836734693878, "no_speech_prob": 9.857165423454717e-05}, {"id": 619, "seek": 433976, "start": 4362.16, "end": 4368.96, "text": " the opportunity to learn a new and useful skill. Most people never think about the world at this", "tokens": [264, 2650, 281, 1466, 257, 777, 293, 4420, 5389, 13, 4534, 561, 1128, 519, 466, 264, 1002, 412, 341], "temperature": 0.0, "avg_logprob": -0.08250288529829546, "compression_ratio": 1.5591836734693878, "no_speech_prob": 9.857165423454717e-05}, {"id": 620, "seek": 436896, "start": 4368.96, "end": 4375.76, "text": " level. Knowledge of reduction provides a new point of view that you can use to better understand", "tokens": [1496, 13, 32906, 295, 11004, 6417, 257, 777, 935, 295, 1910, 300, 291, 393, 764, 281, 1101, 1223], "temperature": 0.0, "avg_logprob": -0.05225248214526054, "compression_ratio": 1.662037037037037, "no_speech_prob": 6.597548781428486e-05}, {"id": 621, "seek": 436896, "start": 4375.76, "end": 4381.44, "text": " your environment, other intelligent agents around you, and modern AI systems.", "tokens": [428, 2823, 11, 661, 13232, 12554, 926, 291, 11, 293, 4363, 7318, 3652, 13], "temperature": 0.0, "avg_logprob": -0.05225248214526054, "compression_ratio": 1.662037037037037, "no_speech_prob": 6.597548781428486e-05}, {"id": 622, "seek": 436896, "start": 4382.24, "end": 4388.88, "text": " Definition of reduction. Reduction is a process that discovers higher-level abstractions and", "tokens": [46245, 849, 295, 11004, 13, 4477, 27549, 307, 257, 1399, 300, 44522, 2946, 12, 12418, 12649, 626, 293], "temperature": 0.0, "avg_logprob": -0.05225248214526054, "compression_ratio": 1.662037037037037, "no_speech_prob": 6.597548781428486e-05}, {"id": 623, "seek": 436896, "start": 4388.88, "end": 4394.8, "text": " lower-level data. We will initially note that reduction is exactly the same as abstraction.", "tokens": [3126, 12, 12418, 1412, 13, 492, 486, 9105, 3637, 300, 11004, 307, 2293, 264, 912, 382, 37765, 13], "temperature": 0.0, "avg_logprob": -0.05225248214526054, "compression_ratio": 1.662037037037037, "no_speech_prob": 6.597548781428486e-05}, {"id": 624, "seek": 439480, "start": 4394.8, "end": 4401.68, "text": " Why do we need a new word? Because the term abstraction is mostly used", "tokens": [1545, 360, 321, 643, 257, 777, 1349, 30, 1436, 264, 1433, 37765, 307, 5240, 1143], "temperature": 0.0, "avg_logprob": -0.07585360844930013, "compression_ratio": 1.6333333333333333, "no_speech_prob": 8.441902900813147e-05}, {"id": 625, "seek": 439480, "start": 4401.68, "end": 4408.08, "text": " by scientists already operating in a pure model space, seeking a higher level of abstraction", "tokens": [538, 7708, 1217, 7447, 294, 257, 6075, 2316, 1901, 11, 11670, 257, 2946, 1496, 295, 37765], "temperature": 0.0, "avg_logprob": -0.07585360844930013, "compression_ratio": 1.6333333333333333, "no_speech_prob": 8.441902900813147e-05}, {"id": 626, "seek": 439480, "start": 4408.08, "end": 4414.88, "text": " in that space. But to them, abstraction is something that just magically happens in their", "tokens": [294, 300, 1901, 13, 583, 281, 552, 11, 37765, 307, 746, 300, 445, 39763, 2314, 294, 641], "temperature": 0.0, "avg_logprob": -0.07585360844930013, "compression_ratio": 1.6333333333333333, "no_speech_prob": 8.441902900813147e-05}, {"id": 627, "seek": 439480, "start": 4414.88, "end": 4421.76, "text": " heads, since there are no scientific theories for how abstraction works. There cannot be,", "tokens": [8050, 11, 1670, 456, 366, 572, 8134, 13667, 337, 577, 37765, 1985, 13, 821, 2644, 312, 11], "temperature": 0.0, "avg_logprob": -0.07585360844930013, "compression_ratio": 1.6333333333333333, "no_speech_prob": 8.441902900813147e-05}, {"id": 628, "seek": 442176, "start": 4421.76, "end": 4429.280000000001, "text": " since abstraction is a concept in epistemology, not science. AI researchers are starting from", "tokens": [1670, 37765, 307, 257, 3410, 294, 2388, 43958, 1793, 11, 406, 3497, 13, 7318, 10309, 366, 2891, 490], "temperature": 0.0, "avg_logprob": -0.06547806636396661, "compression_ratio": 1.6502242152466369, "no_speech_prob": 2.088555265800096e-05}, {"id": 629, "seek": 442176, "start": 4429.280000000001, "end": 4435.6, "text": " something much closer to a rich mundane reality, where there is a lot of confounding context.", "tokens": [746, 709, 4966, 281, 257, 4593, 43497, 4103, 11, 689, 456, 307, 257, 688, 295, 1497, 24625, 4319, 13], "temperature": 0.0, "avg_logprob": -0.06547806636396661, "compression_ratio": 1.6502242152466369, "no_speech_prob": 2.088555265800096e-05}, {"id": 630, "seek": 442176, "start": 4436.16, "end": 4441.92, "text": " We are solving the metal problem of how to move from there into a space that is sufficiently", "tokens": [492, 366, 12606, 264, 5760, 1154, 295, 577, 281, 1286, 490, 456, 666, 257, 1901, 300, 307, 31868], "temperature": 0.0, "avg_logprob": -0.06547806636396661, "compression_ratio": 1.6502242152466369, "no_speech_prob": 2.088555265800096e-05}, {"id": 631, "seek": 442176, "start": 4441.92, "end": 4448.72, "text": " abstract to solve the problem at hand. Here, reduction is a much more appropriate term.", "tokens": [12649, 281, 5039, 264, 1154, 412, 1011, 13, 1692, 11, 11004, 307, 257, 709, 544, 6854, 1433, 13], "temperature": 0.0, "avg_logprob": -0.06547806636396661, "compression_ratio": 1.6502242152466369, "no_speech_prob": 2.088555265800096e-05}, {"id": 632, "seek": 444872, "start": 4448.72, "end": 4455.6, "text": " We can abstract the red pixel or the letter B, but we can reduce a rich context containing", "tokens": [492, 393, 12649, 264, 2182, 19261, 420, 264, 5063, 363, 11, 457, 321, 393, 5407, 257, 4593, 4319, 19273], "temperature": 0.0, "avg_logprob": -0.13000501588333485, "compression_ratio": 1.6481481481481481, "no_speech_prob": 3.5412071156315506e-05}, {"id": 633, "seek": 444872, "start": 4455.6, "end": 4461.6, "text": " that pixel or letter into a higher-level concept. We are swimming in reduction.", "tokens": [300, 19261, 420, 5063, 666, 257, 2946, 12, 12418, 3410, 13, 492, 366, 11989, 294, 11004, 13], "temperature": 0.0, "avg_logprob": -0.13000501588333485, "compression_ratio": 1.6481481481481481, "no_speech_prob": 3.5412071156315506e-05}, {"id": 634, "seek": 444872, "start": 4461.6, "end": 4467.6, "text": " Paradoxically, one of the hardest things about teaching reduction is that we don't see the", "tokens": [3457, 23029, 984, 11, 472, 295, 264, 13158, 721, 466, 4571, 11004, 307, 300, 321, 500, 380, 536, 264], "temperature": 0.0, "avg_logprob": -0.13000501588333485, "compression_ratio": 1.6481481481481481, "no_speech_prob": 3.5412071156315506e-05}, {"id": 635, "seek": 444872, "start": 4467.6, "end": 4473.92, "text": " need to learn about it because we all do it all the time, every millisecond, and the resulting", "tokens": [643, 281, 1466, 466, 309, 570, 321, 439, 360, 309, 439, 264, 565, 11, 633, 27940, 18882, 11, 293, 264, 16505], "temperature": 0.0, "avg_logprob": -0.13000501588333485, "compression_ratio": 1.6481481481481481, "no_speech_prob": 3.5412071156315506e-05}, {"id": 636, "seek": 447392, "start": 4473.92, "end": 4482.24, "text": " reductions, models, become available to our conscious minds as if, by magic, brains reduce", "tokens": [40296, 11, 5245, 11, 1813, 2435, 281, 527, 6648, 9634, 382, 498, 11, 538, 5585, 11, 15442, 5407], "temperature": 0.0, "avg_logprob": -0.13739855149213007, "compression_ratio": 1.4742268041237114, "no_speech_prob": 4.2049425246659666e-05}, {"id": 637, "seek": 447392, "start": 4482.24, "end": 4491.2, "text": " away 99.999% of their sensory input, but this process is subconscious and hence invisible to us.", "tokens": [1314, 11803, 13, 49017, 4, 295, 641, 27233, 4846, 11, 457, 341, 1399, 307, 27389, 293, 16678, 14603, 281, 505, 13], "temperature": 0.0, "avg_logprob": -0.13739855149213007, "compression_ratio": 1.4742268041237114, "no_speech_prob": 4.2049425246659666e-05}, {"id": 638, "seek": 447392, "start": 4491.92, "end": 4500.24, "text": " The situation is much like, supposedly, a fish swimming in water. We are all masters of reduction,", "tokens": [440, 2590, 307, 709, 411, 11, 20581, 11, 257, 3506, 11989, 294, 1281, 13, 492, 366, 439, 19294, 295, 11004, 11], "temperature": 0.0, "avg_logprob": -0.13739855149213007, "compression_ratio": 1.4742268041237114, "no_speech_prob": 4.2049425246659666e-05}, {"id": 639, "seek": 450024, "start": 4500.24, "end": 4506.16, "text": " but we don't know how we do it or that we even do it. We didn't know this would ever matter.", "tokens": [457, 321, 500, 380, 458, 577, 321, 360, 309, 420, 300, 321, 754, 360, 309, 13, 492, 994, 380, 458, 341, 576, 1562, 1871, 13], "temperature": 0.0, "avg_logprob": -0.056382215151222805, "compression_ratio": 1.775609756097561, "no_speech_prob": 3.668574936455116e-05}, {"id": 640, "seek": 450024, "start": 4506.8, "end": 4514.5599999999995, "text": " And generally, it doesn't. Well, it matters in epistemology, and it matters in AI,", "tokens": [400, 5101, 11, 309, 1177, 380, 13, 1042, 11, 309, 7001, 294, 2388, 43958, 1793, 11, 293, 309, 7001, 294, 7318, 11], "temperature": 0.0, "avg_logprob": -0.056382215151222805, "compression_ratio": 1.775609756097561, "no_speech_prob": 3.668574936455116e-05}, {"id": 641, "seek": 450024, "start": 4514.5599999999995, "end": 4521.84, "text": " since we need to actually implement that magic. We as epistemologists must know how abstraction", "tokens": [1670, 321, 643, 281, 767, 4445, 300, 5585, 13, 492, 382, 2388, 43958, 12256, 1633, 458, 577, 37765], "temperature": 0.0, "avg_logprob": -0.056382215151222805, "compression_ratio": 1.775609756097561, "no_speech_prob": 3.668574936455116e-05}, {"id": 642, "seek": 450024, "start": 4521.84, "end": 4528.5599999999995, "text": " is actually performed, and we give the epistemology-level equivalent of abstraction the name", "tokens": [307, 767, 10332, 11, 293, 321, 976, 264, 2388, 43958, 1793, 12, 12418, 10344, 295, 37765, 264, 1315], "temperature": 0.0, "avg_logprob": -0.056382215151222805, "compression_ratio": 1.775609756097561, "no_speech_prob": 3.668574936455116e-05}, {"id": 643, "seek": 452856, "start": 4528.56, "end": 4534.88, "text": " reduction, because that's the recipe for how to accomplish it. We reduce our rich mundane", "tokens": [11004, 11, 570, 300, 311, 264, 6782, 337, 577, 281, 9021, 309, 13, 492, 5407, 527, 4593, 43497], "temperature": 0.0, "avg_logprob": -0.09133097671327137, "compression_ratio": 1.543859649122807, "no_speech_prob": 4.923090818920173e-05}, {"id": 644, "seek": 452856, "start": 4534.88, "end": 4542.400000000001, "text": " reality by discarding, reducing away, what's irrelevant. And by using the name reduction,", "tokens": [4103, 538, 31597, 278, 11, 12245, 1314, 11, 437, 311, 28682, 13, 400, 538, 1228, 264, 1315, 11004, 11], "temperature": 0.0, "avg_logprob": -0.09133097671327137, "compression_ratio": 1.543859649122807, "no_speech_prob": 4.923090818920173e-05}, {"id": 645, "seek": 452856, "start": 4542.400000000001, "end": 4548.4800000000005, "text": " we, as AI epistemologists, keep reminding ourselves how it is properly done.", "tokens": [321, 11, 382, 7318, 2388, 43958, 12256, 11, 1066, 27639, 4175, 577, 309, 307, 6108, 1096, 13], "temperature": 0.0, "avg_logprob": -0.09133097671327137, "compression_ratio": 1.543859649122807, "no_speech_prob": 4.923090818920173e-05}, {"id": 646, "seek": 452856, "start": 4549.120000000001, "end": 4556.160000000001, "text": " Consider the following descriptions of a car. The slide is meant to be read from the bottom up,", "tokens": [17416, 264, 3480, 24406, 295, 257, 1032, 13, 440, 4137, 307, 4140, 281, 312, 1401, 490, 264, 2767, 493, 11], "temperature": 0.0, "avg_logprob": -0.09133097671327137, "compression_ratio": 1.543859649122807, "no_speech_prob": 4.923090818920173e-05}, {"id": 647, "seek": 455616, "start": 4556.16, "end": 4565.36, "text": " to match abstraction levels from low to high. If I'm driving to work, I better be driving my car.", "tokens": [281, 2995, 37765, 4358, 490, 2295, 281, 1090, 13, 759, 286, 478, 4840, 281, 589, 11, 286, 1101, 312, 4840, 452, 1032, 13], "temperature": 0.0, "avg_logprob": -0.08626794815063477, "compression_ratio": 1.6788990825688073, "no_speech_prob": 0.00010555714834481478}, {"id": 648, "seek": 455616, "start": 4566.0, "end": 4572.639999999999, "text": " If the police are looking for a stolen car, they would be looking for red 2010 Toyota Celica.", "tokens": [759, 264, 3804, 366, 1237, 337, 257, 15900, 1032, 11, 436, 576, 312, 1237, 337, 2182, 9657, 22926, 19967, 2262, 13], "temperature": 0.0, "avg_logprob": -0.08626794815063477, "compression_ratio": 1.6788990825688073, "no_speech_prob": 0.00010555714834481478}, {"id": 649, "seek": 455616, "start": 4573.28, "end": 4578.639999999999, "text": " If I'm buying a new car, then I might be looking for just a new Toyota Celica.", "tokens": [759, 286, 478, 6382, 257, 777, 1032, 11, 550, 286, 1062, 312, 1237, 337, 445, 257, 777, 22926, 19967, 2262, 13], "temperature": 0.0, "avg_logprob": -0.08626794815063477, "compression_ratio": 1.6788990825688073, "no_speech_prob": 0.00010555714834481478}, {"id": 650, "seek": 455616, "start": 4579.2, "end": 4585.599999999999, "text": " And a self-driving car would likely only need to understand whether an obstacle is a vehicle or", "tokens": [400, 257, 2698, 12, 47094, 1032, 576, 3700, 787, 643, 281, 1223, 1968, 364, 23112, 307, 257, 5864, 420], "temperature": 0.0, "avg_logprob": -0.08626794815063477, "compression_ratio": 1.6788990825688073, "no_speech_prob": 0.00010555714834481478}, {"id": 651, "seek": 458560, "start": 4585.6, "end": 4593.120000000001, "text": " not, in order to model maximum speed for future movement. We see that we want to pick the appropriate", "tokens": [406, 11, 294, 1668, 281, 2316, 6674, 3073, 337, 2027, 3963, 13, 492, 536, 300, 321, 528, 281, 1888, 264, 6854], "temperature": 0.0, "avg_logprob": -0.06679712590717134, "compression_ratio": 1.572072072072072, "no_speech_prob": 0.0001350470702163875}, {"id": 652, "seek": 458560, "start": 4593.120000000001, "end": 4599.120000000001, "text": " level of abstraction to deal with the same object, or topic, in different situations.", "tokens": [1496, 295, 37765, 281, 2028, 365, 264, 912, 2657, 11, 420, 4829, 11, 294, 819, 6851, 13], "temperature": 0.0, "avg_logprob": -0.06679712590717134, "compression_ratio": 1.572072072072072, "no_speech_prob": 0.0001350470702163875}, {"id": 653, "seek": 458560, "start": 4599.84, "end": 4605.280000000001, "text": " But more importantly, we see that we can get from a more detailed description,", "tokens": [583, 544, 8906, 11, 321, 536, 300, 321, 393, 483, 490, 257, 544, 9942, 3855, 11], "temperature": 0.0, "avg_logprob": -0.06679712590717134, "compression_ratio": 1.572072072072072, "no_speech_prob": 0.0001350470702163875}, {"id": 654, "seek": 458560, "start": 4605.280000000001, "end": 4611.200000000001, "text": " at the bottom, to a more generic one, higher up, by simply discarding some detail.", "tokens": [412, 264, 2767, 11, 281, 257, 544, 19577, 472, 11, 2946, 493, 11, 538, 2935, 31597, 278, 512, 2607, 13], "temperature": 0.0, "avg_logprob": -0.06679712590717134, "compression_ratio": 1.572072072072072, "no_speech_prob": 0.0001350470702163875}, {"id": 655, "seek": 461120, "start": 4611.2, "end": 4617.5199999999995, "text": " I hasten to point out that reduction is more complicated than this simple example of decreasing", "tokens": [286, 6581, 268, 281, 935, 484, 300, 11004, 307, 544, 6179, 813, 341, 2199, 1365, 295, 23223], "temperature": 0.0, "avg_logprob": -0.1271046343303862, "compression_ratio": 1.6398305084745763, "no_speech_prob": 4.991118112229742e-05}, {"id": 656, "seek": 461120, "start": 4617.5199999999995, "end": 4624.48, "text": " specificity shows. What we need to start somewhere in this image allows us to form intuitions that", "tokens": [2685, 507, 3110, 13, 708, 321, 643, 281, 722, 4079, 294, 341, 3256, 4045, 505, 281, 1254, 16224, 626, 300], "temperature": 0.0, "avg_logprob": -0.1271046343303862, "compression_ratio": 1.6398305084745763, "no_speech_prob": 4.991118112229742e-05}, {"id": 657, "seek": 461120, "start": 4624.48, "end": 4631.679999999999, "text": " will serve for a while. True reduction involves operations like shifting from syntax to semantics", "tokens": [486, 4596, 337, 257, 1339, 13, 13587, 11004, 11626, 7705, 411, 17573, 490, 28431, 281, 4361, 45298], "temperature": 0.0, "avg_logprob": -0.1271046343303862, "compression_ratio": 1.6398305084745763, "no_speech_prob": 4.991118112229742e-05}, {"id": 658, "seek": 461120, "start": 4631.679999999999, "end": 4639.679999999999, "text": " or from instance to type. The appearance of car as an abstraction of Toyota, and the step from", "tokens": [420, 490, 5197, 281, 2010, 13, 440, 8967, 295, 1032, 382, 364, 37765, 295, 22926, 11, 293, 264, 1823, 490], "temperature": 0.0, "avg_logprob": -0.1271046343303862, "compression_ratio": 1.6398305084745763, "no_speech_prob": 4.991118112229742e-05}, {"id": 659, "seek": 463968, "start": 4639.68, "end": 4647.84, "text": " my Toyota to a Toyota illustrates these steps. Algorithms for these things are known.", "tokens": [452, 22926, 281, 257, 22926, 41718, 613, 4439, 13, 35014, 6819, 2592, 337, 613, 721, 366, 2570, 13], "temperature": 0.0, "avg_logprob": -0.14072890508742558, "compression_ratio": 1.4860335195530727, "no_speech_prob": 5.604177204077132e-05}, {"id": 660, "seek": 463968, "start": 4648.64, "end": 4656.0, "text": " Salience, part of the trick is to know what to discard. At each level of abstraction,", "tokens": [5996, 1182, 11, 644, 295, 264, 4282, 307, 281, 458, 437, 281, 31597, 13, 1711, 1184, 1496, 295, 37765, 11], "temperature": 0.0, "avg_logprob": -0.14072890508742558, "compression_ratio": 1.4860335195530727, "no_speech_prob": 5.604177204077132e-05}, {"id": 661, "seek": 463968, "start": 4656.0, "end": 4663.76, "text": " something can typically be identified as the least important property. Red and Celica are more", "tokens": [746, 393, 5850, 312, 9234, 382, 264, 1935, 1021, 4707, 13, 4477, 293, 19967, 2262, 366, 544], "temperature": 0.0, "avg_logprob": -0.14072890508742558, "compression_ratio": 1.4860335195530727, "no_speech_prob": 5.604177204077132e-05}, {"id": 662, "seek": 466376, "start": 4663.76, "end": 4673.280000000001, "text": " significant than 2010 for anyone looking for a car. If we had started from my red 2010 Toyota", "tokens": [4776, 813, 9657, 337, 2878, 1237, 337, 257, 1032, 13, 759, 321, 632, 1409, 490, 452, 2182, 9657, 22926], "temperature": 0.0, "avg_logprob": -0.08978506290551388, "compression_ratio": 1.4646464646464648, "no_speech_prob": 2.4339544324902818e-05}, {"id": 663, "seek": 466376, "start": 4673.280000000001, "end": 4681.360000000001, "text": " truck, then the word truck would not be discarded until the top level. Reduction requires understanding", "tokens": [5898, 11, 550, 264, 1349, 5898, 576, 406, 312, 45469, 1826, 264, 1192, 1496, 13, 4477, 27549, 7029, 3701], "temperature": 0.0, "avg_logprob": -0.08978506290551388, "compression_ratio": 1.4646464646464648, "no_speech_prob": 2.4339544324902818e-05}, {"id": 664, "seek": 466376, "start": 4681.360000000001, "end": 4689.4400000000005, "text": " what's relevant. In reduction we keep that which is salient. More later, partial reductions.", "tokens": [437, 311, 7340, 13, 682, 11004, 321, 1066, 300, 597, 307, 1845, 1196, 13, 5048, 1780, 11, 14641, 40296, 13], "temperature": 0.0, "avg_logprob": -0.08978506290551388, "compression_ratio": 1.4646464646464648, "no_speech_prob": 2.4339544324902818e-05}, {"id": 665, "seek": 468944, "start": 4689.44, "end": 4697.36, "text": " Most of the time we do not perform reduction all the way to models. I cannot stress this enough.", "tokens": [4534, 295, 264, 565, 321, 360, 406, 2042, 11004, 439, 264, 636, 281, 5245, 13, 286, 2644, 4244, 341, 1547, 13], "temperature": 0.0, "avg_logprob": -0.0968290821889813, "compression_ratio": 1.658008658008658, "no_speech_prob": 5.6921311625046656e-05}, {"id": 666, "seek": 468944, "start": 4698.0, "end": 4705.12, "text": " We discuss reduction to models for pedagogical reasons. It is easy to initially see the context", "tokens": [492, 2248, 11004, 281, 5245, 337, 5670, 31599, 804, 4112, 13, 467, 307, 1858, 281, 9105, 536, 264, 4319], "temperature": 0.0, "avg_logprob": -0.0968290821889813, "compression_ratio": 1.658008658008658, "no_speech_prob": 5.6921311625046656e-05}, {"id": 667, "seek": 468944, "start": 4705.12, "end": 4712.96, "text": " free model as the goal of reduction. In reality, in brains, we can stop reducing the moment we", "tokens": [1737, 2316, 382, 264, 3387, 295, 11004, 13, 682, 4103, 11, 294, 15442, 11, 321, 393, 1590, 12245, 264, 1623, 321], "temperature": 0.0, "avg_logprob": -0.0968290821889813, "compression_ratio": 1.658008658008658, "no_speech_prob": 5.6921311625046656e-05}, {"id": 668, "seek": 471296, "start": 4712.96, "end": 4719.68, "text": " recognize that we have a working answer or response, such as a command to contract some muscle or", "tokens": [5521, 300, 321, 362, 257, 1364, 1867, 420, 4134, 11, 1270, 382, 257, 5622, 281, 4364, 512, 8679, 420], "temperature": 0.0, "avg_logprob": -0.06318018809858575, "compression_ratio": 1.5805084745762712, "no_speech_prob": 2.875582140404731e-05}, {"id": 669, "seek": 471296, "start": 4719.68, "end": 4726.0, "text": " having understood the meaning of a sentence subconsciously. At this point, there is still", "tokens": [1419, 7320, 264, 3620, 295, 257, 8174, 27389, 356, 13, 1711, 341, 935, 11, 456, 307, 920], "temperature": 0.0, "avg_logprob": -0.06318018809858575, "compression_ratio": 1.5805084745762712, "no_speech_prob": 2.875582140404731e-05}, {"id": 670, "seek": 471296, "start": 4726.0, "end": 4732.16, "text": " some residual context but we use that context productively rather than discard it to move", "tokens": [512, 27980, 4319, 457, 321, 764, 300, 4319, 1674, 3413, 2831, 813, 31597, 309, 281, 1286], "temperature": 0.0, "avg_logprob": -0.06318018809858575, "compression_ratio": 1.5805084745762712, "no_speech_prob": 2.875582140404731e-05}, {"id": 671, "seek": 471296, "start": 4732.16, "end": 4740.16, "text": " to higher levels. Some people claim we use models for all our thinking, but I'm using capital M", "tokens": [281, 2946, 4358, 13, 2188, 561, 3932, 321, 764, 5245, 337, 439, 527, 1953, 11, 457, 286, 478, 1228, 4238, 376], "temperature": 0.0, "avg_logprob": -0.06318018809858575, "compression_ratio": 1.5805084745762712, "no_speech_prob": 2.875582140404731e-05}, {"id": 672, "seek": 474016, "start": 4740.16, "end": 4748.16, "text": " model only to describe a completely context free abstraction. F equals M A is an example of that.", "tokens": [2316, 787, 281, 6786, 257, 2584, 4319, 1737, 37765, 13, 479, 6915, 376, 316, 307, 364, 1365, 295, 300, 13], "temperature": 0.0, "avg_logprob": -0.10239525499015019, "compression_ratio": 1.6244725738396624, "no_speech_prob": 4.084895772393793e-05}, {"id": 673, "seek": 474016, "start": 4748.88, "end": 4755.92, "text": " There is no need to check whether a car is a red car or a Toyota. The equation works not only for", "tokens": [821, 307, 572, 643, 281, 1520, 1968, 257, 1032, 307, 257, 2182, 1032, 420, 257, 22926, 13, 440, 5367, 1985, 406, 787, 337], "temperature": 0.0, "avg_logprob": -0.10239525499015019, "compression_ratio": 1.6244725738396624, "no_speech_prob": 4.084895772393793e-05}, {"id": 674, "seek": 474016, "start": 4755.92, "end": 4763.68, "text": " all cars but for all forces, masses and accelerations. We might come up with a special equation for", "tokens": [439, 5163, 457, 337, 439, 5874, 11, 23935, 293, 10172, 763, 13, 492, 1062, 808, 493, 365, 257, 2121, 5367, 337], "temperature": 0.0, "avg_logprob": -0.10239525499015019, "compression_ratio": 1.6244725738396624, "no_speech_prob": 4.084895772393793e-05}, {"id": 675, "seek": 474016, "start": 4763.68, "end": 4769.76, "text": " acceleration of Tesla cars which would require different inputs like battery charge level", "tokens": [17162, 295, 13666, 5163, 597, 576, 3651, 819, 15743, 411, 5809, 4602, 1496], "temperature": 0.0, "avg_logprob": -0.10239525499015019, "compression_ratio": 1.6244725738396624, "no_speech_prob": 4.084895772393793e-05}, {"id": 676, "seek": 476976, "start": 4769.76, "end": 4776.4800000000005, "text": " and software settings. That would not be a context free model since it would not work on a Toyota.", "tokens": [293, 4722, 6257, 13, 663, 576, 406, 312, 257, 4319, 1737, 2316, 1670, 309, 576, 406, 589, 322, 257, 22926, 13], "temperature": 0.0, "avg_logprob": -0.11035593818215762, "compression_ratio": 1.5708154506437768, "no_speech_prob": 4.747710045194253e-05}, {"id": 677, "seek": 476976, "start": 4777.12, "end": 4785.280000000001, "text": " For almost all tasks, basically, in everything except science and even there, only rarely,", "tokens": [1171, 1920, 439, 9608, 11, 1936, 11, 294, 1203, 3993, 3497, 293, 754, 456, 11, 787, 13752, 11], "temperature": 0.0, "avg_logprob": -0.11035593818215762, "compression_ratio": 1.5708154506437768, "no_speech_prob": 4.747710045194253e-05}, {"id": 678, "seek": 476976, "start": 4785.280000000001, "end": 4792.8, "text": " we only perform as much reduction as is necessary to get the job done. When learning to ski,", "tokens": [321, 787, 2042, 382, 709, 11004, 382, 307, 4818, 281, 483, 264, 1691, 1096, 13, 1133, 2539, 281, 14274, 11], "temperature": 0.0, "avg_logprob": -0.11035593818215762, "compression_ratio": 1.5708154506437768, "no_speech_prob": 4.747710045194253e-05}, {"id": 679, "seek": 476976, "start": 4792.8, "end": 4798.56, "text": " you only figure out how you yourself need to perform given your body and equipment.", "tokens": [291, 787, 2573, 484, 577, 291, 1803, 643, 281, 2042, 2212, 428, 1772, 293, 5927, 13], "temperature": 0.0, "avg_logprob": -0.11035593818215762, "compression_ratio": 1.5708154506437768, "no_speech_prob": 4.747710045194253e-05}, {"id": 680, "seek": 479856, "start": 4798.56, "end": 4804.88, "text": " We do not need to parameterize our skiing skills for someone with twice the body mass", "tokens": [492, 360, 406, 643, 281, 13075, 1125, 527, 32326, 3942, 337, 1580, 365, 6091, 264, 1772, 2758], "temperature": 0.0, "avg_logprob": -0.05188133603050595, "compression_ratio": 1.6666666666666667, "no_speech_prob": 2.6373820219305344e-05}, {"id": 681, "seek": 479856, "start": 4804.88, "end": 4811.04, "text": " because that would be useless to us for the purpose of our own skiing. But a scientist would", "tokens": [570, 300, 576, 312, 14115, 281, 505, 337, 264, 4334, 295, 527, 1065, 32326, 13, 583, 257, 12662, 576], "temperature": 0.0, "avg_logprob": -0.05188133603050595, "compression_ratio": 1.6666666666666667, "no_speech_prob": 2.6373820219305344e-05}, {"id": 682, "seek": 479856, "start": 4811.04, "end": 4817.84, "text": " have to go that far in order to parameterize away one more piece of context from the model", "tokens": [362, 281, 352, 300, 1400, 294, 1668, 281, 13075, 1125, 1314, 472, 544, 2522, 295, 4319, 490, 264, 2316], "temperature": 0.0, "avg_logprob": -0.05188133603050595, "compression_ratio": 1.6666666666666667, "no_speech_prob": 2.6373820219305344e-05}, {"id": 683, "seek": 479856, "start": 4817.84, "end": 4824.72, "text": " they are creating. For instance, when creating a skiing video game or designing a new ski,", "tokens": [436, 366, 4084, 13, 1171, 5197, 11, 562, 4084, 257, 32326, 960, 1216, 420, 14685, 257, 777, 14274, 11], "temperature": 0.0, "avg_logprob": -0.05188133603050595, "compression_ratio": 1.6666666666666667, "no_speech_prob": 2.6373820219305344e-05}, {"id": 684, "seek": 482472, "start": 4824.72, "end": 4830.96, "text": " if we consider the enormous amount of subconscious activity that happens in the brain,", "tokens": [498, 321, 1949, 264, 11322, 2372, 295, 27389, 5191, 300, 2314, 294, 264, 3567, 11], "temperature": 0.0, "avg_logprob": -0.09651906425888473, "compression_ratio": 1.6666666666666667, "no_speech_prob": 6.330890028038993e-05}, {"id": 685, "seek": 482472, "start": 4830.96, "end": 4837.52, "text": " we can safely say that partial reductions are the most common reductions. For instance,", "tokens": [321, 393, 11750, 584, 300, 14641, 40296, 366, 264, 881, 2689, 40296, 13, 1171, 5197, 11], "temperature": 0.0, "avg_logprob": -0.09651906425888473, "compression_ratio": 1.6666666666666667, "no_speech_prob": 6.330890028038993e-05}, {"id": 686, "seek": 482472, "start": 4837.52, "end": 4844.320000000001, "text": " when we take a step forward, our subconscious has analyzed our posture and velocity by using", "tokens": [562, 321, 747, 257, 1823, 2128, 11, 527, 27389, 575, 28181, 527, 18502, 293, 9269, 538, 1228], "temperature": 0.0, "avg_logprob": -0.09651906425888473, "compression_ratio": 1.6666666666666667, "no_speech_prob": 6.330890028038993e-05}, {"id": 687, "seek": 482472, "start": 4844.320000000001, "end": 4850.0, "text": " reduction based on low level nerve signals and is commanding leg muscles to contract an", "tokens": [11004, 2361, 322, 2295, 1496, 16355, 12354, 293, 307, 5622, 278, 1676, 9530, 281, 4364, 364], "temperature": 0.0, "avg_logprob": -0.09651906425888473, "compression_ratio": 1.6666666666666667, "no_speech_prob": 6.330890028038993e-05}, {"id": 688, "seek": 485000, "start": 4850.0, "end": 4857.04, "text": " up precisely timed sequence. This activity is something we are unaware of. Most of us don't", "tokens": [493, 13402, 44696, 8310, 13, 639, 5191, 307, 746, 321, 366, 32065, 295, 13, 4534, 295, 505, 500, 380], "temperature": 0.0, "avg_logprob": -0.09120962231658226, "compression_ratio": 1.5358649789029535, "no_speech_prob": 3.866543556796387e-05}, {"id": 689, "seek": 485000, "start": 4857.04, "end": 4864.0, "text": " even know what leg muscles we have. And there would be no time to perform reduction all the way to", "tokens": [754, 458, 437, 1676, 9530, 321, 362, 13, 400, 456, 576, 312, 572, 565, 281, 2042, 11004, 439, 264, 636, 281], "temperature": 0.0, "avg_logprob": -0.09120962231658226, "compression_ratio": 1.5358649789029535, "no_speech_prob": 3.866543556796387e-05}, {"id": 690, "seek": 485000, "start": 4864.0, "end": 4870.56, "text": " models. That process takes a minimum of a half second and you don't have that kind of time", "tokens": [5245, 13, 663, 1399, 2516, 257, 7285, 295, 257, 1922, 1150, 293, 291, 500, 380, 362, 300, 733, 295, 565], "temperature": 0.0, "avg_logprob": -0.09120962231658226, "compression_ratio": 1.5358649789029535, "no_speech_prob": 3.866543556796387e-05}, {"id": 691, "seek": 485000, "start": 4870.56, "end": 4877.04, "text": " available to respond to an imbalance when walking or skiing. Reduction in society.", "tokens": [2435, 281, 4196, 281, 364, 43007, 562, 4494, 420, 32326, 13, 4477, 27549, 294, 4086, 13], "temperature": 0.0, "avg_logprob": -0.09120962231658226, "compression_ratio": 1.5358649789029535, "no_speech_prob": 3.866543556796387e-05}, {"id": 692, "seek": 487704, "start": 4877.04, "end": 4884.24, "text": " Most of us get paid to understand whatever we need to understand in order to perform our jobs.", "tokens": [4534, 295, 505, 483, 4835, 281, 1223, 2035, 321, 643, 281, 1223, 294, 1668, 281, 2042, 527, 4782, 13], "temperature": 0.0, "avg_logprob": -0.11111348174339117, "compression_ratio": 1.708133971291866, "no_speech_prob": 6.127893720986322e-05}, {"id": 693, "seek": 487704, "start": 4884.88, "end": 4891.76, "text": " In other words, most of us get paid to do reduction. If you are approving building permits,", "tokens": [682, 661, 2283, 11, 881, 295, 505, 483, 4835, 281, 360, 11004, 13, 759, 291, 366, 2075, 798, 2390, 30990, 11], "temperature": 0.0, "avg_logprob": -0.11111348174339117, "compression_ratio": 1.708133971291866, "no_speech_prob": 6.127893720986322e-05}, {"id": 694, "seek": 487704, "start": 4891.76, "end": 4900.16, "text": " you reduce a stack of forms to a one bit verdict of approved or rejected. We accelerate reduction,", "tokens": [291, 5407, 257, 8630, 295, 6422, 281, 257, 472, 857, 33957, 295, 10826, 420, 15749, 13, 492, 21341, 11004, 11], "temperature": 0.0, "avg_logprob": -0.11111348174339117, "compression_ratio": 1.708133971291866, "no_speech_prob": 6.127893720986322e-05}, {"id": 695, "seek": 487704, "start": 4900.16, "end": 4904.72, "text": " and this is the main reason most of us haven't been replaced by robots.", "tokens": [293, 341, 307, 264, 2135, 1778, 881, 295, 505, 2378, 380, 668, 10772, 538, 14733, 13], "temperature": 0.0, "avg_logprob": -0.11111348174339117, "compression_ratio": 1.708133971291866, "no_speech_prob": 6.127893720986322e-05}, {"id": 696, "seek": 490472, "start": 4904.72, "end": 4911.4400000000005, "text": " But we see that when future understanding machines can perform reduction by themselves,", "tokens": [583, 321, 536, 300, 562, 2027, 3701, 8379, 393, 2042, 11004, 538, 2969, 11], "temperature": 0.0, "avg_logprob": -0.12799602899795923, "compression_ratio": 1.5517241379310345, "no_speech_prob": 4.8778674681670964e-05}, {"id": 697, "seek": 490472, "start": 4911.4400000000005, "end": 4916.16, "text": " then we are unlikely to get paid for it. Levels of reduction.", "tokens": [550, 321, 366, 17518, 281, 483, 4835, 337, 309, 13, 16872, 82, 295, 11004, 13], "temperature": 0.0, "avg_logprob": -0.12799602899795923, "compression_ratio": 1.5517241379310345, "no_speech_prob": 4.8778674681670964e-05}, {"id": 698, "seek": 490472, "start": 4916.96, "end": 4923.04, "text": " Suppose a young man and a young woman fall in love, something happens to mess it all up,", "tokens": [21360, 257, 2037, 587, 293, 257, 2037, 3059, 2100, 294, 959, 11, 746, 2314, 281, 2082, 309, 439, 493, 11], "temperature": 0.0, "avg_logprob": -0.12799602899795923, "compression_ratio": 1.5517241379310345, "no_speech_prob": 4.8778674681670964e-05}, {"id": 699, "seek": 490472, "start": 4923.04, "end": 4928.400000000001, "text": " and then they sort this out and reunite. This is what happened in the man's,", "tokens": [293, 550, 436, 1333, 341, 484, 293, 14480, 642, 13, 639, 307, 437, 2011, 294, 264, 587, 311, 11], "temperature": 0.0, "avg_logprob": -0.12799602899795923, "compression_ratio": 1.5517241379310345, "no_speech_prob": 4.8778674681670964e-05}, {"id": 700, "seek": 492840, "start": 4928.4, "end": 4935.44, "text": " which mundane reality. Suppose the man wants to share this experience, because there was some", "tokens": [597, 43497, 4103, 13, 21360, 264, 587, 2738, 281, 2073, 341, 1752, 11, 570, 456, 390, 512], "temperature": 0.0, "avg_logprob": -0.07699541250864665, "compression_ratio": 1.6359649122807018, "no_speech_prob": 8.166492625605315e-05}, {"id": 701, "seek": 492840, "start": 4935.44, "end": 4940.799999999999, "text": " moral to the story that he thinks would be interesting to others and possibly important.", "tokens": [9723, 281, 264, 1657, 300, 415, 7309, 576, 312, 1880, 281, 2357, 293, 6264, 1021, 13], "temperature": 0.0, "avg_logprob": -0.07699541250864665, "compression_ratio": 1.6359649122807018, "no_speech_prob": 8.166492625605315e-05}, {"id": 702, "seek": 492840, "start": 4941.44, "end": 4947.12, "text": " He could analyze what happened and figure out which were the key events in the saga and then", "tokens": [634, 727, 12477, 437, 2011, 293, 2573, 484, 597, 645, 264, 2141, 3931, 294, 264, 34250, 293, 550], "temperature": 0.0, "avg_logprob": -0.07699541250864665, "compression_ratio": 1.6359649122807018, "no_speech_prob": 8.166492625605315e-05}, {"id": 703, "seek": 492840, "start": 4947.12, "end": 4954.16, "text": " have actors on a stage re-enact the story as a play. This is a reduction because the boring parts", "tokens": [362, 10037, 322, 257, 3233, 319, 12, 268, 578, 264, 1657, 382, 257, 862, 13, 639, 307, 257, 11004, 570, 264, 9989, 3166], "temperature": 0.0, "avg_logprob": -0.07699541250864665, "compression_ratio": 1.6359649122807018, "no_speech_prob": 8.166492625605315e-05}, {"id": 704, "seek": 495416, "start": 4954.16, "end": 4961.04, "text": " of the story would not be part of the play. They are discarded as irrelevant, but the story would", "tokens": [295, 264, 1657, 576, 406, 312, 644, 295, 264, 862, 13, 814, 366, 45469, 382, 28682, 11, 457, 264, 1657, 576], "temperature": 0.0, "avg_logprob": -0.0485043078660965, "compression_ratio": 1.7174887892376682, "no_speech_prob": 3.794617441599257e-05}, {"id": 705, "seek": 495416, "start": 4961.04, "end": 4968.08, "text": " be acted out by real people in front of a live audience. If you are in the audience, you can move", "tokens": [312, 20359, 484, 538, 957, 561, 294, 1868, 295, 257, 1621, 4034, 13, 759, 291, 366, 294, 264, 4034, 11, 291, 393, 1286], "temperature": 0.0, "avg_logprob": -0.0485043078660965, "compression_ratio": 1.7174887892376682, "no_speech_prob": 3.794617441599257e-05}, {"id": 706, "seek": 495416, "start": 4968.08, "end": 4974.48, "text": " your head to see behind any actor on the stage and you can clearly see everything on the stage,", "tokens": [428, 1378, 281, 536, 2261, 604, 8747, 322, 264, 3233, 293, 291, 393, 4448, 536, 1203, 322, 264, 3233, 11], "temperature": 0.0, "avg_logprob": -0.0485043078660965, "compression_ratio": 1.7174887892376682, "no_speech_prob": 3.794617441599257e-05}, {"id": 707, "seek": 495416, "start": 4974.48, "end": 4982.08, "text": " not just one actor speaking at a time. He can make a movie about it. Now your point of view", "tokens": [406, 445, 472, 8747, 4124, 412, 257, 565, 13, 634, 393, 652, 257, 3169, 466, 309, 13, 823, 428, 935, 295, 1910], "temperature": 0.0, "avg_logprob": -0.0485043078660965, "compression_ratio": 1.7174887892376682, "no_speech_prob": 3.794617441599257e-05}, {"id": 708, "seek": 498208, "start": 4982.08, "end": 4989.5199999999995, "text": " is pre-defined by the camera angle and cropping. You can no longer see behind an actor, and you", "tokens": [307, 659, 12, 37716, 538, 264, 2799, 5802, 293, 4848, 3759, 13, 509, 393, 572, 2854, 536, 2261, 364, 8747, 11, 293, 291], "temperature": 0.0, "avg_logprob": -0.11680482563219573, "compression_ratio": 1.662280701754386, "no_speech_prob": 3.268839282100089e-05}, {"id": 709, "seek": 498208, "start": 4989.5199999999995, "end": 4996.32, "text": " can often only see those actors that are involved in the main action. He could write a book about it.", "tokens": [393, 2049, 787, 536, 729, 10037, 300, 366, 3288, 294, 264, 2135, 3069, 13, 634, 727, 2464, 257, 1446, 466, 309, 13], "temperature": 0.0, "avg_logprob": -0.11680482563219573, "compression_ratio": 1.662280701754386, "no_speech_prob": 3.268839282100089e-05}, {"id": 710, "seek": 498208, "start": 4996.96, "end": 5003.04, "text": " We no longer can see even the people described in the book, except in our imagination.", "tokens": [492, 572, 2854, 393, 536, 754, 264, 561, 7619, 294, 264, 1446, 11, 3993, 294, 527, 12938, 13], "temperature": 0.0, "avg_logprob": -0.11680482563219573, "compression_ratio": 1.662280701754386, "no_speech_prob": 3.268839282100089e-05}, {"id": 711, "seek": 498208, "start": 5003.76, "end": 5011.28, "text": " A critic review in the theater play may reduce it to, Boy meets girl, Boy loses girl, Boy gets", "tokens": [316, 7850, 3131, 294, 264, 10612, 862, 815, 5407, 309, 281, 11, 9486, 13961, 2013, 11, 9486, 18293, 2013, 11, 9486, 2170], "temperature": 0.0, "avg_logprob": -0.11680482563219573, "compression_ratio": 1.662280701754386, "no_speech_prob": 3.268839282100089e-05}, {"id": 712, "seek": 501128, "start": 5011.28, "end": 5018.639999999999, "text": " girl. A drama school graduate may summarize it as a double reversal plot. This is a description", "tokens": [2013, 13, 316, 9412, 1395, 8080, 815, 20858, 309, 382, 257, 3834, 42778, 7542, 13, 639, 307, 257, 3855], "temperature": 0.0, "avg_logprob": -0.09647201203011177, "compression_ratio": 1.4723618090452262, "no_speech_prob": 0.00010708724585128948}, {"id": 713, "seek": 501128, "start": 5018.639999999999, "end": 5026.16, "text": " that is so free from context, doesn't even specify boys or girls that it could be argued it qualifies", "tokens": [300, 307, 370, 1737, 490, 4319, 11, 1177, 380, 754, 16500, 6347, 420, 4519, 300, 309, 727, 312, 20219, 309, 4101, 11221], "temperature": 0.0, "avg_logprob": -0.09647201203011177, "compression_ratio": 1.4723618090452262, "no_speech_prob": 0.00010708724585128948}, {"id": 714, "seek": 501128, "start": 5026.16, "end": 5035.36, "text": " to be called a model. Plays, movies, books, stories, tropes, etc. are all partial reductions of", "tokens": [281, 312, 1219, 257, 2316, 13, 2149, 3772, 11, 6233, 11, 3642, 11, 3676, 11, 9006, 279, 11, 5183, 13, 366, 439, 14641, 40296, 295], "temperature": 0.0, "avg_logprob": -0.09647201203011177, "compression_ratio": 1.4723618090452262, "no_speech_prob": 0.00010708724585128948}, {"id": 715, "seek": 503536, "start": 5035.36, "end": 5043.92, "text": " reality, and some are more reduced than others. Just like in the red Toyota case, we need to find", "tokens": [4103, 11, 293, 512, 366, 544, 9212, 813, 2357, 13, 1449, 411, 294, 264, 2182, 22926, 1389, 11, 321, 643, 281, 915], "temperature": 0.0, "avg_logprob": -0.07525329799442501, "compression_ratio": 1.6271186440677967, "no_speech_prob": 5.630600935546681e-05}, {"id": 716, "seek": 503536, "start": 5043.92, "end": 5050.96, "text": " the appropriate level of abstraction to work with. The young man in the example, when writing a", "tokens": [264, 6854, 1496, 295, 37765, 281, 589, 365, 13, 440, 2037, 587, 294, 264, 1365, 11, 562, 3579, 257], "temperature": 0.0, "avg_logprob": -0.07525329799442501, "compression_ratio": 1.6271186440677967, "no_speech_prob": 5.630600935546681e-05}, {"id": 717, "seek": 503536, "start": 5050.96, "end": 5057.599999999999, "text": " book or a screenplay, has much in common with a scientist trying to describe something in nature", "tokens": [1446, 420, 257, 2568, 2858, 11, 575, 709, 294, 2689, 365, 257, 12662, 1382, 281, 6786, 746, 294, 3687], "temperature": 0.0, "avg_logprob": -0.07525329799442501, "compression_ratio": 1.6271186440677967, "no_speech_prob": 5.630600935546681e-05}, {"id": 718, "seek": 503536, "start": 5057.599999999999, "end": 5064.5599999999995, "text": " in a reusable context free manner by reducing it to a model. They are model makers, or are at", "tokens": [294, 257, 41807, 4319, 1737, 9060, 538, 12245, 309, 281, 257, 2316, 13, 814, 366, 2316, 19323, 11, 420, 366, 412], "temperature": 0.0, "avg_logprob": -0.07525329799442501, "compression_ratio": 1.6271186440677967, "no_speech_prob": 5.630600935546681e-05}, {"id": 719, "seek": 506456, "start": 5064.56, "end": 5071.360000000001, "text": " least performing partial reduction. They are discarding the irrelevant bits. The opposite of", "tokens": [1935, 10205, 14641, 11004, 13, 814, 366, 31597, 278, 264, 28682, 9239, 13, 440, 6182, 295], "temperature": 0.0, "avg_logprob": -0.06484819593883696, "compression_ratio": 1.7757009345794392, "no_speech_prob": 5.7167490012943745e-05}, {"id": 720, "seek": 506456, "start": 5071.360000000001, "end": 5078.64, "text": " reduction. We also need to be able to move in the opposite direction, from models to reality,", "tokens": [11004, 13, 492, 611, 643, 281, 312, 1075, 281, 1286, 294, 264, 6182, 3513, 11, 490, 5245, 281, 4103, 11], "temperature": 0.0, "avg_logprob": -0.06484819593883696, "compression_ratio": 1.7757009345794392, "no_speech_prob": 5.7167490012943745e-05}, {"id": 721, "seek": 506456, "start": 5079.280000000001, "end": 5086.240000000001, "text": " or at least from more abstract partial models to partial models closer to reality. When an actor", "tokens": [420, 412, 1935, 490, 544, 12649, 14641, 5245, 281, 14641, 5245, 4966, 281, 4103, 13, 1133, 364, 8747], "temperature": 0.0, "avg_logprob": -0.06484819593883696, "compression_ratio": 1.7757009345794392, "no_speech_prob": 5.7167490012943745e-05}, {"id": 722, "seek": 506456, "start": 5086.240000000001, "end": 5092.56, "text": " is given a screenplay, they know it only contains rough directions for what to do and what lines", "tokens": [307, 2212, 257, 2568, 2858, 11, 436, 458, 309, 787, 8306, 5903, 11095, 337, 437, 281, 360, 293, 437, 3876], "temperature": 0.0, "avg_logprob": -0.06484819593883696, "compression_ratio": 1.7757009345794392, "no_speech_prob": 5.7167490012943745e-05}, {"id": 723, "seek": 509256, "start": 5092.56, "end": 5100.160000000001, "text": " to say. The actor's job is to give a little of themselves to flesh out the screenplay to actual", "tokens": [281, 584, 13, 440, 8747, 311, 1691, 307, 281, 976, 257, 707, 295, 2969, 281, 12497, 484, 264, 2568, 2858, 281, 3539], "temperature": 0.0, "avg_logprob": -0.04926640005672679, "compression_ratio": 1.6724890829694323, "no_speech_prob": 8.70528383529745e-05}, {"id": 724, "seek": 509256, "start": 5100.160000000001, "end": 5108.72, "text": " actions, including creating, synthesizing, the appropriate display of emotions, tone of voice,", "tokens": [5909, 11, 3009, 4084, 11, 26617, 3319, 11, 264, 6854, 4674, 295, 8462, 11, 8027, 295, 3177, 11], "temperature": 0.0, "avg_logprob": -0.04926640005672679, "compression_ratio": 1.6724890829694323, "no_speech_prob": 8.70528383529745e-05}, {"id": 725, "seek": 509256, "start": 5108.72, "end": 5116.160000000001, "text": " and body language. They use their experience as people and as actors. They use elements of their", "tokens": [293, 1772, 2856, 13, 814, 764, 641, 1752, 382, 561, 293, 382, 10037, 13, 814, 764, 4959, 295, 641], "temperature": 0.0, "avg_logprob": -0.04926640005672679, "compression_ratio": 1.6724890829694323, "no_speech_prob": 8.70528383529745e-05}, {"id": 726, "seek": 509256, "start": 5116.160000000001, "end": 5122.320000000001, "text": " past lives and skills they have acquired by training to create something people in the audience", "tokens": [1791, 2909, 293, 3942, 436, 362, 17554, 538, 3097, 281, 1884, 746, 561, 294, 264, 4034], "temperature": 0.0, "avg_logprob": -0.04926640005672679, "compression_ratio": 1.6724890829694323, "no_speech_prob": 8.70528383529745e-05}, {"id": 727, "seek": 512232, "start": 5122.32, "end": 5130.08, "text": " might relate to. For example, they may repurpose a personal experience. He is sad as when my", "tokens": [1062, 10961, 281, 13, 1171, 1365, 11, 436, 815, 1085, 31345, 257, 2973, 1752, 13, 634, 307, 4227, 382, 562, 452], "temperature": 0.0, "avg_logprob": -0.14313249718653012, "compression_ratio": 1.521505376344086, "no_speech_prob": 0.00011005511623807251}, {"id": 728, "seek": 512232, "start": 5130.08, "end": 5138.88, "text": " hamster died. Things they learned in drama school, such as speaking, singing, dancing, and swordplay,", "tokens": [7852, 3120, 4539, 13, 9514, 436, 3264, 294, 9412, 1395, 11, 1270, 382, 4124, 11, 6726, 11, 8898, 11, 293, 10576, 2858, 11], "temperature": 0.0, "avg_logprob": -0.14313249718653012, "compression_ratio": 1.521505376344086, "no_speech_prob": 0.00011005511623807251}, {"id": 729, "seek": 512232, "start": 5138.88, "end": 5146.719999999999, "text": " from other actors, what would bogart do, from fiction, from other movies and plays, etc.", "tokens": [490, 661, 10037, 11, 437, 576, 26132, 446, 360, 11, 490, 13266, 11, 490, 661, 6233, 293, 5749, 11, 5183, 13], "temperature": 0.0, "avg_logprob": -0.14313249718653012, "compression_ratio": 1.521505376344086, "no_speech_prob": 0.00011005511623807251}, {"id": 730, "seek": 514672, "start": 5146.72, "end": 5154.88, "text": " The actor's artist who convey whatever the script intends to convey, emotions, a morality cookie,", "tokens": [440, 8747, 311, 5748, 567, 16965, 2035, 264, 5755, 560, 2581, 281, 16965, 11, 8462, 11, 257, 29106, 14417, 11], "temperature": 0.0, "avg_logprob": -0.10870170593261719, "compression_ratio": 1.6422018348623852, "no_speech_prob": 3.771047340705991e-05}, {"id": 731, "seek": 514672, "start": 5154.88, "end": 5162.8, "text": " a political position, titillation, surprise, and so on. Starting from the simple model,", "tokens": [257, 3905, 2535, 11, 3459, 373, 399, 11, 6365, 11, 293, 370, 322, 13, 16217, 490, 264, 2199, 2316, 11], "temperature": 0.0, "avg_logprob": -0.10870170593261719, "compression_ratio": 1.6422018348623852, "no_speech_prob": 3.771047340705991e-05}, {"id": 732, "seek": 514672, "start": 5162.8, "end": 5169.360000000001, "text": " the screenplay, their job is similar to an engineer's when they are faced with a problem", "tokens": [264, 2568, 2858, 11, 641, 1691, 307, 2531, 281, 364, 11403, 311, 562, 436, 366, 11446, 365, 257, 1154], "temperature": 0.0, "avg_logprob": -0.10870170593261719, "compression_ratio": 1.6422018348623852, "no_speech_prob": 3.771047340705991e-05}, {"id": 733, "seek": 514672, "start": 5169.360000000001, "end": 5175.4400000000005, "text": " and use a model to solve it. The engineer would use their experience to decide that", "tokens": [293, 764, 257, 2316, 281, 5039, 309, 13, 440, 11403, 576, 764, 641, 1752, 281, 4536, 300], "temperature": 0.0, "avg_logprob": -0.10870170593261719, "compression_ratio": 1.6422018348623852, "no_speech_prob": 3.771047340705991e-05}, {"id": 734, "seek": 517544, "start": 5175.44, "end": 5182.5599999999995, "text": " M is the mass of the car and not the tire pressure. The actor decides that sadness", "tokens": [376, 307, 264, 2758, 295, 264, 1032, 293, 406, 264, 11756, 3321, 13, 440, 8747, 14898, 300, 22462], "temperature": 0.0, "avg_logprob": -0.10067920684814453, "compression_ratio": 1.4972067039106145, "no_speech_prob": 3.647111952886917e-05}, {"id": 735, "seek": 517544, "start": 5182.5599999999995, "end": 5191.28, "text": " is more appropriate than grief for a certain scene, etc. I call this process, which is the", "tokens": [307, 544, 6854, 813, 18998, 337, 257, 1629, 4145, 11, 5183, 13, 286, 818, 341, 1399, 11, 597, 307, 264], "temperature": 0.0, "avg_logprob": -0.10067920684814453, "compression_ratio": 1.4972067039106145, "no_speech_prob": 3.647111952886917e-05}, {"id": 736, "seek": 517544, "start": 5191.28, "end": 5199.28, "text": " opposite of reduction by the name it is used in problem solving application. We use a model to", "tokens": [6182, 295, 11004, 538, 264, 1315, 309, 307, 1143, 294, 1154, 12606, 3861, 13, 492, 764, 257, 2316, 281], "temperature": 0.0, "avg_logprob": -0.10067920684814453, "compression_ratio": 1.4972067039106145, "no_speech_prob": 3.647111952886917e-05}, {"id": 737, "seek": 519928, "start": 5199.28, "end": 5207.36, "text": " simplify a problem situation, moving it into an abstract and pure model space. We solve the", "tokens": [20460, 257, 1154, 2590, 11, 2684, 309, 666, 364, 12649, 293, 6075, 2316, 1901, 13, 492, 5039, 264], "temperature": 0.0, "avg_logprob": -0.08261045080716492, "compression_ratio": 1.5284090909090908, "no_speech_prob": 7.531543087679893e-05}, {"id": 738, "seek": 519928, "start": 5207.36, "end": 5215.04, "text": " problem there by performing math, perhaps, and then apply the answer to our rich reality", "tokens": [1154, 456, 538, 10205, 5221, 11, 4317, 11, 293, 550, 3079, 264, 1867, 281, 527, 4593, 4103], "temperature": 0.0, "avg_logprob": -0.08261045080716492, "compression_ratio": 1.5284090909090908, "no_speech_prob": 7.531543087679893e-05}, {"id": 739, "seek": 519928, "start": 5215.04, "end": 5222.08, "text": " to the problem we are trying to solve. Many of you may recognize the word application or", "tokens": [281, 264, 1154, 321, 366, 1382, 281, 5039, 13, 5126, 295, 291, 815, 5521, 264, 1349, 3861, 420], "temperature": 0.0, "avg_logprob": -0.08261045080716492, "compression_ratio": 1.5284090909090908, "no_speech_prob": 7.531543087679893e-05}, {"id": 740, "seek": 522208, "start": 5222.08, "end": 5229.84, "text": " its abbreviation, app. That's not as far-fetched as it might seem. Apps are software-based models.", "tokens": [1080, 35839, 399, 11, 724, 13, 663, 311, 406, 382, 1400, 12, 69, 7858, 292, 382, 309, 1062, 1643, 13, 32231, 366, 4722, 12, 6032, 5245, 13], "temperature": 0.0, "avg_logprob": -0.14881526722627528, "compression_ratio": 1.446236559139785, "no_speech_prob": 7.641966658411548e-05}, {"id": 741, "seek": 522208, "start": 5230.5599999999995, "end": 5236.24, "text": " Reduction in application and brains. Back to the issue of partial reductions.", "tokens": [4477, 27549, 294, 3861, 293, 15442, 13, 5833, 281, 264, 2734, 295, 14641, 40296, 13], "temperature": 0.0, "avg_logprob": -0.14881526722627528, "compression_ratio": 1.446236559139785, "no_speech_prob": 7.641966658411548e-05}, {"id": 742, "seek": 522208, "start": 5236.96, "end": 5244.4, "text": " Consider the actor reading a screenplay. They are using their eyes to gather pixels of color", "tokens": [17416, 264, 8747, 3760, 257, 2568, 2858, 13, 814, 366, 1228, 641, 2575, 281, 5448, 18668, 295, 2017], "temperature": 0.0, "avg_logprob": -0.14881526722627528, "compression_ratio": 1.446236559139785, "no_speech_prob": 7.641966658411548e-05}, {"id": 743, "seek": 524440, "start": 5244.4, "end": 5252.32, "text": " and orientation. The brain then performs pattern matching, reduction, from these low-level signals", "tokens": [293, 14764, 13, 440, 3567, 550, 26213, 5102, 14324, 11, 11004, 11, 490, 613, 2295, 12, 12418, 12354], "temperature": 0.0, "avg_logprob": -0.08132064342498779, "compression_ratio": 1.7149321266968325, "no_speech_prob": 6.114709685789421e-05}, {"id": 744, "seek": 524440, "start": 5252.32, "end": 5260.16, "text": " to letters, words, to language, to high-level concepts like love and separation, and eventually", "tokens": [281, 7825, 11, 2283, 11, 281, 2856, 11, 281, 1090, 12, 12418, 10392, 411, 959, 293, 14634, 11, 293, 4728], "temperature": 0.0, "avg_logprob": -0.08132064342498779, "compression_ratio": 1.7149321266968325, "no_speech_prob": 6.114709685789421e-05}, {"id": 745, "seek": 524440, "start": 5260.16, "end": 5266.5599999999995, "text": " to a high-level understanding of the playwright's intents. The actor then takes this high-level", "tokens": [281, 257, 1090, 12, 12418, 3701, 295, 264, 862, 37752, 311, 560, 791, 13, 440, 8747, 550, 2516, 341, 1090, 12, 12418], "temperature": 0.0, "avg_logprob": -0.08132064342498779, "compression_ratio": 1.7149321266968325, "no_speech_prob": 6.114709685789421e-05}, {"id": 746, "seek": 524440, "start": 5266.5599999999995, "end": 5272.639999999999, "text": " understanding and by performing application, they add their own experience to the script", "tokens": [3701, 293, 538, 10205, 3861, 11, 436, 909, 641, 1065, 1752, 281, 264, 5755], "temperature": 0.0, "avg_logprob": -0.08132064342498779, "compression_ratio": 1.7149321266968325, "no_speech_prob": 6.114709685789421e-05}, {"id": 747, "seek": 527264, "start": 5272.64, "end": 5279.360000000001, "text": " to get closer to reality and their performance. Our brains are capable of moving up and down", "tokens": [281, 483, 4966, 281, 4103, 293, 641, 3389, 13, 2621, 15442, 366, 8189, 295, 2684, 493, 293, 760], "temperature": 0.0, "avg_logprob": -0.06836338837941487, "compression_ratio": 1.5377358490566038, "no_speech_prob": 5.0315891712671146e-05}, {"id": 748, "seek": 527264, "start": 5279.360000000001, "end": 5286.08, "text": " many levels of abstraction at once. Perhaps it tracks all of them simultaneously,", "tokens": [867, 4358, 295, 37765, 412, 1564, 13, 10517, 309, 10218, 439, 295, 552, 16561, 11], "temperature": 0.0, "avg_logprob": -0.06836338837941487, "compression_ratio": 1.5377358490566038, "no_speech_prob": 5.0315891712671146e-05}, {"id": 749, "seek": 527264, "start": 5286.08, "end": 5292.08, "text": " keeping layers of abstraction separate. This is a clue for why deep neural networks", "tokens": [5145, 7914, 295, 37765, 4994, 13, 639, 307, 257, 13602, 337, 983, 2452, 18161, 9590], "temperature": 0.0, "avg_logprob": -0.06836338837941487, "compression_ratio": 1.5377358490566038, "no_speech_prob": 5.0315891712671146e-05}, {"id": 750, "seek": 527264, "start": 5292.08, "end": 5296.96, "text": " perform better than shallow ones. Which is what we'll discuss next.", "tokens": [2042, 1101, 813, 20488, 2306, 13, 3013, 307, 437, 321, 603, 2248, 958, 13], "temperature": 0.0, "avg_logprob": -0.06836338837941487, "compression_ratio": 1.5377358490566038, "no_speech_prob": 5.0315891712671146e-05}, {"id": 751, "seek": 529696, "start": 5296.96, "end": 5305.12, "text": " Chapter 5. Why Deep Learning Works. Deep learning performs epistemic reduction.", "tokens": [18874, 1025, 13, 1545, 14895, 15205, 27914, 13, 14895, 2539, 26213, 2388, 468, 3438, 11004, 13], "temperature": 0.0, "avg_logprob": -0.1270710484365399, "compression_ratio": 1.6636363636363636, "no_speech_prob": 0.00020144987502135336}, {"id": 752, "seek": 529696, "start": 5306.32, "end": 5312.72, "text": " A math-free computer science-free description of why deep learning works. We have now built", "tokens": [316, 5221, 12, 10792, 3820, 3497, 12, 10792, 3855, 295, 983, 2452, 2539, 1985, 13, 492, 362, 586, 3094], "temperature": 0.0, "avg_logprob": -0.1270710484365399, "compression_ratio": 1.6636363636363636, "no_speech_prob": 0.00020144987502135336}, {"id": 753, "seek": 529696, "start": 5312.72, "end": 5319.44, "text": " a base of theory for why AI works, what models are, and how to create them, what reductionism", "tokens": [257, 3096, 295, 5261, 337, 983, 7318, 1985, 11, 437, 5245, 366, 11, 293, 577, 281, 1884, 552, 11, 437, 11004, 1434], "temperature": 0.0, "avg_logprob": -0.1270710484365399, "compression_ratio": 1.6636363636363636, "no_speech_prob": 0.00020144987502135336}, {"id": 754, "seek": 531944, "start": 5319.44, "end": 5327.04, "text": " and holism are, and what the process of reduction is. These are the fundamentals of AI epistemology.", "tokens": [293, 4091, 1434, 366, 11, 293, 437, 264, 1399, 295, 11004, 307, 13, 1981, 366, 264, 29505, 295, 7318, 2388, 43958, 1793, 13], "temperature": 0.0, "avg_logprob": -0.0724979964169589, "compression_ratio": 1.618421052631579, "no_speech_prob": 3.3415431971661747e-05}, {"id": 755, "seek": 531944, "start": 5327.759999999999, "end": 5333.759999999999, "text": " This base allows us to discuss various strategies to move towards understanding machines in a", "tokens": [639, 3096, 4045, 505, 281, 2248, 3683, 9029, 281, 1286, 3030, 3701, 8379, 294, 257], "temperature": 0.0, "avg_logprob": -0.0724979964169589, "compression_ratio": 1.618421052631579, "no_speech_prob": 3.3415431971661747e-05}, {"id": 756, "seek": 531944, "start": 5333.759999999999, "end": 5340.24, "text": " well-understood and controlled manner. We are now ready to discuss why deep learning,", "tokens": [731, 12, 6617, 6431, 293, 10164, 9060, 13, 492, 366, 586, 1919, 281, 2248, 983, 2452, 2539, 11], "temperature": 0.0, "avg_logprob": -0.0724979964169589, "compression_ratio": 1.618421052631579, "no_speech_prob": 3.3415431971661747e-05}, {"id": 757, "seek": 531944, "start": 5340.24, "end": 5347.919999999999, "text": " DL, works. This is the fifth and last entry in the AI epistemology primer. Deep learning", "tokens": [413, 43, 11, 1985, 13, 639, 307, 264, 9266, 293, 1036, 8729, 294, 264, 7318, 2388, 43958, 1793, 12595, 13, 14895, 2539], "temperature": 0.0, "avg_logprob": -0.0724979964169589, "compression_ratio": 1.618421052631579, "no_speech_prob": 3.3415431971661747e-05}, {"id": 758, "seek": 534792, "start": 5347.92, "end": 5354.08, "text": " performs reduction. This is an unsurprising claim, considering the preceding chapters.", "tokens": [26213, 11004, 13, 639, 307, 364, 2693, 374, 26203, 3932, 11, 8079, 264, 16969, 278, 20013, 13], "temperature": 0.0, "avg_logprob": -0.08547114118745056, "compression_ratio": 1.5784753363228698, "no_speech_prob": 5.488434180733748e-05}, {"id": 759, "seek": 534792, "start": 5354.8, "end": 5361.68, "text": " There are several mutually compatible theories for how deep learning works. But just as in", "tokens": [821, 366, 2940, 39144, 18218, 13667, 337, 577, 2452, 2539, 1985, 13, 583, 445, 382, 294], "temperature": 0.0, "avg_logprob": -0.08547114118745056, "compression_ratio": 1.5784753363228698, "no_speech_prob": 5.488434180733748e-05}, {"id": 760, "seek": 534792, "start": 5361.68, "end": 5368.64, "text": " the first chapter, we will now discuss the epistemological aspects, why it works,", "tokens": [264, 700, 7187, 11, 321, 486, 586, 2248, 264, 2388, 43958, 4383, 7270, 11, 983, 309, 1985, 11], "temperature": 0.0, "avg_logprob": -0.08547114118745056, "compression_ratio": 1.5784753363228698, "no_speech_prob": 5.488434180733748e-05}, {"id": 761, "seek": 534792, "start": 5368.64, "end": 5375.52, "text": " from several viewpoints and levels, starting from the bottom. We would use examples from the", "tokens": [490, 2940, 1910, 20552, 293, 4358, 11, 2891, 490, 264, 2767, 13, 492, 576, 764, 5110, 490, 264], "temperature": 0.0, "avg_logprob": -0.08547114118745056, "compression_ratio": 1.5784753363228698, "no_speech_prob": 5.488434180733748e-05}, {"id": 762, "seek": 537552, "start": 5375.52, "end": 5382.72, "text": " TensorFlow system and API as a library, as a stand-in for all deep learning family algorithms", "tokens": [37624, 1185, 293, 9362, 382, 257, 6405, 11, 382, 257, 1463, 12, 259, 337, 439, 2452, 2539, 1605, 14642], "temperature": 0.0, "avg_logprob": -0.08802585344056825, "compression_ratio": 1.5208333333333333, "no_speech_prob": 8.392597374040633e-05}, {"id": 763, "seek": 537552, "start": 5382.72, "end": 5389.68, "text": " and TF programs, because the available API functions heavily shape and constrain solutions", "tokens": [293, 40964, 4268, 11, 570, 264, 2435, 9362, 6828, 10950, 3909, 293, 1817, 7146, 6547], "temperature": 0.0, "avg_logprob": -0.08802585344056825, "compression_ratio": 1.5208333333333333, "no_speech_prob": 8.392597374040633e-05}, {"id": 764, "seek": 537552, "start": 5389.68, "end": 5395.040000000001, "text": " that can be implemented in this space. And the generalization should be straightforward enough.", "tokens": [300, 393, 312, 12270, 294, 341, 1901, 13, 400, 264, 2674, 2144, 820, 312, 15325, 1547, 13], "temperature": 0.0, "avg_logprob": -0.08802585344056825, "compression_ratio": 1.5208333333333333, "no_speech_prob": 8.392597374040633e-05}, {"id": 765, "seek": 537552, "start": 5395.76, "end": 5401.280000000001, "text": " Consider the following illustration of image understanding using Keras, an excellent", "tokens": [17416, 264, 3480, 22645, 295, 3256, 3701, 1228, 591, 6985, 11, 364, 7103], "temperature": 0.0, "avg_logprob": -0.08802585344056825, "compression_ratio": 1.5208333333333333, "no_speech_prob": 8.392597374040633e-05}, {"id": 766, "seek": 540128, "start": 5401.28, "end": 5408.4, "text": " abstraction layer on top of TensorFlow. I like to refer to the input layer as being", "tokens": [37765, 4583, 322, 1192, 295, 37624, 13, 286, 411, 281, 2864, 281, 264, 4846, 4583, 382, 885], "temperature": 0.0, "avg_logprob": -0.06367254257202148, "compression_ratio": 1.6635514018691588, "no_speech_prob": 8.521338168065995e-05}, {"id": 767, "seek": 540128, "start": 5408.4, "end": 5414.88, "text": " on the bottom rather than at the far left as in this image. When viewing it my way,", "tokens": [322, 264, 2767, 2831, 813, 412, 264, 1400, 1411, 382, 294, 341, 3256, 13, 1133, 17480, 309, 452, 636, 11], "temperature": 0.0, "avg_logprob": -0.06367254257202148, "compression_ratio": 1.6635514018691588, "no_speech_prob": 8.521338168065995e-05}, {"id": 768, "seek": 540128, "start": 5414.88, "end": 5420.5599999999995, "text": " the low to high dimension we use in my rotated version of the image can be mentally mapped", "tokens": [264, 2295, 281, 1090, 10139, 321, 764, 294, 452, 42146, 3037, 295, 264, 3256, 393, 312, 17072, 33318], "temperature": 0.0, "avg_logprob": -0.06367254257202148, "compression_ratio": 1.6635514018691588, "no_speech_prob": 8.521338168065995e-05}, {"id": 769, "seek": 540128, "start": 5420.5599999999995, "end": 5427.36, "text": " to a low to high stack of abstraction levels. I'm not the only one using this dimension this way.", "tokens": [281, 257, 2295, 281, 1090, 8630, 295, 37765, 4358, 13, 286, 478, 406, 264, 787, 472, 1228, 341, 10139, 341, 636, 13], "temperature": 0.0, "avg_logprob": -0.06367254257202148, "compression_ratio": 1.6635514018691588, "no_speech_prob": 8.521338168065995e-05}, {"id": 770, "seek": 542736, "start": 5427.36, "end": 5433.92, "text": " I hope this rotation isn't too confusing. We can see that there is an obvious data reduction", "tokens": [286, 1454, 341, 12447, 1943, 380, 886, 13181, 13, 492, 393, 536, 300, 456, 307, 364, 6322, 1412, 11004], "temperature": 0.0, "avg_logprob": -0.12537225457124931, "compression_ratio": 1.6441441441441442, "no_speech_prob": 7.927609112812206e-05}, {"id": 771, "seek": 542736, "start": 5433.92, "end": 5440.4, "text": " and an obvious complexity reduction. Can we determine whether the system is also performing", "tokens": [293, 364, 6322, 14024, 11004, 13, 1664, 321, 6997, 1968, 264, 1185, 307, 611, 10205], "temperature": 0.0, "avg_logprob": -0.12537225457124931, "compression_ratio": 1.6441441441441442, "no_speech_prob": 7.927609112812206e-05}, {"id": 772, "seek": 542736, "start": 5440.4, "end": 5447.36, "text": " what I'd like to call the epistemic reduction? Is it reducing a way that which is unimportant?", "tokens": [437, 286, 1116, 411, 281, 818, 264, 2388, 468, 3438, 11004, 30, 1119, 309, 12245, 257, 636, 300, 597, 307, 517, 41654, 30], "temperature": 0.0, "avg_logprob": -0.12537225457124931, "compression_ratio": 1.6441441441441442, "no_speech_prob": 7.927609112812206e-05}, {"id": 773, "seek": 542736, "start": 5447.36, "end": 5453.839999999999, "text": " And if so, how does it accomplish this? How does an operator in a deep learning stack", "tokens": [400, 498, 370, 11, 577, 775, 309, 9021, 341, 30, 1012, 775, 364, 12973, 294, 257, 2452, 2539, 8630], "temperature": 0.0, "avg_logprob": -0.12537225457124931, "compression_ratio": 1.6441441441441442, "no_speech_prob": 7.927609112812206e-05}, {"id": 774, "seek": 545384, "start": 5453.84, "end": 5461.12, "text": " know what makes something important? Salient, up your data, reduction of sorts could be", "tokens": [458, 437, 1669, 746, 1021, 30, 5996, 1196, 11, 493, 428, 1412, 11, 11004, 295, 7527, 727, 312], "temperature": 0.0, "avg_logprob": -0.1466333674288344, "compression_ratio": 1.6077586206896552, "no_speech_prob": 7.426807860611007e-05}, {"id": 775, "seek": 545384, "start": 5461.12, "end": 5469.2, "text": " accomplished by compression schemes or even random deletion. This is undesirable. We need to discard", "tokens": [15419, 538, 19355, 26954, 420, 754, 4974, 1103, 302, 313, 13, 639, 307, 45667, 21493, 13, 492, 643, 281, 31597], "temperature": 0.0, "avg_logprob": -0.1466333674288344, "compression_ratio": 1.6077586206896552, "no_speech_prob": 7.426807860611007e-05}, {"id": 776, "seek": 545384, "start": 5469.2, "end": 5476.64, "text": " the non-salient parts so that in the end, we are left with what is salient. Some people have not", "tokens": [264, 2107, 12, 15142, 1196, 3166, 370, 300, 294, 264, 917, 11, 321, 366, 1411, 365, 437, 307, 1845, 1196, 13, 2188, 561, 362, 406], "temperature": 0.0, "avg_logprob": -0.1466333674288344, "compression_ratio": 1.6077586206896552, "no_speech_prob": 7.426807860611007e-05}, {"id": 777, "seek": 545384, "start": 5476.64, "end": 5482.400000000001, "text": " understood the importance of salient's based reduction and useless compression power of", "tokens": [7320, 264, 7379, 295, 1845, 1196, 311, 2361, 11004, 293, 14115, 19355, 1347, 295], "temperature": 0.0, "avg_logprob": -0.1466333674288344, "compression_ratio": 1.6077586206896552, "no_speech_prob": 7.426807860611007e-05}, {"id": 778, "seek": 548240, "start": 5482.4, "end": 5488.5599999999995, "text": " reversible algorithms as a measurement of intelligence, which is no more useful than", "tokens": [44788, 14642, 382, 257, 13160, 295, 7599, 11, 597, 307, 572, 544, 4420, 813], "temperature": 0.0, "avg_logprob": -0.07483464680360945, "compression_ratio": 1.5267489711934157, "no_speech_prob": 4.0548831748310477e-05}, {"id": 779, "seek": 548240, "start": 5488.5599999999995, "end": 5496.08, "text": " believing a simple video camera can understand what it sees. So let me conjure up a bit like in", "tokens": [16594, 257, 2199, 960, 2799, 393, 1223, 437, 309, 8194, 13, 407, 718, 385, 20295, 540, 493, 257, 857, 411, 294], "temperature": 0.0, "avg_logprob": -0.07483464680360945, "compression_ratio": 1.5267489711934157, "no_speech_prob": 4.0548831748310477e-05}, {"id": 780, "seek": 548240, "start": 5496.08, "end": 5503.44, "text": " the movie, Inside Out, a fairy tale of what goes on in a deep learning network, except we'll do it,", "tokens": [264, 3169, 11, 15123, 5925, 11, 257, 19104, 17172, 295, 437, 1709, 322, 294, 257, 2452, 2539, 3209, 11, 3993, 321, 603, 360, 309, 11], "temperature": 0.0, "avg_logprob": -0.07483464680360945, "compression_ratio": 1.5267489711934157, "no_speech_prob": 4.0548831748310477e-05}, {"id": 781, "seek": 548240, "start": 5503.44, "end": 5510.5599999999995, "text": " bottom up. Suppose we have built a system for finding faces in an image with the intent of", "tokens": [2767, 493, 13, 21360, 321, 362, 3094, 257, 1185, 337, 5006, 8475, 294, 364, 3256, 365, 264, 8446, 295], "temperature": 0.0, "avg_logprob": -0.07483464680360945, "compression_ratio": 1.5267489711934157, "no_speech_prob": 4.0548831748310477e-05}, {"id": 782, "seek": 551056, "start": 5510.56, "end": 5516.8, "text": " incorporating that as a feature in a camera. Many cameras have this feature already,", "tokens": [33613, 300, 382, 257, 4111, 294, 257, 2799, 13, 5126, 8622, 362, 341, 4111, 1217, 11], "temperature": 0.0, "avg_logprob": -0.06006312370300293, "compression_ratio": 1.6444444444444444, "no_speech_prob": 3.73685616068542e-05}, {"id": 783, "seek": 551056, "start": 5516.8, "end": 5523.6, "text": " so this is not a far-fetched example. We implement an image understanding neural network,", "tokens": [370, 341, 307, 406, 257, 1400, 12, 69, 7858, 292, 1365, 13, 492, 4445, 364, 3256, 3701, 18161, 3209, 11], "temperature": 0.0, "avg_logprob": -0.06006312370300293, "compression_ratio": 1.6444444444444444, "no_speech_prob": 3.73685616068542e-05}, {"id": 784, "seek": 551056, "start": 5523.6, "end": 5530.72, "text": " show the system many kinds of images for a few days, perhaps using so-called supervised learning", "tokens": [855, 264, 1185, 867, 3685, 295, 5267, 337, 257, 1326, 1708, 11, 4317, 1228, 370, 12, 11880, 46533, 2539], "temperature": 0.0, "avg_logprob": -0.06006312370300293, "compression_ratio": 1.6444444444444444, "no_speech_prob": 3.73685616068542e-05}, {"id": 785, "seek": 551056, "start": 5530.72, "end": 5537.200000000001, "text": " in order to improve this story, and then we show it an image of a family having a picnic in a park", "tokens": [294, 1668, 281, 3470, 341, 1657, 11, 293, 550, 321, 855, 309, 364, 3256, 295, 257, 1605, 1419, 257, 32137, 294, 257, 3884], "temperature": 0.0, "avg_logprob": -0.06006312370300293, "compression_ratio": 1.6444444444444444, "no_speech_prob": 3.73685616068542e-05}, {"id": 786, "seek": 553720, "start": 5537.2, "end": 5543.5199999999995, "text": " and ask the system to outline where the faces are so that the camera can focus sharply on them.", "tokens": [293, 1029, 264, 1185, 281, 16387, 689, 264, 8475, 366, 370, 300, 264, 2799, 393, 1879, 42893, 322, 552, 13], "temperature": 0.0, "avg_logprob": -0.05543154277158587, "compression_ratio": 1.628691983122363, "no_speech_prob": 3.894248584401794e-05}, {"id": 787, "seek": 553720, "start": 5544.24, "end": 5550.8, "text": " The input image is converted from RGB color values to an input array and the data in this array is", "tokens": [440, 4846, 3256, 307, 16424, 490, 31231, 2017, 4190, 281, 364, 4846, 10225, 293, 264, 1412, 294, 341, 10225, 307], "temperature": 0.0, "avg_logprob": -0.05543154277158587, "compression_ratio": 1.628691983122363, "no_speech_prob": 3.894248584401794e-05}, {"id": 788, "seek": 553720, "start": 5550.8, "end": 5557.28, "text": " then shuffled through many layers of operators. And for many of these layers, there are fewer", "tokens": [550, 402, 33974, 807, 867, 7914, 295, 19077, 13, 400, 337, 867, 295, 613, 7914, 11, 456, 366, 13366], "temperature": 0.0, "avg_logprob": -0.05543154277158587, "compression_ratio": 1.628691983122363, "no_speech_prob": 3.894248584401794e-05}, {"id": 789, "seek": 553720, "start": 5557.28, "end": 5564.32, "text": " outputs than there are inputs, as you can see above, which means some things have to be discarded", "tokens": [23930, 813, 456, 366, 15743, 11, 382, 291, 393, 536, 3673, 11, 597, 1355, 512, 721, 362, 281, 312, 45469], "temperature": 0.0, "avg_logprob": -0.05543154277158587, "compression_ratio": 1.628691983122363, "no_speech_prob": 3.894248584401794e-05}, {"id": 790, "seek": 556432, "start": 5564.32, "end": 5572.32, "text": " by the processing. Each layer receives initially signals, from below, that is, from the input,", "tokens": [538, 264, 9007, 13, 6947, 4583, 20717, 9105, 12354, 11, 490, 2507, 11, 300, 307, 11, 490, 264, 4846, 11], "temperature": 0.0, "avg_logprob": -0.06530561564881125, "compression_ratio": 1.6167400881057268, "no_speech_prob": 3.180873318342492e-05}, {"id": 791, "seek": 556432, "start": 5572.32, "end": 5578.96, "text": " or from lower levels of abstraction, and produces some reduced output to send to the next layer", "tokens": [420, 490, 3126, 4358, 295, 37765, 11, 293, 14725, 512, 9212, 5598, 281, 2845, 281, 264, 958, 4583], "temperature": 0.0, "avg_logprob": -0.06530561564881125, "compression_ratio": 1.6167400881057268, "no_speech_prob": 3.180873318342492e-05}, {"id": 792, "seek": 556432, "start": 5578.96, "end": 5586.08, "text": " operator above. To continue detail, at some early level, some operator is given a few", "tokens": [12973, 3673, 13, 1407, 2354, 2607, 11, 412, 512, 2440, 1496, 11, 512, 12973, 307, 2212, 257, 1326], "temperature": 0.0, "avg_logprob": -0.06530561564881125, "compression_ratio": 1.6167400881057268, "no_speech_prob": 3.180873318342492e-05}, {"id": 793, "seek": 556432, "start": 5586.08, "end": 5592.0, "text": " adjacent pixels and determines that there is a vertical, slightly curved line dividing the", "tokens": [24441, 18668, 293, 24799, 300, 456, 307, 257, 9429, 11, 4748, 24991, 1622, 26764, 264], "temperature": 0.0, "avg_logprob": -0.06530561564881125, "compression_ratio": 1.6167400881057268, "no_speech_prob": 3.180873318342492e-05}, {"id": 794, "seek": 559200, "start": 5592.0, "end": 5599.44, "text": " darker green area from the lighter green area. So it tells the operator above the simpler line", "tokens": [12741, 3092, 1859, 490, 264, 11546, 3092, 1859, 13, 407, 309, 5112, 264, 12973, 3673, 264, 18587, 1622], "temperature": 0.0, "avg_logprob": -0.08315096131290298, "compression_ratio": 1.663716814159292, "no_speech_prob": 2.7065172616858035e-05}, {"id": 795, "seek": 559200, "start": 5599.44, "end": 5606.08, "text": " or color-based description using some encoding we don't really care about. The operator at the", "tokens": [420, 2017, 12, 6032, 3855, 1228, 512, 43430, 321, 500, 380, 534, 1127, 466, 13, 440, 12973, 412, 264], "temperature": 0.0, "avg_logprob": -0.08315096131290298, "compression_ratio": 1.663716814159292, "no_speech_prob": 2.7065172616858035e-05}, {"id": 796, "seek": 559200, "start": 5606.08, "end": 5612.0, "text": " level above might have gotten another matching curve and says, these match what I saw a lot of", "tokens": [1496, 3673, 1062, 362, 5768, 1071, 14324, 7605, 293, 1619, 11, 613, 2995, 437, 286, 1866, 257, 688, 295], "temperature": 0.0, "avg_logprob": -0.08315096131290298, "compression_ratio": 1.663716814159292, "no_speech_prob": 2.7065172616858035e-05}, {"id": 797, "seek": 559200, "start": 5612.0, "end": 5619.04, "text": " when the label blade of grass was given as a ground truth label during supervised learning.", "tokens": [562, 264, 7645, 10959, 295, 8054, 390, 2212, 382, 257, 2727, 3494, 7645, 1830, 46533, 2539, 13], "temperature": 0.0, "avg_logprob": -0.08315096131290298, "compression_ratio": 1.663716814159292, "no_speech_prob": 2.7065172616858035e-05}, {"id": 798, "seek": 561904, "start": 5619.04, "end": 5624.64, "text": " If no label is known, then we again assume some other uninteresting representation.", "tokens": [759, 572, 7645, 307, 2570, 11, 550, 321, 797, 6552, 512, 661, 49234, 8714, 10290, 13], "temperature": 0.0, "avg_logprob": -0.1888866878691174, "compression_ratio": 1.5822222222222222, "no_speech_prob": 2.8737043976434506e-05}, {"id": 799, "seek": 561904, "start": 5624.64, "end": 5631.68, "text": " It is okay to propagate results without human-labeled signals because whatever signaling scheme is", "tokens": [467, 307, 1392, 281, 48256, 3542, 1553, 1952, 12, 75, 18657, 292, 12354, 570, 2035, 38639, 12232, 307], "temperature": 0.0, "avg_logprob": -0.1888866878691174, "compression_ratio": 1.5822222222222222, "no_speech_prob": 2.8737043976434506e-05}, {"id": 800, "seek": 561904, "start": 5631.68, "end": 5638.24, "text": " used will be learned by the level above. The operator above that says, when I get lots of", "tokens": [1143, 486, 312, 3264, 538, 264, 1496, 3673, 13, 440, 12973, 3673, 300, 1619, 11, 562, 286, 483, 3195, 295], "temperature": 0.0, "avg_logprob": -0.1888866878691174, "compression_ratio": 1.5822222222222222, "no_speech_prob": 2.8737043976434506e-05}, {"id": 801, "seek": 561904, "start": 5638.24, "end": 5644.32, "text": " blades of grass signals, I reduce all of that to a long signal as I send it upward.", "tokens": [20066, 295, 8054, 12354, 11, 286, 5407, 439, 295, 300, 281, 257, 938, 6358, 382, 286, 2845, 309, 23452, 13], "temperature": 0.0, "avg_logprob": -0.1888866878691174, "compression_ratio": 1.5822222222222222, "no_speech_prob": 2.8737043976434506e-05}, {"id": 802, "seek": 564432, "start": 5644.32, "end": 5650.719999999999, "text": " And eventually we reach the higher operator layers and someone there says, we are a face-finder", "tokens": [400, 4728, 321, 2524, 264, 2946, 12973, 7914, 293, 1580, 456, 1619, 11, 321, 366, 257, 1851, 12, 38977], "temperature": 0.0, "avg_logprob": -0.1877368146722967, "compression_ratio": 1.6017316017316017, "no_speech_prob": 9.146385855274275e-05}, {"id": 803, "seek": 564432, "start": 5650.719999999999, "end": 5657.44, "text": " application. We are completely uninterested in lawns and discards the lawn as non-cellient.", "tokens": [3861, 13, 492, 366, 2584, 49234, 21885, 294, 19915, 82, 293, 2983, 2287, 264, 19915, 382, 2107, 12, 4164, 1196, 13], "temperature": 0.0, "avg_logprob": -0.1877368146722967, "compression_ratio": 1.6017316017316017, "no_speech_prob": 9.146385855274275e-05}, {"id": 804, "seek": 564432, "start": 5657.44, "end": 5664.32, "text": " What remains after you discard all non-faces are the faces. You cannot discard anything", "tokens": [708, 7023, 934, 291, 31597, 439, 2107, 12, 69, 2116, 366, 264, 8475, 13, 509, 2644, 31597, 1340], "temperature": 0.0, "avg_logprob": -0.1877368146722967, "compression_ratio": 1.6017316017316017, "no_speech_prob": 9.146385855274275e-05}, {"id": 805, "seek": 564432, "start": 5664.32, "end": 5671.12, "text": " until you know what it is, or can at least estimate whether it's worth learning. Specifically,", "tokens": [1826, 291, 458, 437, 309, 307, 11, 420, 393, 412, 1935, 12539, 1968, 309, 311, 3163, 2539, 13, 26058, 11], "temperature": 0.0, "avg_logprob": -0.1877368146722967, "compression_ratio": 1.6017316017316017, "no_speech_prob": 9.146385855274275e-05}, {"id": 806, "seek": 567112, "start": 5671.12, "end": 5679.04, "text": " until you understand it at the level of abstraction you are operating at. The low-level blade of", "tokens": [1826, 291, 1223, 309, 412, 264, 1496, 295, 37765, 291, 366, 7447, 412, 13, 440, 2295, 12, 12418, 10959, 295], "temperature": 0.0, "avg_logprob": -0.09993013842352505, "compression_ratio": 1.8038277511961722, "no_speech_prob": 4.719619391835295e-05}, {"id": 807, "seek": 567112, "start": 5679.04, "end": 5684.8, "text": " grass recognizers could not discard the grass because they had no clue about the high-level", "tokens": [8054, 3068, 22525, 727, 406, 31597, 264, 8054, 570, 436, 632, 572, 13602, 466, 264, 1090, 12, 12418], "temperature": 0.0, "avg_logprob": -0.09993013842352505, "compression_ratio": 1.8038277511961722, "no_speech_prob": 4.719619391835295e-05}, {"id": 808, "seek": 567112, "start": 5684.8, "end": 5691.68, "text": " saliencies of lawn or not in face or not that the higher layers specialize in. You can only tell", "tokens": [1845, 31294, 295, 19915, 420, 406, 294, 1851, 420, 406, 300, 264, 2946, 7914, 37938, 294, 13, 509, 393, 787, 980], "temperature": 0.0, "avg_logprob": -0.09993013842352505, "compression_ratio": 1.8038277511961722, "no_speech_prob": 4.719619391835295e-05}, {"id": 809, "seek": 567112, "start": 5691.68, "end": 5698.0, "text": " what salient or not, important or not at the level of understanding and abstraction you are", "tokens": [437, 1845, 1196, 420, 406, 11, 1021, 420, 406, 412, 264, 1496, 295, 3701, 293, 37765, 291, 366], "temperature": 0.0, "avg_logprob": -0.09993013842352505, "compression_ratio": 1.8038277511961722, "no_speech_prob": 4.719619391835295e-05}, {"id": 810, "seek": 569800, "start": 5698.0, "end": 5705.28, "text": " operating at. Each layer receives lower-level descriptions from below, discards what it", "tokens": [7447, 412, 13, 6947, 4583, 20717, 3126, 12, 12418, 24406, 490, 2507, 11, 2983, 2287, 437, 309], "temperature": 0.0, "avg_logprob": -0.04922330670240449, "compression_ratio": 1.5584415584415585, "no_speech_prob": 9.588759712642059e-05}, {"id": 811, "seek": 569800, "start": 5705.28, "end": 5711.84, "text": " recognizes as irrelevant, and sends its own version of higher-level descriptions upward", "tokens": [26564, 382, 28682, 11, 293, 14790, 1080, 1065, 3037, 295, 2946, 12, 12418, 24406, 23452], "temperature": 0.0, "avg_logprob": -0.04922330670240449, "compression_ratio": 1.5584415584415585, "no_speech_prob": 9.588759712642059e-05}, {"id": 812, "seek": 569800, "start": 5711.84, "end": 5717.76, "text": " until we reach someone who knows what we are really looking for. This is of course why deep", "tokens": [1826, 321, 2524, 1580, 567, 3255, 437, 321, 366, 534, 1237, 337, 13, 639, 307, 295, 1164, 983, 2452], "temperature": 0.0, "avg_logprob": -0.04922330670240449, "compression_ratio": 1.5584415584415585, "no_speech_prob": 9.588759712642059e-05}, {"id": 813, "seek": 569800, "start": 5717.76, "end": 5726.64, "text": " learning is deep. This idea itself is not new. It was discussed by Oliver Selfridge in 1959.", "tokens": [2539, 307, 2452, 13, 639, 1558, 2564, 307, 406, 777, 13, 467, 390, 7152, 538, 23440, 16348, 15804, 294, 45608, 13], "temperature": 0.0, "avg_logprob": -0.04922330670240449, "compression_ratio": 1.5584415584415585, "no_speech_prob": 9.588759712642059e-05}, {"id": 814, "seek": 572664, "start": 5726.64, "end": 5733.92, "text": " He described an idea called, Pandemonium, which was largely ignored by the AI community because of", "tokens": [634, 7619, 364, 1558, 1219, 11, 16995, 36228, 2197, 11, 597, 390, 11611, 19735, 538, 264, 7318, 1768, 570, 295], "temperature": 0.0, "avg_logprob": -0.08152260099138532, "compression_ratio": 1.4863813229571985, "no_speech_prob": 2.3392318325932138e-05}, {"id": 815, "seek": 572664, "start": 5733.92, "end": 5740.4800000000005, "text": " its radical departure from the logic-based AI promoted by people like John McCarthy and Marvin", "tokens": [1080, 12001, 25866, 490, 264, 9952, 12, 6032, 7318, 21162, 538, 561, 411, 2619, 44085, 293, 48722], "temperature": 0.0, "avg_logprob": -0.08152260099138532, "compression_ratio": 1.4863813229571985, "no_speech_prob": 2.3392318325932138e-05}, {"id": 816, "seek": 572664, "start": 5740.4800000000005, "end": 5748.400000000001, "text": " Minsky. But Pandemonium presaged, by almost 60 years, the layer-by-layer architecture with", "tokens": [376, 44153, 13, 583, 16995, 36228, 2197, 1183, 2980, 11, 538, 1920, 4060, 924, 11, 264, 4583, 12, 2322, 12, 8376, 260, 9482, 365], "temperature": 0.0, "avg_logprob": -0.08152260099138532, "compression_ratio": 1.4863813229571985, "no_speech_prob": 2.3392318325932138e-05}, {"id": 817, "seek": 572664, "start": 5748.400000000001, "end": 5755.12, "text": " signals passing up and down that is used today in all deep neural networks. This is the reason my", "tokens": [12354, 8437, 493, 293, 760, 300, 307, 1143, 965, 294, 439, 2452, 18161, 9590, 13, 639, 307, 264, 1778, 452], "temperature": 0.0, "avg_logprob": -0.08152260099138532, "compression_ratio": 1.4863813229571985, "no_speech_prob": 2.3392318325932138e-05}, {"id": 818, "seek": 575512, "start": 5755.12, "end": 5762.72, "text": " online handle is at Pandemonica. So do any TensorFlow operators support this reduction?", "tokens": [2950, 4813, 307, 412, 16995, 36228, 2262, 13, 407, 360, 604, 37624, 19077, 1406, 341, 11004, 30], "temperature": 0.0, "avg_logprob": -0.09133083373308182, "compression_ratio": 1.5988372093023255, "no_speech_prob": 0.0001147683578892611}, {"id": 819, "seek": 575512, "start": 5763.36, "end": 5771.04, "text": " Let's start by examining the pooling operators. There are a few in the diagram. They are conceptually", "tokens": [961, 311, 722, 538, 34662, 264, 7005, 278, 19077, 13, 821, 366, 257, 1326, 294, 264, 10686, 13, 814, 366, 3410, 671], "temperature": 0.0, "avg_logprob": -0.09133083373308182, "compression_ratio": 1.5988372093023255, "no_speech_prob": 0.0001147683578892611}, {"id": 820, "seek": 575512, "start": 5771.04, "end": 5778.0, "text": " simple. There are over 50 pooling operators in TensorFlow. There is an operator named", "tokens": [2199, 13, 821, 366, 670, 2625, 7005, 278, 19077, 294, 37624, 13, 821, 307, 364, 12973, 4926], "temperature": 0.0, "avg_logprob": -0.09133083373308182, "compression_ratio": 1.5988372093023255, "no_speech_prob": 0.0001147683578892611}, {"id": 821, "seek": 577800, "start": 5778.0, "end": 5787.04, "text": " 2x2 Max Pool operator. In the diagram, it is used four times. It is given four inputs with", "tokens": [568, 87, 17, 7402, 46188, 12973, 13, 682, 264, 10686, 11, 309, 307, 1143, 1451, 1413, 13, 467, 307, 2212, 1451, 15743, 365], "temperature": 0.0, "avg_logprob": -0.08262111829674762, "compression_ratio": 1.6462882096069869, "no_speech_prob": 5.041279655415565e-05}, {"id": 822, "seek": 577800, "start": 5787.04, "end": 5794.0, "text": " varying values and propagates the highest value of those as its only output. Close to the input", "tokens": [22984, 4190, 293, 12425, 1024, 264, 6343, 2158, 295, 729, 382, 1080, 787, 5598, 13, 16346, 281, 264, 4846], "temperature": 0.0, "avg_logprob": -0.08262111829674762, "compression_ratio": 1.6462882096069869, "no_speech_prob": 5.041279655415565e-05}, {"id": 823, "seek": 577800, "start": 5794.0, "end": 5799.68, "text": " layer of these four values may be four adjacent pixels where their values might be a brightness", "tokens": [4583, 295, 613, 1451, 4190, 815, 312, 1451, 24441, 18668, 689, 641, 4190, 1062, 312, 257, 21367], "temperature": 0.0, "avg_logprob": -0.08262111829674762, "compression_ratio": 1.6462882096069869, "no_speech_prob": 5.041279655415565e-05}, {"id": 824, "seek": 577800, "start": 5799.68, "end": 5807.44, "text": " in some color channel, but higher up they mean whatever they mean. In effect, the Max Pool 2x2", "tokens": [294, 512, 2017, 2269, 11, 457, 2946, 493, 436, 914, 2035, 436, 914, 13, 682, 1802, 11, 264, 7402, 46188, 568, 87, 17], "temperature": 0.0, "avg_logprob": -0.08262111829674762, "compression_ratio": 1.6462882096069869, "no_speech_prob": 5.041279655415565e-05}, {"id": 825, "seek": 580744, "start": 5807.44, "end": 5815.679999999999, "text": " discards the least important 75% of its input data, preserving and propagating only one", "tokens": [2983, 2287, 264, 1935, 1021, 9562, 4, 295, 1080, 4846, 1412, 11, 33173, 293, 12425, 990, 787, 472], "temperature": 0.0, "avg_logprob": -0.062294054722440415, "compression_ratio": 1.6863905325443787, "no_speech_prob": 6.126570224296302e-05}, {"id": 826, "seek": 580744, "start": 5815.679999999999, "end": 5824.0, "text": " highest value. In the case of pixels, it might mean the brightest color value. In the case of blades", "tokens": [6343, 2158, 13, 682, 264, 1389, 295, 18668, 11, 309, 1062, 914, 264, 36271, 2017, 2158, 13, 682, 264, 1389, 295, 20066], "temperature": 0.0, "avg_logprob": -0.062294054722440415, "compression_ratio": 1.6863905325443787, "no_speech_prob": 6.126570224296302e-05}, {"id": 827, "seek": 580744, "start": 5824.0, "end": 5831.2, "text": " of grass, it might mean there is at least one blade of grass here. The interpretation of what is", "tokens": [295, 8054, 11, 309, 1062, 914, 456, 307, 412, 1935, 472, 10959, 295, 8054, 510, 13, 440, 14174, 295, 437, 307], "temperature": 0.0, "avg_logprob": -0.062294054722440415, "compression_ratio": 1.6863905325443787, "no_speech_prob": 6.126570224296302e-05}, {"id": 828, "seek": 583120, "start": 5831.2, "end": 5839.2, "text": " discarded depends on the layer, because in a very real sense, layers represent levels of reduction,", "tokens": [45469, 5946, 322, 264, 4583, 11, 570, 294, 257, 588, 957, 2020, 11, 7914, 2906, 4358, 295, 11004, 11], "temperature": 0.0, "avg_logprob": -0.09306186721438453, "compression_ratio": 1.6768558951965065, "no_speech_prob": 4.180732503300533e-05}, {"id": 829, "seek": 583120, "start": 5839.2, "end": 5846.4, "text": " abstraction levels, if you prefer that term. And we should now be clearly seeing one of the most", "tokens": [37765, 4358, 11, 498, 291, 4382, 300, 1433, 13, 400, 321, 820, 586, 312, 4448, 2577, 472, 295, 264, 881], "temperature": 0.0, "avg_logprob": -0.09306186721438453, "compression_ratio": 1.6768558951965065, "no_speech_prob": 4.180732503300533e-05}, {"id": 830, "seek": 583120, "start": 5846.4, "end": 5852.639999999999, "text": " important ideas in deep neural networks, the reduction has to be done at multiple levels", "tokens": [1021, 3487, 294, 2452, 18161, 9590, 11, 264, 11004, 575, 281, 312, 1096, 412, 3866, 4358], "temperature": 0.0, "avg_logprob": -0.09306186721438453, "compression_ratio": 1.6768558951965065, "no_speech_prob": 4.180732503300533e-05}, {"id": 831, "seek": 583120, "start": 5852.639999999999, "end": 5859.36, "text": " of abstraction. Each set of decisions about what is reduced away as irrelevant and what is kept as", "tokens": [295, 37765, 13, 6947, 992, 295, 5327, 466, 437, 307, 9212, 1314, 382, 28682, 293, 437, 307, 4305, 382], "temperature": 0.0, "avg_logprob": -0.09306186721438453, "compression_ratio": 1.6768558951965065, "no_speech_prob": 4.180732503300533e-05}, {"id": 832, "seek": 585936, "start": 5859.36, "end": 5866.48, "text": " possibly relevant can only be made at an appropriate abstraction level. We cannot yet abstract away", "tokens": [6264, 7340, 393, 787, 312, 1027, 412, 364, 6854, 37765, 1496, 13, 492, 2644, 1939, 12649, 1314], "temperature": 0.0, "avg_logprob": -0.05878584805656882, "compression_ratio": 1.6307053941908713, "no_speech_prob": 4.5240638428367674e-05}, {"id": 833, "seek": 585936, "start": 5866.48, "end": 5873.12, "text": " the lawn if all we know is there are dark and light green areas levels. This is a simplification.", "tokens": [264, 19915, 498, 439, 321, 458, 307, 456, 366, 2877, 293, 1442, 3092, 3179, 4358, 13, 639, 307, 257, 6883, 3774, 13], "temperature": 0.0, "avg_logprob": -0.05878584805656882, "compression_ratio": 1.6307053941908713, "no_speech_prob": 4.5240638428367674e-05}, {"id": 834, "seek": 585936, "start": 5873.679999999999, "end": 5879.839999999999, "text": " Decisions made in this manner will be heated only if they have contributed to positive outcomes in", "tokens": [12427, 4252, 1027, 294, 341, 9060, 486, 312, 18806, 787, 498, 436, 362, 18434, 281, 3353, 10070, 294], "temperature": 0.0, "avg_logprob": -0.05878584805656882, "compression_ratio": 1.6307053941908713, "no_speech_prob": 4.5240638428367674e-05}, {"id": 835, "seek": 585936, "start": 5879.839999999999, "end": 5886.88, "text": " learning. Unreliable and useless decision makers will be ignored using any of several mechanisms", "tokens": [2539, 13, 1156, 265, 2081, 712, 293, 14115, 3537, 19323, 486, 312, 19735, 1228, 604, 295, 2940, 15902], "temperature": 0.0, "avg_logprob": -0.05878584805656882, "compression_ratio": 1.6307053941908713, "no_speech_prob": 4.5240638428367674e-05}, {"id": 836, "seek": 588688, "start": 5886.88, "end": 5895.04, "text": " that we may apply during learning. More later, for now, we continue by examining the most popular", "tokens": [300, 321, 815, 3079, 1830, 2539, 13, 5048, 1780, 11, 337, 586, 11, 321, 2354, 538, 34662, 264, 881, 3743], "temperature": 0.0, "avg_logprob": -0.10889303989899464, "compression_ratio": 1.6457399103139014, "no_speech_prob": 7.519545761169866e-05}, {"id": 837, "seek": 588688, "start": 5895.04, "end": 5902.8, "text": " subset of all TensorFlow operators. The convolution family from the TensorFlow manual,", "tokens": [25993, 295, 439, 37624, 19077, 13, 440, 45216, 1605, 490, 264, 37624, 9688, 11], "temperature": 0.0, "avg_logprob": -0.10889303989899464, "compression_ratio": 1.6457399103139014, "no_speech_prob": 7.519545761169866e-05}, {"id": 838, "seek": 588688, "start": 5902.8, "end": 5908.96, "text": " note that although these ops are called convolution, they are strictly speaking cross", "tokens": [3637, 300, 4878, 613, 44663, 366, 1219, 45216, 11, 436, 366, 20792, 4124, 3278], "temperature": 0.0, "avg_logprob": -0.10889303989899464, "compression_ratio": 1.6457399103139014, "no_speech_prob": 7.519545761169866e-05}, {"id": 839, "seek": 588688, "start": 5908.96, "end": 5915.84, "text": " correlation. Convolution layers discover cross correlations and co-occurrences of various kinds.", "tokens": [20009, 13, 2656, 85, 3386, 7914, 4411, 3278, 13983, 763, 293, 598, 12, 905, 14112, 38983, 295, 3683, 3685, 13], "temperature": 0.0, "avg_logprob": -0.10889303989899464, "compression_ratio": 1.6457399103139014, "no_speech_prob": 7.519545761169866e-05}, {"id": 840, "seek": 591584, "start": 5915.84, "end": 5922.64, "text": " Co-occurrences to known patterns in the image at various locations. Spatial relationships", "tokens": [3066, 12, 905, 14112, 38983, 281, 2570, 8294, 294, 264, 3256, 412, 3683, 9253, 13, 1738, 267, 831, 6159], "temperature": 0.0, "avg_logprob": -0.1012873708465953, "compression_ratio": 1.5940170940170941, "no_speech_prob": 0.00012000837159575894}, {"id": 841, "seek": 591584, "start": 5922.64, "end": 5929.2, "text": " within an image itself, like Jeff Hinton's recent example of the mouth normally being found below", "tokens": [1951, 364, 3256, 2564, 11, 411, 7506, 389, 12442, 311, 5162, 1365, 295, 264, 4525, 5646, 885, 1352, 2507], "temperature": 0.0, "avg_logprob": -0.1012873708465953, "compression_ratio": 1.5940170940170941, "no_speech_prob": 0.00012000837159575894}, {"id": 842, "seek": 591584, "start": 5929.2, "end": 5936.56, "text": " the nose. And more obviously, in the supervised learning case, correlations between discovered", "tokens": [264, 6690, 13, 400, 544, 2745, 11, 294, 264, 46533, 2539, 1389, 11, 13983, 763, 1296, 6941], "temperature": 0.0, "avg_logprob": -0.1012873708465953, "compression_ratio": 1.5940170940170941, "no_speech_prob": 0.00012000837159575894}, {"id": 843, "seek": 591584, "start": 5936.56, "end": 5942.8, "text": " patterns and the available meta-information, tags, labels that correlate with the patterns", "tokens": [8294, 293, 264, 2435, 19616, 12, 20941, 11, 18632, 11, 16949, 300, 48742, 365, 264, 8294], "temperature": 0.0, "avg_logprob": -0.1012873708465953, "compression_ratio": 1.5940170940170941, "no_speech_prob": 0.00012000837159575894}, {"id": 844, "seek": 594280, "start": 5942.8, "end": 5949.360000000001, "text": " the system may discover. This is what allows an image-understander to tag the occurrence of a", "tokens": [264, 1185, 815, 4411, 13, 639, 307, 437, 4045, 364, 3256, 12, 6617, 1115, 260, 281, 6162, 264, 36122, 295, 257], "temperature": 0.0, "avg_logprob": -0.09285596359607785, "compression_ratio": 1.6622222222222223, "no_speech_prob": 4.706693289335817e-05}, {"id": 845, "seek": 594280, "start": 5949.360000000001, "end": 5956.72, "text": " nose in an image with the text string nose. Beyond this, such systems may learn to understand", "tokens": [6690, 294, 364, 3256, 365, 264, 2487, 6798, 6690, 13, 19707, 341, 11, 1270, 3652, 815, 1466, 281, 1223], "temperature": 0.0, "avg_logprob": -0.09285596359607785, "compression_ratio": 1.6622222222222223, "no_speech_prob": 4.706693289335817e-05}, {"id": 846, "seek": 594280, "start": 5956.72, "end": 5964.320000000001, "text": " concepts like behind and under. The information that is propagated to the higher levels in the", "tokens": [10392, 411, 2261, 293, 833, 13, 440, 1589, 300, 307, 12425, 770, 281, 264, 2946, 4358, 294, 264], "temperature": 0.0, "avg_logprob": -0.09285596359607785, "compression_ratio": 1.6622222222222223, "no_speech_prob": 4.706693289335817e-05}, {"id": 847, "seek": 594280, "start": 5964.320000000001, "end": 5971.4400000000005, "text": " network now describes these correlations. Uncorrelated information is viewed as non-salient", "tokens": [3209, 586, 15626, 613, 13983, 763, 13, 1156, 19558, 12004, 1589, 307, 19174, 382, 2107, 12, 15142, 1196], "temperature": 0.0, "avg_logprob": -0.09285596359607785, "compression_ratio": 1.6622222222222223, "no_speech_prob": 4.706693289335817e-05}, {"id": 848, "seek": 597144, "start": 5971.44, "end": 5978.879999999999, "text": " and is discarded. In the Crescent diagram, this discarding is done by a max pooling layer after", "tokens": [293, 307, 45469, 13, 682, 264, 383, 495, 2207, 10686, 11, 341, 31597, 278, 307, 1096, 538, 257, 11469, 7005, 278, 4583, 934], "temperature": 0.0, "avg_logprob": -0.15791137798412427, "compression_ratio": 1.529100529100529, "no_speech_prob": 6.924969056854025e-05}, {"id": 849, "seek": 597144, "start": 5978.879999999999, "end": 5986.5599999999995, "text": " the convolution plus ReLU layers. ReLU is a kind of layer operator that discards negative values,", "tokens": [264, 45216, 1804, 1300, 43, 52, 7914, 13, 1300, 43, 52, 307, 257, 733, 295, 4583, 12973, 300, 2983, 2287, 3671, 4190, 11], "temperature": 0.0, "avg_logprob": -0.15791137798412427, "compression_ratio": 1.529100529100529, "no_speech_prob": 6.924969056854025e-05}, {"id": 850, "seek": 597144, "start": 5986.5599999999995, "end": 5993.12, "text": " introducing a non-linearity that is important for DL but not really important for our analysis.", "tokens": [15424, 257, 2107, 12, 1889, 17409, 300, 307, 1021, 337, 413, 43, 457, 406, 534, 1021, 337, 527, 5215, 13], "temperature": 0.0, "avg_logprob": -0.15791137798412427, "compression_ratio": 1.529100529100529, "no_speech_prob": 6.924969056854025e-05}, {"id": 851, "seek": 599312, "start": 5993.12, "end": 6001.44, "text": " This pattern of three layers, convolution, then ReLU, then a pooling layer, is quite popular", "tokens": [639, 5102, 295, 1045, 7914, 11, 45216, 11, 550, 1300, 43, 52, 11, 550, 257, 7005, 278, 4583, 11, 307, 1596, 3743], "temperature": 0.0, "avg_logprob": -0.10935386021931966, "compression_ratio": 1.6043478260869566, "no_speech_prob": 5.603829413303174e-05}, {"id": 852, "seek": 599312, "start": 6001.44, "end": 6007.76, "text": " because this combination is performing one reliable reduction step. These three-layer types", "tokens": [570, 341, 6562, 307, 10205, 472, 12924, 11004, 1823, 13, 1981, 1045, 12, 8376, 260, 3467], "temperature": 0.0, "avg_logprob": -0.10935386021931966, "compression_ratio": 1.6043478260869566, "no_speech_prob": 5.603829413303174e-05}, {"id": 853, "seek": 599312, "start": 6007.76, "end": 6015.12, "text": " in this packaged sequence may appear many times in a DL computational graph. In each of these", "tokens": [294, 341, 38162, 8310, 815, 4204, 867, 1413, 294, 257, 413, 43, 28270, 4295, 13, 682, 1184, 295, 613], "temperature": 0.0, "avg_logprob": -0.10935386021931966, "compression_ratio": 1.6043478260869566, "no_speech_prob": 5.603829413303174e-05}, {"id": 854, "seek": 599312, "start": 6015.12, "end": 6021.2, "text": " three-layer packages is reducing away things that levels below had no chance of evaluating", "tokens": [1045, 12, 8376, 260, 17401, 307, 12245, 1314, 721, 300, 4358, 2507, 632, 572, 2931, 295, 27479], "temperature": 0.0, "avg_logprob": -0.10935386021931966, "compression_ratio": 1.6043478260869566, "no_speech_prob": 5.603829413303174e-05}, {"id": 855, "seek": 602120, "start": 6021.2, "end": 6028.16, "text": " for saliency because they didn't understand their input at the correct level. Again,", "tokens": [337, 1845, 7848, 570, 436, 994, 380, 1223, 641, 4846, 412, 264, 3006, 1496, 13, 3764, 11], "temperature": 0.0, "avg_logprob": -0.06657576251339603, "compression_ratio": 1.7383177570093458, "no_speech_prob": 5.1820112275891006e-05}, {"id": 856, "seek": 602120, "start": 6028.16, "end": 6034.5599999999995, "text": " this is why deep learning is deep because you can only do reduction by discarding the irrelevant", "tokens": [341, 307, 983, 2452, 2539, 307, 2452, 570, 291, 393, 787, 360, 11004, 538, 31597, 278, 264, 28682], "temperature": 0.0, "avg_logprob": -0.06657576251339603, "compression_ratio": 1.7383177570093458, "no_speech_prob": 5.1820112275891006e-05}, {"id": 857, "seek": 602120, "start": 6034.5599999999995, "end": 6041.28, "text": " if you understand what is relevant and irrelevant at each different level of abstraction. Is", "tokens": [498, 291, 1223, 437, 307, 7340, 293, 28682, 412, 1184, 819, 1496, 295, 37765, 13, 1119], "temperature": 0.0, "avg_logprob": -0.06657576251339603, "compression_ratio": 1.7383177570093458, "no_speech_prob": 5.1820112275891006e-05}, {"id": 858, "seek": 602120, "start": 6041.28, "end": 6048.0, "text": " deep learning science or not? While the deep learning process can be described using mathematical", "tokens": [2452, 2539, 3497, 420, 406, 30, 3987, 264, 2452, 2539, 1399, 393, 312, 7619, 1228, 18894], "temperature": 0.0, "avg_logprob": -0.06657576251339603, "compression_ratio": 1.7383177570093458, "no_speech_prob": 5.1820112275891006e-05}, {"id": 859, "seek": 604800, "start": 6048.0, "end": 6056.48, "text": " notation, mostly using linear algebra, the process itself isn't scientific. We cannot explain how", "tokens": [24657, 11, 5240, 1228, 8213, 21989, 11, 264, 1399, 2564, 1943, 380, 8134, 13, 492, 2644, 2903, 577], "temperature": 0.0, "avg_logprob": -0.08482221971478379, "compression_ratio": 1.5248618784530388, "no_speech_prob": 2.6691888706409372e-05}, {"id": 860, "seek": 604800, "start": 6056.48, "end": 6063.36, "text": " this system is capable of forming any kind of understanding by just staring at these equations,", "tokens": [341, 1185, 307, 8189, 295, 15745, 604, 733, 295, 3701, 538, 445, 18043, 412, 613, 11787, 11], "temperature": 0.0, "avg_logprob": -0.08482221971478379, "compression_ratio": 1.5248618784530388, "no_speech_prob": 2.6691888706409372e-05}, {"id": 861, "seek": 604800, "start": 6063.36, "end": 6068.48, "text": " since understanding is an emergent effect of repeated reductions over many layers.", "tokens": [1670, 3701, 307, 364, 4345, 6930, 1802, 295, 10477, 40296, 670, 867, 7914, 13], "temperature": 0.0, "avg_logprob": -0.08482221971478379, "compression_ratio": 1.5248618784530388, "no_speech_prob": 2.6691888706409372e-05}, {"id": 862, "seek": 606848, "start": 6068.48, "end": 6077.679999999999, "text": " Consider the convolution operators. As the TF manual quote clearly states, convolution layers discover", "tokens": [17416, 264, 45216, 19077, 13, 1018, 264, 40964, 9688, 6513, 4448, 4368, 11, 45216, 7914, 4411], "temperature": 0.0, "avg_logprob": -0.08450075907584949, "compression_ratio": 1.6666666666666667, "no_speech_prob": 9.615522139938548e-05}, {"id": 863, "seek": 606848, "start": 6077.679999999999, "end": 6085.919999999999, "text": " correlations. Many blades of grass together typically means a lawn. In TF, a lot of cycles", "tokens": [13983, 763, 13, 5126, 20066, 295, 8054, 1214, 5850, 1355, 257, 19915, 13, 682, 40964, 11, 257, 688, 295, 17796], "temperature": 0.0, "avg_logprob": -0.08450075907584949, "compression_ratio": 1.6666666666666667, "no_speech_prob": 9.615522139938548e-05}, {"id": 864, "seek": 606848, "start": 6085.919999999999, "end": 6092.32, "text": " are spent on discovering these correlations. Once found, the correlation leads to some", "tokens": [366, 4418, 322, 24773, 613, 13983, 763, 13, 3443, 1352, 11, 264, 20009, 6689, 281, 512], "temperature": 0.0, "avg_logprob": -0.08450075907584949, "compression_ratio": 1.6666666666666667, "no_speech_prob": 9.615522139938548e-05}, {"id": 865, "seek": 606848, "start": 6092.32, "end": 6097.759999999999, "text": " adjustments of some way to make the correct reduction more likely to be rediscovered", "tokens": [18624, 295, 512, 636, 281, 652, 264, 3006, 11004, 544, 3700, 281, 312, 2182, 40080, 292], "temperature": 0.0, "avg_logprob": -0.08450075907584949, "compression_ratio": 1.6666666666666667, "no_speech_prob": 9.615522139938548e-05}, {"id": 866, "seek": 609776, "start": 6097.76, "end": 6103.280000000001, "text": " the next round, because this reduction is done multiple times. But in essence,", "tokens": [264, 958, 3098, 11, 570, 341, 11004, 307, 1096, 3866, 1413, 13, 583, 294, 12801, 11], "temperature": 0.0, "avg_logprob": -0.10502369870844576, "compression_ratio": 1.6156716417910448, "no_speech_prob": 4.064248423674144e-05}, {"id": 867, "seek": 609776, "start": 6103.280000000001, "end": 6108.64, "text": " all correlations are forgotten and have to be rediscovered in every path through the deep", "tokens": [439, 13983, 763, 366, 11832, 293, 362, 281, 312, 2182, 40080, 292, 294, 633, 3100, 807, 264, 2452], "temperature": 0.0, "avg_logprob": -0.10502369870844576, "compression_ratio": 1.6156716417910448, "no_speech_prob": 4.064248423674144e-05}, {"id": 868, "seek": 609776, "start": 6108.64, "end": 6114.16, "text": " learning loop of upward signaling and downward gradient descent with minute adjustments to", "tokens": [2539, 6367, 295, 23452, 38639, 293, 24805, 16235, 23475, 365, 3456, 18624, 281], "temperature": 0.0, "avg_logprob": -0.10502369870844576, "compression_ratio": 1.6156716417910448, "no_speech_prob": 4.064248423674144e-05}, {"id": 869, "seek": 609776, "start": 6114.16, "end": 6121.2, "text": " erring variables. This system is in effect learning from its mistakes, which is a good sign,", "tokens": [1189, 2937, 9102, 13, 639, 1185, 307, 294, 1802, 2539, 490, 1080, 8038, 11, 597, 307, 257, 665, 1465, 11], "temperature": 0.0, "avg_logprob": -0.10502369870844576, "compression_ratio": 1.6156716417910448, "no_speech_prob": 4.064248423674144e-05}, {"id": 870, "seek": 609776, "start": 6121.2, "end": 6126.56, "text": " since that may well be the only way to learn anything. At least at these levels.", "tokens": [1670, 300, 815, 731, 312, 264, 787, 636, 281, 1466, 1340, 13, 1711, 1935, 412, 613, 4358, 13], "temperature": 0.0, "avg_logprob": -0.10502369870844576, "compression_ratio": 1.6156716417910448, "no_speech_prob": 4.064248423674144e-05}, {"id": 871, "seek": 612656, "start": 6126.56, "end": 6133.84, "text": " This up and down may be repeated many times for each image in the learning set. This up and down", "tokens": [639, 493, 293, 760, 815, 312, 10477, 867, 1413, 337, 1184, 3256, 294, 264, 2539, 992, 13, 639, 493, 293, 760], "temperature": 0.0, "avg_logprob": -0.09159872748635033, "compression_ratio": 1.6590909090909092, "no_speech_prob": 9.010619396576658e-05}, {"id": 872, "seek": 612656, "start": 6133.84, "end": 6140.400000000001, "text": " makes some sense for image understanding. Some are using the same algorithms for text.", "tokens": [1669, 512, 2020, 337, 3256, 3701, 13, 2188, 366, 1228, 264, 912, 14642, 337, 2487, 13], "temperature": 0.0, "avg_logprob": -0.09159872748635033, "compression_ratio": 1.6590909090909092, "no_speech_prob": 9.010619396576658e-05}, {"id": 873, "seek": 612656, "start": 6141.120000000001, "end": 6147.68, "text": " Fortunately, in the text case, there are very efficient alternatives to this ridiculously", "tokens": [20652, 11, 294, 264, 2487, 1389, 11, 456, 366, 588, 7148, 20478, 281, 341, 41358], "temperature": 0.0, "avg_logprob": -0.09159872748635033, "compression_ratio": 1.6590909090909092, "no_speech_prob": 9.010619396576658e-05}, {"id": 874, "seek": 612656, "start": 6147.68, "end": 6155.200000000001, "text": " expensive algorithm. For starters, we can represent the discovered correlations explicitly,", "tokens": [5124, 9284, 13, 1171, 35131, 11, 321, 393, 2906, 264, 6941, 13983, 763, 20803, 11], "temperature": 0.0, "avg_logprob": -0.09159872748635033, "compression_ratio": 1.6590909090909092, "no_speech_prob": 9.010619396576658e-05}, {"id": 875, "seek": 615520, "start": 6155.2, "end": 6161.5199999999995, "text": " using regular pointers or object references in our programming languages.", "tokens": [1228, 3890, 44548, 420, 2657, 15400, 294, 527, 9410, 8650, 13], "temperature": 0.0, "avg_logprob": -0.1617063597628945, "compression_ratio": 1.647887323943662, "no_speech_prob": 4.5823770051356405e-05}, {"id": 876, "seek": 615520, "start": 6162.32, "end": 6170.32, "text": " Or, synapses in brains. This software neuron correlates with that software neuron says a", "tokens": [1610, 11, 5451, 2382, 279, 294, 15442, 13, 639, 4722, 34090, 13983, 1024, 365, 300, 4722, 34090, 1619, 257], "temperature": 0.0, "avg_logprob": -0.1617063597628945, "compression_ratio": 1.647887323943662, "no_speech_prob": 4.5823770051356405e-05}, {"id": 877, "seek": 615520, "start": 6170.32, "end": 6176.8, "text": " synapse or reference connecting this to that. We shall discuss such systems in the section on", "tokens": [5451, 11145, 420, 6408, 11015, 341, 281, 300, 13, 492, 4393, 2248, 1270, 3652, 294, 264, 3541, 322], "temperature": 0.0, "avg_logprob": -0.1617063597628945, "compression_ratio": 1.647887323943662, "no_speech_prob": 4.5823770051356405e-05}, {"id": 878, "seek": 615520, "start": 6176.8, "end": 6183.76, "text": " organic learning, which is coming up next. Then either the deep learning family of algorithms,", "tokens": [10220, 2539, 11, 597, 307, 1348, 493, 958, 13, 1396, 2139, 264, 2452, 2539, 1605, 295, 14642, 11], "temperature": 0.0, "avg_logprob": -0.1617063597628945, "compression_ratio": 1.647887323943662, "no_speech_prob": 4.5823770051356405e-05}, {"id": 879, "seek": 618376, "start": 6183.76, "end": 6190.4800000000005, "text": " or organic learning, are scientific in any meaningful way. They jump to conclusions on", "tokens": [420, 10220, 2539, 11, 366, 8134, 294, 604, 10995, 636, 13, 814, 3012, 281, 22865, 322], "temperature": 0.0, "avg_logprob": -0.10138071588723056, "compression_ratio": 1.5859030837004404, "no_speech_prob": 3.399951310711913e-05}, {"id": 880, "seek": 618376, "start": 6190.4800000000005, "end": 6197.6, "text": " scant evidence and trust correlations without insisting on provable causality. This is disallowed", "tokens": [795, 394, 4467, 293, 3361, 13983, 763, 1553, 13466, 278, 322, 1439, 712, 3302, 1860, 13, 639, 307, 717, 13253, 292], "temperature": 0.0, "avg_logprob": -0.10138071588723056, "compression_ratio": 1.5859030837004404, "no_speech_prob": 3.399951310711913e-05}, {"id": 881, "seek": 618376, "start": 6197.6, "end": 6203.280000000001, "text": " in scientific theory, where absolutely reliable causality is the coin of the realm.", "tokens": [294, 8134, 5261, 11, 689, 3122, 12924, 3302, 1860, 307, 264, 11464, 295, 264, 15355, 13], "temperature": 0.0, "avg_logprob": -0.10138071588723056, "compression_ratio": 1.5859030837004404, "no_speech_prob": 3.399951310711913e-05}, {"id": 882, "seek": 618376, "start": 6204.0, "end": 6210.96, "text": " F equals m a or go home. The most deep neural network programming is uncomfortably close to", "tokens": [479, 6915, 275, 257, 420, 352, 1280, 13, 440, 881, 2452, 18161, 3209, 9410, 307, 8585, 2728, 1188, 1998, 281], "temperature": 0.0, "avg_logprob": -0.10138071588723056, "compression_ratio": 1.5859030837004404, "no_speech_prob": 3.399951310711913e-05}, {"id": 883, "seek": 621096, "start": 6210.96, "end": 6217.84, "text": " trial and error, with only minor clues about how to improve the system when reaching mediocre results.", "tokens": [7308, 293, 6713, 11, 365, 787, 6696, 20936, 466, 577, 281, 3470, 264, 1185, 562, 9906, 45415, 3542, 13], "temperature": 0.0, "avg_logprob": -0.07253981204259963, "compression_ratio": 1.6324786324786325, "no_speech_prob": 3.6789675505133346e-05}, {"id": 884, "seek": 621096, "start": 6218.4800000000005, "end": 6225.12, "text": " Adding more layers doesn't always help. These kinds of problems are the everyday reality to", "tokens": [31204, 544, 7914, 1177, 380, 1009, 854, 13, 1981, 3685, 295, 2740, 366, 264, 7429, 4103, 281], "temperature": 0.0, "avg_logprob": -0.07253981204259963, "compression_ratio": 1.6324786324786325, "no_speech_prob": 3.6789675505133346e-05}, {"id": 885, "seek": 621096, "start": 6225.12, "end": 6232.08, "text": " most practitioners of deep neural networks. With no a priori models, there will be no a priori", "tokens": [881, 25742, 295, 2452, 18161, 9590, 13, 2022, 572, 257, 4059, 72, 5245, 11, 456, 486, 312, 572, 257, 4059, 72], "temperature": 0.0, "avg_logprob": -0.07253981204259963, "compression_ratio": 1.6324786324786325, "no_speech_prob": 3.6789675505133346e-05}, {"id": 886, "seek": 621096, "start": 6232.08, "end": 6238.4800000000005, "text": " guarantees. The best estimate of the reliability and correctness of any deep neural network,", "tokens": [32567, 13, 440, 1151, 12539, 295, 264, 24550, 293, 3006, 1287, 295, 604, 2452, 18161, 3209, 11], "temperature": 0.0, "avg_logprob": -0.07253981204259963, "compression_ratio": 1.6324786324786325, "no_speech_prob": 3.6789675505133346e-05}, {"id": 887, "seek": 623848, "start": 6238.48, "end": 6244.5599999999995, "text": " or even any holistic system we can ever devise, is going to be extensive testing.", "tokens": [420, 754, 604, 30334, 1185, 321, 393, 1562, 1905, 908, 11, 307, 516, 281, 312, 13246, 4997, 13], "temperature": 0.0, "avg_logprob": -0.08855336468394209, "compression_ratio": 1.6233183856502242, "no_speech_prob": 3.717530853464268e-05}, {"id": 888, "seek": 623848, "start": 6245.2, "end": 6251.44, "text": " We're on this later. Why would we ever use engineered systems that cannot be guaranteed", "tokens": [492, 434, 322, 341, 1780, 13, 1545, 576, 321, 1562, 764, 38648, 3652, 300, 2644, 312, 18031], "temperature": 0.0, "avg_logprob": -0.08855336468394209, "compression_ratio": 1.6233183856502242, "no_speech_prob": 3.717530853464268e-05}, {"id": 889, "seek": 623848, "start": 6251.44, "end": 6258.879999999999, "text": " to provide the correct answer? Because we have no choice. We only use holistic methods when the", "tokens": [281, 2893, 264, 3006, 1867, 30, 1436, 321, 362, 572, 3922, 13, 492, 787, 764, 30334, 7150, 562, 264], "temperature": 0.0, "avg_logprob": -0.08855336468394209, "compression_ratio": 1.6233183856502242, "no_speech_prob": 3.717530853464268e-05}, {"id": 890, "seek": 623848, "start": 6258.879999999999, "end": 6265.919999999999, "text": " reliable reductionist methods are unavailable. As is the case when the task requires the ability", "tokens": [12924, 11004, 468, 7150, 366, 36541, 32699, 13, 1018, 307, 264, 1389, 562, 264, 5633, 7029, 264, 3485], "temperature": 0.0, "avg_logprob": -0.08855336468394209, "compression_ratio": 1.6233183856502242, "no_speech_prob": 3.717530853464268e-05}, {"id": 891, "seek": 626592, "start": 6265.92, "end": 6272.32, "text": " to perform autonomous reduction of context rich slices of our rich complex reality as a whole.", "tokens": [281, 2042, 23797, 11004, 295, 4319, 4593, 19793, 295, 527, 4593, 3997, 4103, 382, 257, 1379, 13], "temperature": 0.0, "avg_logprob": -0.10724275252398323, "compression_ratio": 1.628099173553719, "no_speech_prob": 5.88377624808345e-05}, {"id": 892, "seek": 626592, "start": 6273.04, "end": 6279.84, "text": " When the task requires understanding, don't we have an alternative to these under liable machines?", "tokens": [1133, 264, 5633, 7029, 3701, 11, 500, 380, 321, 362, 364, 8535, 281, 613, 833, 375, 712, 8379, 30], "temperature": 0.0, "avg_logprob": -0.10724275252398323, "compression_ratio": 1.628099173553719, "no_speech_prob": 5.88377624808345e-05}, {"id": 893, "seek": 626592, "start": 6280.4800000000005, "end": 6287.52, "text": " Sure we do. There are billions of humans on the planet that are already masters of this complex", "tokens": [4894, 321, 360, 13, 821, 366, 17375, 295, 6255, 322, 264, 5054, 300, 366, 1217, 19294, 295, 341, 3997], "temperature": 0.0, "avg_logprob": -0.10724275252398323, "compression_ratio": 1.628099173553719, "no_speech_prob": 5.88377624808345e-05}, {"id": 894, "seek": 626592, "start": 6287.52, "end": 6294.8, "text": " task because they live in the rich world and need skills that are unavailable with reductionist methods,", "tokens": [5633, 570, 436, 1621, 294, 264, 4593, 1002, 293, 643, 3942, 300, 366, 36541, 32699, 365, 11004, 468, 7150, 11], "temperature": 0.0, "avg_logprob": -0.10724275252398323, "compression_ratio": 1.628099173553719, "no_speech_prob": 5.88377624808345e-05}, {"id": 895, "seek": 629480, "start": 6294.8, "end": 6301.12, "text": " starting with low level things like object permanence. So you can replace a well performing", "tokens": [2891, 365, 2295, 1496, 721, 411, 2657, 8105, 655, 13, 407, 291, 393, 7406, 257, 731, 10205], "temperature": 0.0, "avg_logprob": -0.09563732147216797, "compression_ratio": 1.619047619047619, "no_speech_prob": 3.828853368759155e-05}, {"id": 896, "seek": 629480, "start": 6301.12, "end": 6307.68, "text": " but theoretically unproven contraption, a holistic understanding machine built out of deep neural", "tokens": [457, 29400, 517, 4318, 553, 10742, 1695, 11, 257, 30334, 3701, 3479, 3094, 484, 295, 2452, 18161], "temperature": 0.0, "avg_logprob": -0.09563732147216797, "compression_ratio": 1.619047619047619, "no_speech_prob": 3.828853368759155e-05}, {"id": 897, "seek": 629480, "start": 6307.68, "end": 6314.08, "text": " networks, with a well performing human being using a deeply mystical kind of understanding", "tokens": [9590, 11, 365, 257, 731, 10205, 1952, 885, 1228, 257, 8760, 40565, 733, 295, 3701], "temperature": 0.0, "avg_logprob": -0.09563732147216797, "compression_ratio": 1.619047619047619, "no_speech_prob": 3.828853368759155e-05}, {"id": 898, "seek": 629480, "start": 6314.08, "end": 6320.8, "text": " hidden in their opaque heads. Who earns much more per hour. This doesn't look like much of an", "tokens": [7633, 294, 641, 42687, 8050, 13, 2102, 46936, 709, 544, 680, 1773, 13, 639, 1177, 380, 574, 411, 709, 295, 364], "temperature": 0.0, "avg_logprob": -0.09563732147216797, "compression_ratio": 1.619047619047619, "no_speech_prob": 3.828853368759155e-05}, {"id": 899, "seek": 632080, "start": 6320.8, "end": 6327.6, "text": " improvement. The machine cannot be proven correct because it doesn't function like normal computers.", "tokens": [10444, 13, 440, 3479, 2644, 312, 12785, 3006, 570, 309, 1177, 380, 2445, 411, 2710, 10807, 13], "temperature": 0.0, "avg_logprob": -0.0948049459564552, "compression_ratio": 1.542857142857143, "no_speech_prob": 3.7786430766573176e-05}, {"id": 900, "seek": 632080, "start": 6328.16, "end": 6336.0, "text": " It is performing reduction, the skill formally restricted to animals. A holistic skill. My", "tokens": [467, 307, 10205, 11004, 11, 264, 5389, 25983, 20608, 281, 4882, 13, 316, 30334, 5389, 13, 1222], "temperature": 0.0, "avg_logprob": -0.0948049459564552, "compression_ratio": 1.542857142857143, "no_speech_prob": 3.7786430766573176e-05}, {"id": 901, "seek": 632080, "start": 6336.0, "end": 6343.28, "text": " favorite soundbite is a mere corollary to the frame problem by McCarthy and Hayes. You have seen", "tokens": [2954, 1626, 65, 642, 307, 257, 8401, 1181, 1833, 822, 281, 264, 3920, 1154, 538, 44085, 293, 8721, 279, 13, 509, 362, 1612], "temperature": 0.0, "avg_logprob": -0.0948049459564552, "compression_ratio": 1.542857142857143, "no_speech_prob": 3.7786430766573176e-05}, {"id": 902, "seek": 632080, "start": 6343.28, "end": 6349.28, "text": " it and you will see it again, since it is one of the stronger results of AI epistemology.", "tokens": [309, 293, 291, 486, 536, 309, 797, 11, 1670, 309, 307, 472, 295, 264, 7249, 3542, 295, 7318, 2388, 43958, 1793, 13], "temperature": 0.0, "avg_logprob": -0.0948049459564552, "compression_ratio": 1.542857142857143, "no_speech_prob": 3.7786430766573176e-05}, {"id": 903, "seek": 634928, "start": 6349.28, "end": 6356.5599999999995, "text": " But we will, in but a few years, agree on a definition of intelligence that makes autonomous", "tokens": [583, 321, 486, 11, 294, 457, 257, 1326, 924, 11, 3986, 322, 257, 7123, 295, 7599, 300, 1669, 23797], "temperature": 0.0, "avg_logprob": -0.14866910661969865, "compression_ratio": 1.422680412371134, "no_speech_prob": 6.843318260507658e-05}, {"id": 904, "seek": 634928, "start": 6356.5599999999995, "end": 6364.24, "text": " reduction a requirement. This once semi-heretic soundbite will then be obvious to all. If it", "tokens": [11004, 257, 11695, 13, 639, 1564, 12909, 12, 511, 3532, 1626, 65, 642, 486, 550, 312, 6322, 281, 439, 13, 759, 309], "temperature": 0.0, "avg_logprob": -0.14866910661969865, "compression_ratio": 1.422680412371134, "no_speech_prob": 6.843318260507658e-05}, {"id": 905, "seek": 634928, "start": 6364.24, "end": 6373.599999999999, "text": " isn't already, our intelligences are fallible. Chapter 6. Experimental Epistemology for AI", "tokens": [1943, 380, 1217, 11, 527, 5613, 2667, 366, 2100, 964, 13, 18874, 1386, 13, 37933, 304, 9970, 43958, 1793, 337, 7318], "temperature": 0.0, "avg_logprob": -0.14866910661969865, "compression_ratio": 1.422680412371134, "no_speech_prob": 6.843318260507658e-05}, {"id": 906, "seek": 637360, "start": 6373.6, "end": 6381.04, "text": " We can now create computer based experimental implementations to epistemology level theories", "tokens": [492, 393, 586, 1884, 3820, 2361, 17069, 4445, 763, 281, 2388, 43958, 1793, 1496, 13667], "temperature": 0.0, "avg_logprob": -0.056748124617564524, "compression_ratio": 1.6905829596412556, "no_speech_prob": 0.00010404372733319178}, {"id": 907, "seek": 637360, "start": 6381.04, "end": 6388.160000000001, "text": " in order to test them and learn from the outcomes. Experimental epistemology is the use of the", "tokens": [294, 1668, 281, 1500, 552, 293, 1466, 490, 264, 10070, 13, 37933, 304, 2388, 43958, 1793, 307, 264, 764, 295, 264], "temperature": 0.0, "avg_logprob": -0.056748124617564524, "compression_ratio": 1.6905829596412556, "no_speech_prob": 0.00010404372733319178}, {"id": 908, "seek": 637360, "start": 6388.160000000001, "end": 6394.64, "text": " experimental methods of the cognitive sciences to shed light on debates within epistemology,", "tokens": [17069, 7150, 295, 264, 15605, 17677, 281, 14951, 1442, 322, 24203, 1951, 2388, 43958, 1793, 11], "temperature": 0.0, "avg_logprob": -0.056748124617564524, "compression_ratio": 1.6905829596412556, "no_speech_prob": 0.00010404372733319178}, {"id": 909, "seek": 637360, "start": 6394.64, "end": 6402.08, "text": " the philosophical study of knowledge and rationally justified belief. Some skeptics contend that", "tokens": [264, 25066, 2979, 295, 3601, 293, 24258, 379, 27808, 7107, 13, 2188, 19128, 1167, 660, 521, 300], "temperature": 0.0, "avg_logprob": -0.056748124617564524, "compression_ratio": 1.6905829596412556, "no_speech_prob": 0.00010404372733319178}, {"id": 910, "seek": 640208, "start": 6402.08, "end": 6409.6, "text": " experimental epistemology or experimental philosophy more generally is an oxymoron.", "tokens": [17069, 2388, 43958, 1793, 420, 17069, 10675, 544, 5101, 307, 364, 5976, 4199, 284, 266, 13], "temperature": 0.0, "avg_logprob": -0.08623271866848595, "compression_ratio": 1.8177339901477831, "no_speech_prob": 5.4306867241393775e-05}, {"id": 911, "seek": 640208, "start": 6410.32, "end": 6417.6, "text": " If you are doing experiments, they say, you are not doing philosophy. You are doing psychology", "tokens": [759, 291, 366, 884, 12050, 11, 436, 584, 11, 291, 366, 406, 884, 10675, 13, 509, 366, 884, 15105], "temperature": 0.0, "avg_logprob": -0.08623271866848595, "compression_ratio": 1.8177339901477831, "no_speech_prob": 5.4306867241393775e-05}, {"id": 912, "seek": 640208, "start": 6417.6, "end": 6424.08, "text": " or some other scientific activity. It is true that the part of experimental philosophy that is", "tokens": [420, 512, 661, 8134, 5191, 13, 467, 307, 2074, 300, 264, 644, 295, 17069, 10675, 300, 307], "temperature": 0.0, "avg_logprob": -0.08623271866848595, "compression_ratio": 1.8177339901477831, "no_speech_prob": 5.4306867241393775e-05}, {"id": 913, "seek": 640208, "start": 6424.08, "end": 6430.24, "text": " devoted to carrying out experiments and performing statistical analyses on the data obtained is", "tokens": [21815, 281, 9792, 484, 12050, 293, 10205, 22820, 37560, 322, 264, 1412, 14879, 307], "temperature": 0.0, "avg_logprob": -0.08623271866848595, "compression_ratio": 1.8177339901477831, "no_speech_prob": 5.4306867241393775e-05}, {"id": 914, "seek": 643024, "start": 6430.24, "end": 6437.599999999999, "text": " primarily a scientific rather than a philosophical activity. However, because the experiments are", "tokens": [10029, 257, 8134, 2831, 813, 257, 25066, 5191, 13, 2908, 11, 570, 264, 12050, 366], "temperature": 0.0, "avg_logprob": -0.06374795777457101, "compression_ratio": 1.776190476190476, "no_speech_prob": 4.9152793508255854e-05}, {"id": 915, "seek": 643024, "start": 6437.599999999999, "end": 6444.5599999999995, "text": " designed to shed light on debates within philosophy, the experiments themselves grow out of mainstream", "tokens": [4761, 281, 14951, 1442, 322, 24203, 1951, 10675, 11, 264, 12050, 2969, 1852, 484, 295, 15960], "temperature": 0.0, "avg_logprob": -0.06374795777457101, "compression_ratio": 1.776190476190476, "no_speech_prob": 4.9152793508255854e-05}, {"id": 916, "seek": 643024, "start": 6444.5599999999995, "end": 6450.48, "text": " philosophical debate and their results are injected back into the debate, with an item", "tokens": [25066, 7958, 293, 641, 3542, 366, 36967, 646, 666, 264, 7958, 11, 365, 364, 3174], "temperature": 0.0, "avg_logprob": -0.06374795777457101, "compression_ratio": 1.776190476190476, "no_speech_prob": 4.9152793508255854e-05}, {"id": 917, "seek": 643024, "start": 6450.48, "end": 6457.04, "text": " moving the debate forward. This part of experimental philosophy is indeed philosophy,", "tokens": [2684, 264, 7958, 2128, 13, 639, 644, 295, 17069, 10675, 307, 6451, 10675, 11], "temperature": 0.0, "avg_logprob": -0.06374795777457101, "compression_ratio": 1.776190476190476, "no_speech_prob": 4.9152793508255854e-05}, {"id": 918, "seek": 645704, "start": 6457.04, "end": 6464.96, "text": " not philosophy as usual perhaps, but philosophy nonetheless. Experimental epistemology by James", "tokens": [406, 10675, 382, 7713, 4317, 11, 457, 10675, 26756, 13, 37933, 304, 2388, 43958, 1793, 538, 5678], "temperature": 0.0, "avg_logprob": -0.08620169321695964, "compression_ratio": 1.6180257510729614, "no_speech_prob": 1.464888464397518e-05}, {"id": 919, "seek": 645704, "start": 6464.96, "end": 6471.68, "text": " R. B. B. Traditional experimental epistemology conducted experiments on interviews and psychological", "tokens": [497, 13, 363, 13, 363, 13, 46738, 17069, 2388, 43958, 1793, 13809, 12050, 322, 12318, 293, 14346], "temperature": 0.0, "avg_logprob": -0.08620169321695964, "compression_ratio": 1.6180257510729614, "no_speech_prob": 1.464888464397518e-05}, {"id": 920, "seek": 645704, "start": 6471.68, "end": 6478.64, "text": " tests on human volunteers or relied on population statistics. As one of the newer branches of", "tokens": [6921, 322, 1952, 14352, 420, 35463, 322, 4415, 12523, 13, 1018, 472, 295, 264, 17628, 14770, 295], "temperature": 0.0, "avg_logprob": -0.08620169321695964, "compression_ratio": 1.6180257510729614, "no_speech_prob": 1.464888464397518e-05}, {"id": 921, "seek": 645704, "start": 6478.64, "end": 6484.56, "text": " cognitive science, machine learning has now provided us with a very different approach", "tokens": [15605, 3497, 11, 3479, 2539, 575, 586, 5649, 505, 365, 257, 588, 819, 3109], "temperature": 0.0, "avg_logprob": -0.08620169321695964, "compression_ratio": 1.6180257510729614, "no_speech_prob": 1.464888464397518e-05}, {"id": 922, "seek": 648456, "start": 6484.56, "end": 6491.4400000000005, "text": " to this domain. We can now create computer-based experimental implementations to epistemology", "tokens": [281, 341, 9274, 13, 492, 393, 586, 1884, 3820, 12, 6032, 17069, 4445, 763, 281, 2388, 43958, 1793], "temperature": 0.0, "avg_logprob": -0.04498015611599653, "compression_ratio": 1.7116279069767442, "no_speech_prob": 6.566711817868054e-05}, {"id": 923, "seek": 648456, "start": 6491.4400000000005, "end": 6497.84, "text": " level theories in order to test them and learn from the outcomes. In machine learning, the most", "tokens": [1496, 13667, 294, 1668, 281, 1500, 552, 293, 1466, 490, 264, 10070, 13, 682, 3479, 2539, 11, 264, 881], "temperature": 0.0, "avg_logprob": -0.04498015611599653, "compression_ratio": 1.7116279069767442, "no_speech_prob": 6.566711817868054e-05}, {"id": 924, "seek": 648456, "start": 6497.84, "end": 6504.400000000001, "text": " important epistemology level concepts and hypotheses are about reasoning, understanding,", "tokens": [1021, 2388, 43958, 1793, 1496, 10392, 293, 49969, 366, 466, 21577, 11, 3701, 11], "temperature": 0.0, "avg_logprob": -0.04498015611599653, "compression_ratio": 1.7116279069767442, "no_speech_prob": 6.566711817868054e-05}, {"id": 925, "seek": 648456, "start": 6504.400000000001, "end": 6512.4800000000005, "text": " learning, epistemic reduction, abstraction, creativity, prediction, attention, instincts,", "tokens": [2539, 11, 2388, 468, 3438, 11004, 11, 37765, 11, 12915, 11, 17630, 11, 3202, 11, 38997, 11], "temperature": 0.0, "avg_logprob": -0.04498015611599653, "compression_ratio": 1.7116279069767442, "no_speech_prob": 6.566711817868054e-05}, {"id": 926, "seek": 651248, "start": 6512.48, "end": 6520.48, "text": " intuitions, concepts, resiliency, models, reductionism, wholism, and other things all", "tokens": [16224, 626, 11, 10392, 11, 48712, 11, 5245, 11, 11004, 1434, 11, 315, 401, 1434, 11, 293, 661, 721, 439], "temperature": 0.0, "avg_logprob": -0.11750874374852036, "compression_ratio": 1.576470588235294, "no_speech_prob": 4.990591332898475e-05}, {"id": 927, "seek": 651248, "start": 6520.48, "end": 6529.04, "text": " sharing these features. One, science has no equations, formulas, or other models for how", "tokens": [5414, 613, 4122, 13, 1485, 11, 3497, 575, 572, 11787, 11, 30546, 11, 420, 661, 5245, 337, 577], "temperature": 0.0, "avg_logprob": -0.11750874374852036, "compression_ratio": 1.576470588235294, "no_speech_prob": 4.990591332898475e-05}, {"id": 928, "seek": 651248, "start": 6529.04, "end": 6538.08, "text": " they work. They're epistemology level concepts, not science level concepts. Two, our theories", "tokens": [436, 589, 13, 814, 434, 2388, 43958, 1793, 1496, 10392, 11, 406, 3497, 1496, 10392, 13, 4453, 11, 527, 13667], "temperature": 0.0, "avg_logprob": -0.11750874374852036, "compression_ratio": 1.576470588235294, "no_speech_prob": 4.990591332898475e-05}, {"id": 929, "seek": 653808, "start": 6538.08, "end": 6544.72, "text": " about these concepts have to be sufficiently solid and detailed to allow for computer implementations.", "tokens": [466, 613, 10392, 362, 281, 312, 31868, 5100, 293, 9942, 281, 2089, 337, 3820, 4445, 763, 13], "temperature": 0.0, "avg_logprob": -0.08083287204604551, "compression_ratio": 1.6652360515021458, "no_speech_prob": 1.6847900042193942e-05}, {"id": 930, "seek": 653808, "start": 6545.44, "end": 6552.48, "text": " This is because science itself is built on top of epistemology level concepts, and practitioners", "tokens": [639, 307, 570, 3497, 2564, 307, 3094, 322, 1192, 295, 2388, 43958, 1793, 1496, 10392, 11, 293, 25742], "temperature": 0.0, "avg_logprob": -0.08083287204604551, "compression_ratio": 1.6652360515021458, "no_speech_prob": 1.6847900042193942e-05}, {"id": 931, "seek": 653808, "start": 6552.48, "end": 6558.48, "text": " need to be aware of this or they will experience cognitive dissonance-induced confusion and stress.", "tokens": [643, 281, 312, 3650, 295, 341, 420, 436, 486, 1752, 15605, 717, 3015, 719, 12, 471, 41209, 15075, 293, 4244, 13], "temperature": 0.0, "avg_logprob": -0.08083287204604551, "compression_ratio": 1.6652360515021458, "no_speech_prob": 1.6847900042193942e-05}, {"id": 932, "seek": 653808, "start": 6559.12, "end": 6564.72, "text": " The red pill of machine learning confronts the elephant in the room of machine learning.", "tokens": [440, 2182, 8100, 295, 3479, 2539, 12422, 82, 264, 19791, 294, 264, 1808, 295, 3479, 2539, 13], "temperature": 0.0, "avg_logprob": -0.08083287204604551, "compression_ratio": 1.6652360515021458, "no_speech_prob": 1.6847900042193942e-05}, {"id": 933, "seek": 656472, "start": 6564.72, "end": 6572.16, "text": " Machine learning is not scientific. What can we learn from AI epistemology? An excerpt from the", "tokens": [22155, 2539, 307, 406, 8134, 13, 708, 393, 321, 1466, 490, 7318, 2388, 43958, 1793, 30, 1107, 42760, 662, 490, 264], "temperature": 0.0, "avg_logprob": -0.08221422490619477, "compression_ratio": 1.6200873362445414, "no_speech_prob": 5.805581895401701e-05}, {"id": 934, "seek": 656472, "start": 6572.16, "end": 6579.04, "text": " red pill can say the following statements from the domain of epistemology and how each of them", "tokens": [2182, 8100, 393, 584, 264, 3480, 12363, 490, 264, 9274, 295, 2388, 43958, 1793, 293, 577, 1184, 295, 552], "temperature": 0.0, "avg_logprob": -0.08221422490619477, "compression_ratio": 1.6200873362445414, "no_speech_prob": 5.805581895401701e-05}, {"id": 935, "seek": 656472, "start": 6579.04, "end": 6585.76, "text": " can be viewed as an implementation hint for AI designers. We are already able to measure", "tokens": [393, 312, 19174, 382, 364, 11420, 12075, 337, 7318, 16196, 13, 492, 366, 1217, 1075, 281, 3481], "temperature": 0.0, "avg_logprob": -0.08221422490619477, "compression_ratio": 1.6200873362445414, "no_speech_prob": 5.805581895401701e-05}, {"id": 936, "seek": 656472, "start": 6585.76, "end": 6592.0, "text": " their effects and system competence. You can only learn that which you already almost know.", "tokens": [641, 5065, 293, 1185, 39965, 13, 509, 393, 787, 1466, 300, 597, 291, 1217, 1920, 458, 13], "temperature": 0.0, "avg_logprob": -0.08221422490619477, "compression_ratio": 1.6200873362445414, "no_speech_prob": 5.805581895401701e-05}, {"id": 937, "seek": 659200, "start": 6592.0, "end": 6601.2, "text": " Patrick Winston, MIT. Our intelligences are fallible. Monica Anderson. In order to detect", "tokens": [13980, 33051, 11, 13100, 13, 2621, 5613, 2667, 366, 2100, 964, 13, 25363, 18768, 13, 682, 1668, 281, 5531], "temperature": 0.0, "avg_logprob": -0.120929810308641, "compression_ratio": 1.5485714285714285, "no_speech_prob": 0.00016934207815211266}, {"id": 938, "seek": 659200, "start": 6601.2, "end": 6608.08, "text": " that something is new, you need to recognize everything old. Monica Anderson. You cannot", "tokens": [300, 746, 307, 777, 11, 291, 643, 281, 5521, 1203, 1331, 13, 25363, 18768, 13, 509, 2644], "temperature": 0.0, "avg_logprob": -0.120929810308641, "compression_ratio": 1.5485714285714285, "no_speech_prob": 0.00016934207815211266}, {"id": 939, "seek": 659200, "start": 6608.08, "end": 6615.12, "text": " reason about that which you do not understand. Monica Anderson. You are known by the company", "tokens": [1778, 466, 300, 597, 291, 360, 406, 1223, 13, 25363, 18768, 13, 509, 366, 2570, 538, 264, 2237], "temperature": 0.0, "avg_logprob": -0.120929810308641, "compression_ratio": 1.5485714285714285, "no_speech_prob": 0.00016934207815211266}, {"id": 940, "seek": 661512, "start": 6615.12, "end": 6622.4, "text": " you keep, simple version of the Yanida Lemur from Category Theory and the justification for embeddings", "tokens": [291, 1066, 11, 2199, 3037, 295, 264, 13633, 2887, 16905, 374, 490, 383, 48701, 29009, 293, 264, 31591, 337, 12240, 29432], "temperature": 0.0, "avg_logprob": -0.12433204366200006, "compression_ratio": 1.5238095238095237, "no_speech_prob": 5.440497989184223e-05}, {"id": 941, "seek": 661512, "start": 6622.4, "end": 6628.8, "text": " in deep learning. All useful novelty in the universe is due to processes of variation and", "tokens": [294, 2452, 2539, 13, 1057, 4420, 44805, 294, 264, 6445, 307, 3462, 281, 7555, 295, 12990, 293], "temperature": 0.0, "avg_logprob": -0.12433204366200006, "compression_ratio": 1.5238095238095237, "no_speech_prob": 5.440497989184223e-05}, {"id": 942, "seek": 661512, "start": 6628.8, "end": 6637.2, "text": " selection. The selectionist manifesto. Selectionism is the generalization of Darwinism. This is", "tokens": [9450, 13, 440, 9450, 468, 10067, 78, 13, 1100, 5450, 1434, 307, 264, 2674, 2144, 295, 30233, 1434, 13, 639, 307], "temperature": 0.0, "avg_logprob": -0.12433204366200006, "compression_ratio": 1.5238095238095237, "no_speech_prob": 5.440497989184223e-05}, {"id": 943, "seek": 663720, "start": 6637.2, "end": 6645.92, "text": " right genetic algorithms work. Science has no equations for concepts like understanding, reasoning,", "tokens": [558, 12462, 14642, 589, 13, 8976, 575, 572, 11787, 337, 10392, 411, 3701, 11, 21577, 11], "temperature": 0.0, "avg_logprob": -0.05694109354263697, "compression_ratio": 1.64, "no_speech_prob": 4.628010356100276e-05}, {"id": 944, "seek": 663720, "start": 6645.92, "end": 6651.76, "text": " learning, abstraction, or modeling since they are all epistemology level concepts.", "tokens": [2539, 11, 37765, 11, 420, 15983, 1670, 436, 366, 439, 2388, 43958, 1793, 1496, 10392, 13], "temperature": 0.0, "avg_logprob": -0.05694109354263697, "compression_ratio": 1.64, "no_speech_prob": 4.628010356100276e-05}, {"id": 945, "seek": 663720, "start": 6652.4, "end": 6659.28, "text": " We cannot even start using science until we have decided what model to use. We must use our", "tokens": [492, 2644, 754, 722, 1228, 3497, 1826, 321, 362, 3047, 437, 2316, 281, 764, 13, 492, 1633, 764, 527], "temperature": 0.0, "avg_logprob": -0.05694109354263697, "compression_ratio": 1.64, "no_speech_prob": 4.628010356100276e-05}, {"id": 946, "seek": 663720, "start": 6659.28, "end": 6665.84, "text": " experience to perform epistemic reductions, discarding the irrelevant, starting from the messy", "tokens": [1752, 281, 2042, 2388, 468, 3438, 40296, 11, 31597, 278, 264, 28682, 11, 2891, 490, 264, 16191], "temperature": 0.0, "avg_logprob": -0.05694109354263697, "compression_ratio": 1.64, "no_speech_prob": 4.628010356100276e-05}, {"id": 947, "seek": 666584, "start": 6665.84, "end": 6672.8, "text": " real world problem situation until we are left with a scientific model we can use, such as an", "tokens": [957, 1002, 1154, 2590, 1826, 321, 366, 1411, 365, 257, 8134, 2316, 321, 393, 764, 11, 1270, 382, 364], "temperature": 0.0, "avg_logprob": -0.09077822984154545, "compression_ratio": 1.5238095238095237, "no_speech_prob": 2.5925291993189603e-05}, {"id": 948, "seek": 666584, "start": 6672.8, "end": 6680.16, "text": " equation. The focus in AI research should be on exactly how we can get our machines to perform", "tokens": [5367, 13, 440, 1879, 294, 7318, 2132, 820, 312, 322, 2293, 577, 321, 393, 483, 527, 8379, 281, 2042], "temperature": 0.0, "avg_logprob": -0.09077822984154545, "compression_ratio": 1.5238095238095237, "no_speech_prob": 2.5925291993189603e-05}, {"id": 949, "seek": 666584, "start": 6680.16, "end": 6687.68, "text": " this pre-scientific epistemic reduction by themselves and the answer to that cannot be found inside", "tokens": [341, 659, 12, 82, 5412, 1089, 2388, 468, 3438, 11004, 538, 2969, 293, 264, 1867, 281, 300, 2644, 312, 1352, 1854], "temperature": 0.0, "avg_logprob": -0.09077822984154545, "compression_ratio": 1.5238095238095237, "no_speech_prob": 2.5925291993189603e-05}, {"id": 950, "seek": 668768, "start": 6687.68, "end": 6696.0, "text": " of science. Chapter 7. The Red Pill of Machine Learning. Reductionism is the use of models.", "tokens": [295, 3497, 13, 18874, 1614, 13, 440, 4477, 44656, 295, 22155, 15205, 13, 4477, 27549, 1434, 307, 264, 764, 295, 5245, 13], "temperature": 0.0, "avg_logprob": -0.13612769200251654, "compression_ratio": 1.5393258426966292, "no_speech_prob": 0.0001399212924297899}, {"id": 951, "seek": 668768, "start": 6696.64, "end": 6705.4400000000005, "text": " Holism is the avoidance of models. Models are scientific models, theories, hypotheses, formulas,", "tokens": [11086, 1434, 307, 264, 5042, 719, 295, 5245, 13, 6583, 1625, 366, 8134, 5245, 11, 13667, 11, 49969, 11, 30546, 11], "temperature": 0.0, "avg_logprob": -0.13612769200251654, "compression_ratio": 1.5393258426966292, "no_speech_prob": 0.0001399212924297899}, {"id": 952, "seek": 668768, "start": 6705.4400000000005, "end": 6712.4800000000005, "text": " equations, naive models based on personal experiences, superstitions, and traditional", "tokens": [11787, 11, 29052, 5245, 2361, 322, 2973, 5235, 11, 29423, 2451, 11, 293, 5164], "temperature": 0.0, "avg_logprob": -0.13612769200251654, "compression_ratio": 1.5393258426966292, "no_speech_prob": 0.0001399212924297899}, {"id": 953, "seek": 671248, "start": 6712.48, "end": 6720.799999999999, "text": " computer programs. The deep learning revolution of 2012 changed how we think about artificial", "tokens": [3820, 4268, 13, 440, 2452, 2539, 8894, 295, 9125, 3105, 577, 321, 519, 466, 11677], "temperature": 0.0, "avg_logprob": -0.058171529036301836, "compression_ratio": 1.6637931034482758, "no_speech_prob": 3.728127558133565e-05}, {"id": 954, "seek": 671248, "start": 6720.799999999999, "end": 6728.08, "text": " intelligence, machine learning, and deep neural networks. What changed, and what does this mean", "tokens": [7599, 11, 3479, 2539, 11, 293, 2452, 18161, 9590, 13, 708, 3105, 11, 293, 437, 775, 341, 914], "temperature": 0.0, "avg_logprob": -0.058171529036301836, "compression_ratio": 1.6637931034482758, "no_speech_prob": 3.728127558133565e-05}, {"id": 955, "seek": 671248, "start": 6728.08, "end": 6734.799999999999, "text": " going forward? The new cognitive capabilities in our machines are the result of a shift in the way", "tokens": [516, 2128, 30, 440, 777, 15605, 10862, 294, 527, 8379, 366, 264, 1874, 295, 257, 5513, 294, 264, 636], "temperature": 0.0, "avg_logprob": -0.058171529036301836, "compression_ratio": 1.6637931034482758, "no_speech_prob": 3.728127558133565e-05}, {"id": 956, "seek": 671248, "start": 6734.799999999999, "end": 6741.599999999999, "text": " we think about problem solving. It is the most significant change ever in artificial intelligence", "tokens": [321, 519, 466, 1154, 12606, 13, 467, 307, 264, 881, 4776, 1319, 1562, 294, 11677, 7599], "temperature": 0.0, "avg_logprob": -0.058171529036301836, "compression_ratio": 1.6637931034482758, "no_speech_prob": 3.728127558133565e-05}, {"id": 957, "seek": 674160, "start": 6741.6, "end": 6749.360000000001, "text": " AI, if not in science as a whole. Machine learning, ML based systems are successfully", "tokens": [7318, 11, 498, 406, 294, 3497, 382, 257, 1379, 13, 22155, 2539, 11, 21601, 2361, 3652, 366, 10727], "temperature": 0.0, "avg_logprob": -0.07990979552268981, "compression_ratio": 1.5862068965517242, "no_speech_prob": 3.0240920750657097e-05}, {"id": 958, "seek": 674160, "start": 6749.360000000001, "end": 6756.0, "text": " attacking both simple and complex problems using novel methods that only became available after", "tokens": [15010, 1293, 2199, 293, 3997, 2740, 1228, 7613, 7150, 300, 787, 3062, 2435, 934], "temperature": 0.0, "avg_logprob": -0.07990979552268981, "compression_ratio": 1.5862068965517242, "no_speech_prob": 3.0240920750657097e-05}, {"id": 959, "seek": 674160, "start": 6756.0, "end": 6763.76, "text": " 2012. We are experiencing a revolution at the level of epistemology which will affect much more", "tokens": [9125, 13, 492, 366, 11139, 257, 8894, 412, 264, 1496, 295, 2388, 43958, 1793, 597, 486, 3345, 709, 544], "temperature": 0.0, "avg_logprob": -0.07990979552268981, "compression_ratio": 1.5862068965517242, "no_speech_prob": 3.0240920750657097e-05}, {"id": 960, "seek": 674160, "start": 6763.76, "end": 6770.08, "text": " than just the field of machine learning. We want to add more of these novel methods to our", "tokens": [813, 445, 264, 2519, 295, 3479, 2539, 13, 492, 528, 281, 909, 544, 295, 613, 7613, 7150, 281, 527], "temperature": 0.0, "avg_logprob": -0.07990979552268981, "compression_ratio": 1.5862068965517242, "no_speech_prob": 3.0240920750657097e-05}, {"id": 961, "seek": 677008, "start": 6770.08, "end": 6776.48, "text": " standard problem solving toolkit, but we need to understand the trade-offs and the conflict.", "tokens": [3832, 1154, 12606, 40167, 11, 457, 321, 643, 281, 1223, 264, 4923, 12, 19231, 293, 264, 6596, 13], "temperature": 0.0, "avg_logprob": -0.06988723189742477, "compression_ratio": 1.5932203389830508, "no_speech_prob": 4.584324051393196e-05}, {"id": 962, "seek": 677008, "start": 6777.04, "end": 6784.24, "text": " I argue that understanding deep neural networks, DNNs, and other ML technologies requires that", "tokens": [286, 9695, 300, 3701, 2452, 18161, 9590, 11, 21500, 45, 82, 11, 293, 661, 21601, 7943, 7029, 300], "temperature": 0.0, "avg_logprob": -0.06988723189742477, "compression_ratio": 1.5932203389830508, "no_speech_prob": 4.584324051393196e-05}, {"id": 963, "seek": 677008, "start": 6784.24, "end": 6791.04, "text": " practitioners adopt a holistic stance which is, at important levels, blatantly incompatible with", "tokens": [25742, 6878, 257, 30334, 21033, 597, 307, 11, 412, 1021, 4358, 11, 42780, 3627, 40393, 267, 964, 365], "temperature": 0.0, "avg_logprob": -0.06988723189742477, "compression_ratio": 1.5932203389830508, "no_speech_prob": 4.584324051393196e-05}, {"id": 964, "seek": 677008, "start": 6791.04, "end": 6797.76, "text": " the reductionist stance of modern science. As ML practitioners we have to make hard choices", "tokens": [264, 11004, 468, 21033, 295, 4363, 3497, 13, 1018, 21601, 25742, 321, 362, 281, 652, 1152, 7994], "temperature": 0.0, "avg_logprob": -0.06988723189742477, "compression_ratio": 1.5932203389830508, "no_speech_prob": 4.584324051393196e-05}, {"id": 965, "seek": 679776, "start": 6797.76, "end": 6804.16, "text": " that seemingly contradict many of our core scientific convictions. As a result we may get", "tokens": [300, 18709, 28900, 867, 295, 527, 4965, 8134, 44757, 13, 1018, 257, 1874, 321, 815, 483], "temperature": 0.0, "avg_logprob": -0.050121200084686277, "compression_ratio": 1.5916666666666666, "no_speech_prob": 4.007219831692055e-05}, {"id": 966, "seek": 679776, "start": 6804.16, "end": 6810.96, "text": " the feeling something is wrong. The conflict is real and important and the seemingly counter-intuitive", "tokens": [264, 2633, 746, 307, 2085, 13, 440, 6596, 307, 957, 293, 1021, 293, 264, 18709, 5682, 12, 686, 48314], "temperature": 0.0, "avg_logprob": -0.050121200084686277, "compression_ratio": 1.5916666666666666, "no_speech_prob": 4.007219831692055e-05}, {"id": 967, "seek": 679776, "start": 6810.96, "end": 6818.16, "text": " choices make sense only when viewed in the light of epistemology. Improved clarity in these matters", "tokens": [7994, 652, 2020, 787, 562, 19174, 294, 264, 1442, 295, 2388, 43958, 1793, 13, 8270, 340, 937, 16992, 294, 613, 7001], "temperature": 0.0, "avg_logprob": -0.050121200084686277, "compression_ratio": 1.5916666666666666, "no_speech_prob": 4.007219831692055e-05}, {"id": 968, "seek": 679776, "start": 6818.16, "end": 6823.76, "text": " should alleviate the cognitive dissonance experienced by some ML practitioners and should", "tokens": [820, 42701, 264, 15605, 717, 3015, 719, 6751, 538, 512, 21601, 25742, 293, 820], "temperature": 0.0, "avg_logprob": -0.050121200084686277, "compression_ratio": 1.5916666666666666, "no_speech_prob": 4.007219831692055e-05}, {"id": 969, "seek": 682376, "start": 6823.76, "end": 6829.68, "text": " accelerate progress in these fields. The title refers to the eye-opening clarity", "tokens": [21341, 4205, 294, 613, 7909, 13, 440, 4876, 14942, 281, 264, 3313, 12, 404, 4559, 16992], "temperature": 0.0, "avg_logprob": -0.13054908405650745, "compression_ratio": 1.5726872246696035, "no_speech_prob": 3.3648662792984396e-05}, {"id": 970, "seek": 682376, "start": 6829.68, "end": 6836.400000000001, "text": " some machine learning practitioners achieve when adopting a holistic stance. Parallel dichotomies", "tokens": [512, 3479, 2539, 25742, 4584, 562, 32328, 257, 30334, 21033, 13, 3457, 336, 338, 10390, 42939, 530], "temperature": 0.0, "avg_logprob": -0.13054908405650745, "compression_ratio": 1.5726872246696035, "no_speech_prob": 3.3648662792984396e-05}, {"id": 971, "seek": 682376, "start": 6836.400000000001, "end": 6843.52, "text": " sentient sync research is natural language understanding, NLU. We are creating novel", "tokens": [2279, 1196, 20271, 2132, 307, 3303, 2856, 3701, 11, 426, 43, 52, 13, 492, 366, 4084, 7613], "temperature": 0.0, "avg_logprob": -0.13054908405650745, "compression_ratio": 1.5726872246696035, "no_speech_prob": 3.3648662792984396e-05}, {"id": 972, "seek": 682376, "start": 6843.52, "end": 6850.0, "text": " systems that allow computers to learn to understand human natural languages. Any one of them,", "tokens": [3652, 300, 2089, 10807, 281, 1466, 281, 1223, 1952, 3303, 8650, 13, 2639, 472, 295, 552, 11], "temperature": 0.0, "avg_logprob": -0.13054908405650745, "compression_ratio": 1.5726872246696035, "no_speech_prob": 3.3648662792984396e-05}, {"id": 973, "seek": 685000, "start": 6850.0, "end": 6857.44, "text": " we use deep neural networks of our own design. The goal is to achieve some kind of human-like", "tokens": [321, 764, 2452, 18161, 9590, 295, 527, 1065, 1715, 13, 440, 3387, 307, 281, 4584, 512, 733, 295, 1952, 12, 4092], "temperature": 0.0, "avg_logprob": -0.08331075039776889, "compression_ratio": 1.550420168067227, "no_speech_prob": 8.71204974828288e-05}, {"id": 974, "seek": 685000, "start": 6857.44, "end": 6864.0, "text": " but not necessarily human-level understanding. This is very different from traditional natural", "tokens": [457, 406, 4725, 1952, 12, 12418, 3701, 13, 639, 307, 588, 819, 490, 5164, 3303], "temperature": 0.0, "avg_logprob": -0.08331075039776889, "compression_ratio": 1.550420168067227, "no_speech_prob": 8.71204974828288e-05}, {"id": 975, "seek": 685000, "start": 6864.0, "end": 6871.84, "text": " language processing, NLP, which relies on human-made models of some language, such as English,", "tokens": [2856, 9007, 11, 426, 43, 47, 11, 597, 30910, 322, 1952, 12, 10341, 5245, 295, 512, 2856, 11, 1270, 382, 3669, 11], "temperature": 0.0, "avg_logprob": -0.08331075039776889, "compression_ratio": 1.550420168067227, "no_speech_prob": 8.71204974828288e-05}, {"id": 976, "seek": 685000, "start": 6871.84, "end": 6878.8, "text": " and perhaps models of fragments of the world. The NLP and NLU disciplines have chosen", "tokens": [293, 4317, 5245, 295, 29197, 295, 264, 1002, 13, 440, 426, 43, 47, 293, 426, 43, 52, 21919, 362, 8614], "temperature": 0.0, "avg_logprob": -0.08331075039776889, "compression_ratio": 1.550420168067227, "no_speech_prob": 8.71204974828288e-05}, {"id": 977, "seek": 687880, "start": 6878.8, "end": 6885.4400000000005, "text": " opposite answers to their difficult two-way choices. They are now defined by these choices,", "tokens": [6182, 6338, 281, 641, 2252, 732, 12, 676, 7994, 13, 814, 366, 586, 7642, 538, 613, 7994, 11], "temperature": 0.0, "avg_logprob": -0.09067982719058082, "compression_ratio": 1.617391304347826, "no_speech_prob": 5.341472206055187e-05}, {"id": 978, "seek": 687880, "start": 6885.4400000000005, "end": 6891.52, "text": " and we can use their stances to highlight the main conflict. The split is so deep", "tokens": [293, 321, 393, 764, 641, 342, 2676, 281, 5078, 264, 2135, 6596, 13, 440, 7472, 307, 370, 2452], "temperature": 0.0, "avg_logprob": -0.09067982719058082, "compression_ratio": 1.617391304347826, "no_speech_prob": 5.341472206055187e-05}, {"id": 979, "seek": 687880, "start": 6891.52, "end": 6898.08, "text": " that it cuts through many layers of our reality. The following dichotomies are all manifestations", "tokens": [300, 309, 9992, 807, 867, 7914, 295, 527, 4103, 13, 440, 3480, 10390, 42939, 530, 366, 439, 46931], "temperature": 0.0, "avg_logprob": -0.09067982719058082, "compression_ratio": 1.617391304347826, "no_speech_prob": 5.341472206055187e-05}, {"id": 980, "seek": 687880, "start": 6898.08, "end": 6905.360000000001, "text": " of this incompatibility at different levels, listed by impact, but discussed in no particular order.", "tokens": [295, 341, 40393, 267, 2841, 412, 819, 4358, 11, 10052, 538, 2712, 11, 457, 7152, 294, 572, 1729, 1668, 13], "temperature": 0.0, "avg_logprob": -0.09067982719058082, "compression_ratio": 1.617391304347826, "no_speech_prob": 5.341472206055187e-05}, {"id": 981, "seek": 690536, "start": 6905.36, "end": 6915.28, "text": " The main science, the complex, including the mundane, epistemology, reductionism,", "tokens": [440, 2135, 3497, 11, 264, 3997, 11, 3009, 264, 43497, 11, 2388, 43958, 1793, 11, 11004, 1434, 11], "temperature": 0.0, "avg_logprob": -0.27668812091533956, "compression_ratio": 1.4970760233918128, "no_speech_prob": 0.00033540173899382353}, {"id": 982, "seek": 690536, "start": 6916.0, "end": 6927.04, "text": " realism, meanings, reasoning, understanding, problem solving, plan it, then do it, just do it.", "tokens": [38484, 11, 28138, 11, 21577, 11, 3701, 11, 1154, 12606, 11, 1393, 309, 11, 550, 360, 309, 11, 445, 360, 309, 13], "temperature": 0.0, "avg_logprob": -0.27668812091533956, "compression_ratio": 1.4970760233918128, "no_speech_prob": 0.00033540173899382353}, {"id": 983, "seek": 690536, "start": 6927.679999999999, "end": 6934.96, "text": " Artificial intelligence, 20th century, good, old-fashioned AI machine learning,", "tokens": [5735, 10371, 7599, 11, 945, 392, 4901, 11, 665, 11, 1331, 12, 37998, 7318, 3479, 2539, 11], "temperature": 0.0, "avg_logprob": -0.27668812091533956, "compression_ratio": 1.4970760233918128, "no_speech_prob": 0.00033540173899382353}, {"id": 984, "seek": 693496, "start": 6934.96, "end": 6944.72, "text": " deep neural networks, natural language and computers, NLP, NLU. The problem-solving level", "tokens": [2452, 18161, 9590, 11, 3303, 2856, 293, 10807, 11, 426, 45196, 11, 426, 43, 52, 13, 440, 1154, 12, 30926, 798, 1496], "temperature": 0.0, "avg_logprob": -0.08787335455417633, "compression_ratio": 1.4550264550264551, "no_speech_prob": 0.00011020554666174576}, {"id": 985, "seek": 693496, "start": 6944.72, "end": 6952.08, "text": " provides many familiar examples of these issues. In our mundane lives, we solve many kinds of", "tokens": [6417, 867, 4963, 5110, 295, 613, 2663, 13, 682, 527, 43497, 2909, 11, 321, 5039, 867, 3685, 295], "temperature": 0.0, "avg_logprob": -0.08787335455417633, "compression_ratio": 1.4550264550264551, "no_speech_prob": 0.00011020554666174576}, {"id": 986, "seek": 693496, "start": 6952.08, "end": 6958.4, "text": " problems every day but our strategies for solving them fall into just those two categories.", "tokens": [2740, 633, 786, 457, 527, 9029, 337, 12606, 552, 2100, 666, 445, 729, 732, 10479, 13], "temperature": 0.0, "avg_logprob": -0.08787335455417633, "compression_ratio": 1.4550264550264551, "no_speech_prob": 0.00011020554666174576}, {"id": 987, "seek": 695840, "start": 6958.4, "end": 6966.719999999999, "text": " For any complicated problem, we had better have a plan before we start, but most problems", "tokens": [1171, 604, 6179, 1154, 11, 321, 632, 1101, 362, 257, 1393, 949, 321, 722, 11, 457, 881, 2740], "temperature": 0.0, "avg_logprob": -0.1181375240457469, "compression_ratio": 1.65, "no_speech_prob": 8.121225982904434e-05}, {"id": 988, "seek": 695840, "start": 6966.719999999999, "end": 6973.92, "text": " the brain deals with every day are things we never have to think about because we do not need to plan", "tokens": [264, 3567, 11215, 365, 633, 786, 366, 721, 321, 1128, 362, 281, 519, 466, 570, 321, 360, 406, 643, 281, 1393], "temperature": 0.0, "avg_logprob": -0.1181375240457469, "compression_ratio": 1.65, "no_speech_prob": 8.121225982904434e-05}, {"id": 989, "seek": 695840, "start": 6973.92, "end": 6980.0, "text": " a reason about them. These are the millions of low-level problems we encountered in our", "tokens": [257, 1778, 466, 552, 13, 1981, 366, 264, 6803, 295, 2295, 12, 12418, 2740, 321, 20381, 294, 527], "temperature": 0.0, "avg_logprob": -0.1181375240457469, "compression_ratio": 1.65, "no_speech_prob": 8.121225982904434e-05}, {"id": 990, "seek": 695840, "start": 6980.0, "end": 6985.839999999999, "text": " mundane life every day, and this is the world that our AIs will have to operate in.", "tokens": [43497, 993, 633, 786, 11, 293, 341, 307, 264, 1002, 300, 527, 316, 6802, 486, 362, 281, 9651, 294, 13], "temperature": 0.0, "avg_logprob": -0.1181375240457469, "compression_ratio": 1.65, "no_speech_prob": 8.121225982904434e-05}, {"id": 991, "seek": 698584, "start": 6985.84, "end": 6993.28, "text": " Consider someone walking across the floor. Their brain signals their leg muscles to contract in", "tokens": [17416, 1580, 4494, 2108, 264, 4123, 13, 6710, 3567, 12354, 641, 1676, 9530, 281, 4364, 294], "temperature": 0.0, "avg_logprob": -0.11388963308089818, "compression_ratio": 1.6330275229357798, "no_speech_prob": 0.00017313288117293268}, {"id": 992, "seek": 698584, "start": 6993.28, "end": 7000.24, "text": " the correct cadence. Do they need to consciously plan each step? Do they reason about how to", "tokens": [264, 3006, 46109, 13, 1144, 436, 643, 281, 32538, 1393, 1184, 1823, 30, 1144, 436, 1778, 466, 577, 281], "temperature": 0.0, "avg_logprob": -0.11388963308089818, "compression_ratio": 1.6330275229357798, "no_speech_prob": 0.00017313288117293268}, {"id": 993, "seek": 698584, "start": 7000.24, "end": 7006.88, "text": " maintain their balance? No. They probably don't even know what leg muscles they have.", "tokens": [6909, 641, 4772, 30, 883, 13, 814, 1391, 500, 380, 754, 458, 437, 1676, 9530, 436, 362, 13], "temperature": 0.0, "avg_logprob": -0.11388963308089818, "compression_ratio": 1.6330275229357798, "no_speech_prob": 0.00017313288117293268}, {"id": 994, "seek": 698584, "start": 7007.6, "end": 7014.16, "text": " Consider understanding this sentence. Did you use reasoning? Did you use grammar?", "tokens": [17416, 3701, 341, 8174, 13, 2589, 291, 764, 21577, 30, 2589, 291, 764, 22317, 30], "temperature": 0.0, "avg_logprob": -0.11388963308089818, "compression_ratio": 1.6330275229357798, "no_speech_prob": 0.00017313288117293268}, {"id": 995, "seek": 701416, "start": 7014.16, "end": 7020.639999999999, "text": " If you are a fluent speaker, you do not need grammars to understand or produce language,", "tokens": [759, 291, 366, 257, 40799, 8145, 11, 291, 360, 406, 643, 17570, 685, 281, 1223, 420, 5258, 2856, 11], "temperature": 0.0, "avg_logprob": -0.10511573553085327, "compression_ratio": 1.5911111111111111, "no_speech_prob": 8.844213880365714e-05}, {"id": 996, "seek": 701416, "start": 7021.28, "end": 7028.4, "text": " and you do not have time to reason about language while hearing it spoken. Reasoning is slow,", "tokens": [293, 291, 360, 406, 362, 565, 281, 1778, 466, 2856, 1339, 4763, 309, 10759, 13, 39693, 278, 307, 2964, 11], "temperature": 0.0, "avg_logprob": -0.10511573553085327, "compression_ratio": 1.5911111111111111, "no_speech_prob": 8.844213880365714e-05}, {"id": 997, "seek": 701416, "start": 7028.4, "end": 7034.24, "text": " but understanding is instantaneous. Consider someone braking for a stoplight.", "tokens": [457, 3701, 307, 45596, 13, 17416, 1580, 32140, 337, 257, 1590, 2764, 13], "temperature": 0.0, "avg_logprob": -0.10511573553085327, "compression_ratio": 1.5911111111111111, "no_speech_prob": 8.844213880365714e-05}, {"id": 998, "seek": 701416, "start": 7034.96, "end": 7041.44, "text": " How hard should they push on the brake pedal? Do they compute the required differential equation?", "tokens": [1012, 1152, 820, 436, 2944, 322, 264, 13997, 19122, 30, 1144, 436, 14722, 264, 4739, 15756, 5367, 30], "temperature": 0.0, "avg_logprob": -0.10511573553085327, "compression_ratio": 1.5911111111111111, "no_speech_prob": 8.844213880365714e-05}, {"id": 999, "seek": 704144, "start": 7041.44, "end": 7045.839999999999, "text": " Should such equations be part of the driver's license test?", "tokens": [6454, 1270, 11787, 312, 644, 295, 264, 6787, 311, 10476, 1500, 30], "temperature": 0.0, "avg_logprob": -0.1180486297607422, "compression_ratio": 1.6291079812206573, "no_speech_prob": 3.801673301495612e-05}, {"id": 1000, "seek": 704144, "start": 7046.5599999999995, "end": 7053.5199999999995, "text": " Consider someone making breakfast. Did they have to reason about anything or plan anything,", "tokens": [17416, 1580, 1455, 8201, 13, 2589, 436, 362, 281, 1778, 466, 1340, 420, 1393, 1340, 11], "temperature": 0.0, "avg_logprob": -0.1180486297607422, "compression_ratio": 1.6291079812206573, "no_speech_prob": 3.801673301495612e-05}, {"id": 1001, "seek": 704144, "start": 7053.5199999999995, "end": 7060.24, "text": " or did they just do what worked yesterday, without thinking about it? Without consciously planning", "tokens": [420, 630, 436, 445, 360, 437, 2732, 5186, 11, 1553, 1953, 466, 309, 30, 9129, 32538, 5038], "temperature": 0.0, "avg_logprob": -0.1180486297607422, "compression_ratio": 1.6291079812206573, "no_speech_prob": 3.801673301495612e-05}, {"id": 1002, "seek": 704144, "start": 7060.24, "end": 7068.24, "text": " it? Walking and talking, braking and breakfasting, like almost everything we use our brains for,", "tokens": [309, 30, 26964, 293, 1417, 11, 32140, 293, 8201, 278, 11, 411, 1920, 1203, 321, 764, 527, 15442, 337, 11], "temperature": 0.0, "avg_logprob": -0.1180486297607422, "compression_ratio": 1.6291079812206573, "no_speech_prob": 3.801673301495612e-05}, {"id": 1003, "seek": 706824, "start": 7068.24, "end": 7074.48, "text": " rely on learning from our experiences in order to reuse anything that has worked in the past,", "tokens": [10687, 322, 2539, 490, 527, 5235, 294, 1668, 281, 26225, 1340, 300, 575, 2732, 294, 264, 1791, 11], "temperature": 0.0, "avg_logprob": -0.07942471446761165, "compression_ratio": 1.6106194690265487, "no_speech_prob": 3.9007336454233155e-05}, {"id": 1004, "seek": 706824, "start": 7075.12, "end": 7083.04, "text": " and, over time, we learn to correct our mistakes. These strategies are simple enough that we can", "tokens": [293, 11, 670, 565, 11, 321, 1466, 281, 3006, 527, 8038, 13, 1981, 9029, 366, 2199, 1547, 300, 321, 393], "temperature": 0.0, "avg_logprob": -0.07942471446761165, "compression_ratio": 1.6106194690265487, "no_speech_prob": 3.9007336454233155e-05}, {"id": 1005, "seek": 706824, "start": 7083.04, "end": 7089.36, "text": " identify them in other life forms. Dogs understand a lot but do not reason much,", "tokens": [5876, 552, 294, 661, 993, 6422, 13, 35504, 1223, 257, 688, 457, 360, 406, 1778, 709, 11], "temperature": 0.0, "avg_logprob": -0.07942471446761165, "compression_ratio": 1.6106194690265487, "no_speech_prob": 3.9007336454233155e-05}, {"id": 1006, "seek": 706824, "start": 7089.92, "end": 7096.24, "text": " and we can see how they could be implemented in something like neurons and brains. The split", "tokens": [293, 321, 393, 536, 577, 436, 727, 312, 12270, 294, 746, 411, 22027, 293, 15442, 13, 440, 7472], "temperature": 0.0, "avg_logprob": -0.07942471446761165, "compression_ratio": 1.6106194690265487, "no_speech_prob": 3.9007336454233155e-05}, {"id": 1007, "seek": 709624, "start": 7096.24, "end": 7102.96, "text": " in our brains between reasoning and understanding was examined at length in Thinking Fast and Slow", "tokens": [294, 527, 15442, 1296, 21577, 293, 3701, 390, 30972, 412, 4641, 294, 24460, 15968, 293, 17703], "temperature": 0.0, "avg_logprob": -0.09203665769552882, "compression_ratio": 1.5855855855855856, "no_speech_prob": 9.502498141955584e-05}, {"id": 1008, "seek": 709624, "start": 7102.96, "end": 7109.599999999999, "text": " by Daniel Kahneman. The absolute majority of the brain's effort is spent processing low-level", "tokens": [538, 8033, 591, 12140, 15023, 13, 440, 8236, 6286, 295, 264, 3567, 311, 4630, 307, 4418, 9007, 2295, 12, 12418], "temperature": 0.0, "avg_logprob": -0.09203665769552882, "compression_ratio": 1.5855855855855856, "no_speech_prob": 9.502498141955584e-05}, {"id": 1009, "seek": 709624, "start": 7109.599999999999, "end": 7117.76, "text": " sensory input, mostly from the eyes. He calls this System 1. It provides understanding.", "tokens": [27233, 4846, 11, 5240, 490, 264, 2575, 13, 634, 5498, 341, 8910, 502, 13, 467, 6417, 3701, 13], "temperature": 0.0, "avg_logprob": -0.09203665769552882, "compression_ratio": 1.5855855855855856, "no_speech_prob": 9.502498141955584e-05}, {"id": 1010, "seek": 709624, "start": 7118.4, "end": 7123.92, "text": " Reasoning is done by System 2 based on the understanding from System 1.", "tokens": [39693, 278, 307, 1096, 538, 8910, 568, 2361, 322, 264, 3701, 490, 8910, 502, 13], "temperature": 0.0, "avg_logprob": -0.09203665769552882, "compression_ratio": 1.5855855855855856, "no_speech_prob": 9.502498141955584e-05}, {"id": 1011, "seek": 712392, "start": 7123.92, "end": 7130.08, "text": " What most problems we deal with on a daily basis do not require System 2 at all.", "tokens": [708, 881, 2740, 321, 2028, 365, 322, 257, 5212, 5143, 360, 406, 3651, 8910, 568, 412, 439, 13], "temperature": 0.0, "avg_logprob": -0.09687798779185225, "compression_ratio": 1.506276150627615, "no_speech_prob": 5.7582012232160196e-05}, {"id": 1012, "seek": 712392, "start": 7130.72, "end": 7136.32, "text": " Artificial intelligence and machine learning computers can solve any suitable problem when", "tokens": [5735, 10371, 7599, 293, 3479, 2539, 10807, 393, 5039, 604, 12873, 1154, 562], "temperature": 0.0, "avg_logprob": -0.09687798779185225, "compression_ratio": 1.506276150627615, "no_speech_prob": 5.7582012232160196e-05}, {"id": 1013, "seek": 712392, "start": 7136.32, "end": 7143.04, "text": " given sufficient human help, such as a complete plan for the solution in the form of a computer", "tokens": [2212, 11563, 1952, 854, 11, 1270, 382, 257, 3566, 1393, 337, 264, 3827, 294, 264, 1254, 295, 257, 3820], "temperature": 0.0, "avg_logprob": -0.09687798779185225, "compression_ratio": 1.506276150627615, "no_speech_prob": 5.7582012232160196e-05}, {"id": 1014, "seek": 712392, "start": 7143.04, "end": 7151.4400000000005, "text": " program and valid input data. But since the AIML Revolution of 2012, we now know how to make", "tokens": [1461, 293, 7363, 4846, 1412, 13, 583, 1670, 264, 316, 6324, 43, 16617, 295, 9125, 11, 321, 586, 458, 577, 281, 652], "temperature": 0.0, "avg_logprob": -0.09687798779185225, "compression_ratio": 1.506276150627615, "no_speech_prob": 5.7582012232160196e-05}, {"id": 1015, "seek": 715144, "start": 7151.44, "end": 7158.24, "text": " computers understand certain problem domains through machine learning. The acquired understanding", "tokens": [10807, 1223, 1629, 1154, 25514, 807, 3479, 2539, 13, 440, 17554, 3701], "temperature": 0.0, "avg_logprob": -0.0572776993115743, "compression_ratio": 1.5869565217391304, "no_speech_prob": 2.7589367164182477e-05}, {"id": 1016, "seek": 715144, "start": 7158.24, "end": 7165.919999999999, "text": " allows the machine to just do it for many different problems in the domain, without any human planning,", "tokens": [4045, 264, 3479, 281, 445, 360, 309, 337, 867, 819, 2740, 294, 264, 9274, 11, 1553, 604, 1952, 5038, 11], "temperature": 0.0, "avg_logprob": -0.0572776993115743, "compression_ratio": 1.5869565217391304, "no_speech_prob": 2.7589367164182477e-05}, {"id": 1017, "seek": 715144, "start": 7165.919999999999, "end": 7174.32, "text": " reasoning, or programming, and using incomplete, unreliable, and noisy input data. This is", "tokens": [21577, 11, 420, 9410, 11, 293, 1228, 31709, 11, 20584, 2081, 712, 11, 293, 24518, 4846, 1412, 13, 639, 307], "temperature": 0.0, "avg_logprob": -0.0572776993115743, "compression_ratio": 1.5869565217391304, "no_speech_prob": 2.7589367164182477e-05}, {"id": 1018, "seek": 717432, "start": 7174.32, "end": 7182.0, "text": " changing how we are building systems with cognitive capabilities. Everyone working in ML or AI needs", "tokens": [4473, 577, 321, 366, 2390, 3652, 365, 15605, 10862, 13, 5198, 1364, 294, 21601, 420, 7318, 2203], "temperature": 0.0, "avg_logprob": -0.0828066905339559, "compression_ratio": 1.4517766497461928, "no_speech_prob": 4.24432655563578e-05}, {"id": 1019, "seek": 717432, "start": 7182.0, "end": 7188.88, "text": " to understand the trade-offs we must make at the most fundamental, epistemological levels.", "tokens": [281, 1223, 264, 4923, 12, 19231, 321, 1633, 652, 412, 264, 881, 8088, 11, 2388, 43958, 4383, 4358, 13], "temperature": 0.0, "avg_logprob": -0.0828066905339559, "compression_ratio": 1.4517766497461928, "no_speech_prob": 4.24432655563578e-05}, {"id": 1020, "seek": 717432, "start": 7189.5199999999995, "end": 7195.759999999999, "text": " Modern ML requires examining and seriously rethinking many things we were taught to vigilantly", "tokens": [19814, 21601, 7029, 34662, 293, 6638, 319, 39873, 867, 721, 321, 645, 5928, 281, 39093, 3627], "temperature": 0.0, "avg_logprob": -0.0828066905339559, "compression_ratio": 1.4517766497461928, "no_speech_prob": 4.24432655563578e-05}, {"id": 1021, "seek": 719576, "start": 7195.76, "end": 7205.04, "text": " strive for in our science, technology, engineering, and mathematics STEM educations. Things like", "tokens": [23829, 337, 294, 527, 3497, 11, 2899, 11, 7043, 11, 293, 18666, 25043, 2400, 763, 13, 9514, 411], "temperature": 0.0, "avg_logprob": -0.0773643639128087, "compression_ratio": 1.455497382198953, "no_speech_prob": 7.748866482870653e-05}, {"id": 1022, "seek": 719576, "start": 7205.04, "end": 7212.400000000001, "text": " correlation is bad, but causality is good and do not jump to conclusions on scant evidence", "tokens": [20009, 307, 1578, 11, 457, 3302, 1860, 307, 665, 293, 360, 406, 3012, 281, 22865, 322, 795, 394, 4467], "temperature": 0.0, "avg_logprob": -0.0773643639128087, "compression_ratio": 1.455497382198953, "no_speech_prob": 7.748866482870653e-05}, {"id": 1023, "seek": 719576, "start": 7212.400000000001, "end": 7219.2, "text": " are still solid advice everywhere inside science. But when building understanding systems,", "tokens": [366, 920, 5100, 5192, 5315, 1854, 3497, 13, 583, 562, 2390, 3701, 3652, 11], "temperature": 0.0, "avg_logprob": -0.0773643639128087, "compression_ratio": 1.455497382198953, "no_speech_prob": 7.748866482870653e-05}, {"id": 1024, "seek": 721920, "start": 7219.2, "end": 7226.0, "text": " these established strategies and modes of thinking no longer work, because correlation discovery and", "tokens": [613, 7545, 9029, 293, 14068, 295, 1953, 572, 2854, 589, 11, 570, 20009, 12114, 293], "temperature": 0.0, "avg_logprob": -0.09512388706207275, "compression_ratio": 1.5226130653266332, "no_speech_prob": 6.110336107667536e-05}, {"id": 1025, "seek": 721920, "start": 7226.0, "end": 7233.5199999999995, "text": " handling of sparse, unreliable, and inconsistent input data are exactly the kinds of tasks we will", "tokens": [13175, 295, 637, 11668, 11, 20584, 2081, 712, 11, 293, 36891, 4846, 1412, 366, 2293, 264, 3685, 295, 9608, 321, 486], "temperature": 0.0, "avg_logprob": -0.09512388706207275, "compression_ratio": 1.5226130653266332, "no_speech_prob": 6.110336107667536e-05}, {"id": 1026, "seek": 721920, "start": 7233.5199999999995, "end": 7242.24, "text": " have to perform and perform well at these pre-scientific levels. In order to understand how to do this,", "tokens": [362, 281, 2042, 293, 2042, 731, 412, 613, 659, 12, 82, 5412, 1089, 4358, 13, 682, 1668, 281, 1223, 577, 281, 360, 341, 11], "temperature": 0.0, "avg_logprob": -0.09512388706207275, "compression_ratio": 1.5226130653266332, "no_speech_prob": 6.110336107667536e-05}, {"id": 1027, "seek": 724224, "start": 7242.24, "end": 7249.599999999999, "text": " we must switch to a holistic stance. A motivating example, beginning machine learning students", "tokens": [321, 1633, 3679, 281, 257, 30334, 21033, 13, 316, 41066, 1365, 11, 2863, 3479, 2539, 1731], "temperature": 0.0, "avg_logprob": -0.0867980991501406, "compression_ratio": 1.5864978902953586, "no_speech_prob": 8.335769234690815e-05}, {"id": 1028, "seek": 724224, "start": 7249.599999999999, "end": 7256.719999999999, "text": " are given exercises like this. They are given a large spreadsheet, which lists data about houses", "tokens": [366, 2212, 11900, 411, 341, 13, 814, 366, 2212, 257, 2416, 27733, 11, 597, 14511, 1412, 466, 8078], "temperature": 0.0, "avg_logprob": -0.0867980991501406, "compression_ratio": 1.5864978902953586, "no_speech_prob": 8.335769234690815e-05}, {"id": 1029, "seek": 724224, "start": 7256.719999999999, "end": 7263.599999999999, "text": " sold a certain year in the US. This information includes among other things, the zip code of the", "tokens": [3718, 257, 1629, 1064, 294, 264, 2546, 13, 639, 1589, 5974, 3654, 661, 721, 11, 264, 20730, 3089, 295, 264], "temperature": 0.0, "avg_logprob": -0.0867980991501406, "compression_ratio": 1.5864978902953586, "no_speech_prob": 8.335769234690815e-05}, {"id": 1030, "seek": 724224, "start": 7263.599999999999, "end": 7270.719999999999, "text": " house, the living area and square feet, lot size, the number of bedrooms and bathrooms,", "tokens": [1782, 11, 264, 2647, 1859, 293, 3732, 3521, 11, 688, 2744, 11, 264, 1230, 295, 39955, 293, 39537, 11], "temperature": 0.0, "avg_logprob": -0.0867980991501406, "compression_ratio": 1.5864978902953586, "no_speech_prob": 8.335769234690815e-05}, {"id": 1031, "seek": 727072, "start": 7270.72, "end": 7277.6, "text": " the year the house was built, and the final sale price of the house. We would like to be able to", "tokens": [264, 1064, 264, 1782, 390, 3094, 11, 293, 264, 2572, 8680, 3218, 295, 264, 1782, 13, 492, 576, 411, 281, 312, 1075, 281], "temperature": 0.0, "avg_logprob": -0.07391761107878252, "compression_ratio": 1.7013574660633484, "no_speech_prob": 5.6135057093342766e-05}, {"id": 1032, "seek": 727072, "start": 7277.6, "end": 7284.56, "text": " predict this final sale price, given the corresponding data for current house we are about to list for", "tokens": [6069, 341, 2572, 8680, 3218, 11, 2212, 264, 11760, 1412, 337, 2190, 1782, 321, 366, 466, 281, 1329, 337], "temperature": 0.0, "avg_logprob": -0.07391761107878252, "compression_ratio": 1.7013574660633484, "no_speech_prob": 5.6135057093342766e-05}, {"id": 1033, "seek": 727072, "start": 7284.56, "end": 7290.96, "text": " sale. The given spreadsheet is the data the student will use to train a deep neural network.", "tokens": [8680, 13, 440, 2212, 27733, 307, 264, 1412, 264, 3107, 486, 764, 281, 3847, 257, 2452, 18161, 3209, 13], "temperature": 0.0, "avg_logprob": -0.07391761107878252, "compression_ratio": 1.7013574660633484, "no_speech_prob": 5.6135057093342766e-05}, {"id": 1034, "seek": 727072, "start": 7291.52, "end": 7297.2, "text": " It is the entire learning corpus. It contains everything the system will ever know.", "tokens": [467, 307, 264, 2302, 2539, 1181, 31624, 13, 467, 8306, 1203, 264, 1185, 486, 1562, 458, 13], "temperature": 0.0, "avg_logprob": -0.07391761107878252, "compression_ratio": 1.7013574660633484, "no_speech_prob": 5.6135057093342766e-05}, {"id": 1035, "seek": 729720, "start": 7297.2, "end": 7305.44, "text": " These students can download deep neural network libraries like Keras and TensorFlow and runnable", "tokens": [1981, 1731, 393, 5484, 2452, 18161, 3209, 15148, 411, 591, 6985, 293, 37624, 293, 1190, 77, 712], "temperature": 0.0, "avg_logprob": -0.15695999562740326, "compression_ratio": 1.5, "no_speech_prob": 6.215113535290584e-05}, {"id": 1036, "seek": 729720, "start": 7305.44, "end": 7313.28, "text": " examples for many kinds of problems, including useful training data from places like Hugging Face", "tokens": [5110, 337, 867, 3685, 295, 2740, 11, 3009, 4420, 3097, 1412, 490, 3190, 411, 46892, 3249, 4047], "temperature": 0.0, "avg_logprob": -0.15695999562740326, "compression_ratio": 1.5, "no_speech_prob": 6.215113535290584e-05}, {"id": 1037, "seek": 729720, "start": 7313.28, "end": 7321.599999999999, "text": " and GitHub. Next the student trains, learns their network using the given data. This may take a while,", "tokens": [293, 23331, 13, 3087, 264, 3107, 16329, 11, 27152, 641, 3209, 1228, 264, 2212, 1412, 13, 639, 815, 747, 257, 1339, 11], "temperature": 0.0, "avg_logprob": -0.15695999562740326, "compression_ratio": 1.5, "no_speech_prob": 6.215113535290584e-05}, {"id": 1038, "seek": 732160, "start": 7321.6, "end": 7328.400000000001, "text": " but when learning finishes, they can give the system data for a house it has never seen and", "tokens": [457, 562, 2539, 23615, 11, 436, 393, 976, 264, 1185, 1412, 337, 257, 1782, 309, 575, 1128, 1612, 293], "temperature": 0.0, "avg_logprob": -0.0841416311852726, "compression_ratio": 1.7066666666666668, "no_speech_prob": 3.191108407918364e-05}, {"id": 1039, "seek": 732160, "start": 7328.400000000001, "end": 7335.04, "text": " it will quite reliably predict what the house might sell for. This was the goal of the exercise.", "tokens": [309, 486, 1596, 49927, 6069, 437, 264, 1782, 1062, 3607, 337, 13, 639, 390, 264, 3387, 295, 264, 5380, 13], "temperature": 0.0, "avg_logprob": -0.0841416311852726, "compression_ratio": 1.7066666666666668, "no_speech_prob": 3.191108407918364e-05}, {"id": 1040, "seek": 732160, "start": 7335.68, "end": 7342.240000000001, "text": " The student has created a system that understands how to estimate real estate prices from listings,", "tokens": [440, 3107, 575, 2942, 257, 1185, 300, 15146, 577, 281, 12539, 957, 9749, 7901, 490, 45615, 11], "temperature": 0.0, "avg_logprob": -0.0841416311852726, "compression_ratio": 1.7066666666666668, "no_speech_prob": 3.191108407918364e-05}, {"id": 1041, "seek": 732160, "start": 7342.88, "end": 7349.68, "text": " but the student still does not understand anything about real estate. The predictive capability", "tokens": [457, 264, 3107, 920, 775, 406, 1223, 1340, 466, 957, 9749, 13, 440, 35521, 13759], "temperature": 0.0, "avg_logprob": -0.0841416311852726, "compression_ratio": 1.7066666666666668, "no_speech_prob": 3.191108407918364e-05}, {"id": 1042, "seek": 734968, "start": 7349.68, "end": 7356.16, "text": " that many people working in real estate would be willing to pay money for is 100% based on", "tokens": [300, 867, 561, 1364, 294, 957, 9749, 576, 312, 4950, 281, 1689, 1460, 337, 307, 2319, 4, 2361, 322], "temperature": 0.0, "avg_logprob": -0.0953821248786394, "compression_ratio": 1.620253164556962, "no_speech_prob": 2.5248647943953983e-05}, {"id": 1043, "seek": 734968, "start": 7356.16, "end": 7362.88, "text": " understanding in the deep neural network, in the computer, and because all the libraries in many", "tokens": [3701, 294, 264, 2452, 18161, 3209, 11, 294, 264, 3820, 11, 293, 570, 439, 264, 15148, 294, 867], "temperature": 0.0, "avg_logprob": -0.0953821248786394, "compression_ratio": 1.620253164556962, "no_speech_prob": 2.5248647943953983e-05}, {"id": 1044, "seek": 734968, "start": 7362.88, "end": 7369.52, "text": " pre-sold examples of this nature were freely available, the student did not have to do much", "tokens": [659, 12, 45537, 5110, 295, 341, 3687, 645, 16433, 2435, 11, 264, 3107, 630, 406, 362, 281, 360, 709], "temperature": 0.0, "avg_logprob": -0.0953821248786394, "compression_ratio": 1.620253164556962, "no_speech_prob": 2.5248647943953983e-05}, {"id": 1045, "seek": 734968, "start": 7369.52, "end": 7378.88, "text": " programming either. The vision. This is desirable. This is what AI should mean. The computer understands", "tokens": [9410, 2139, 13, 440, 5201, 13, 639, 307, 30533, 13, 639, 307, 437, 7318, 820, 914, 13, 440, 3820, 15146], "temperature": 0.0, "avg_logprob": -0.0953821248786394, "compression_ratio": 1.620253164556962, "no_speech_prob": 2.5248647943953983e-05}, {"id": 1046, "seek": 737888, "start": 7378.88, "end": 7385.84, "text": " the problem so that we don't have to. Programming in the future will be like having a conversation", "tokens": [264, 1154, 370, 300, 321, 500, 380, 362, 281, 13, 8338, 2810, 294, 264, 2027, 486, 312, 411, 1419, 257, 3761], "temperature": 0.0, "avg_logprob": -0.07215789730629224, "compression_ratio": 1.6212765957446809, "no_speech_prob": 5.2842286095255986e-05}, {"id": 1047, "seek": 737888, "start": 7385.84, "end": 7392.400000000001, "text": " with a competent coworker, and when the machine understands exactly what we want done, it will", "tokens": [365, 257, 29998, 31998, 260, 11, 293, 562, 264, 3479, 15146, 2293, 437, 321, 528, 1096, 11, 309, 486], "temperature": 0.0, "avg_logprob": -0.07215789730629224, "compression_ratio": 1.6212765957446809, "no_speech_prob": 5.2842286095255986e-05}, {"id": 1048, "seek": 737888, "start": 7392.400000000001, "end": 7400.08, "text": " simply do it. No programming required on our part or on part of the machine, once a suitable,", "tokens": [2935, 360, 309, 13, 883, 9410, 4739, 322, 527, 644, 420, 322, 644, 295, 264, 3479, 11, 1564, 257, 12873, 11], "temperature": 0.0, "avg_logprob": -0.07215789730629224, "compression_ratio": 1.6212765957446809, "no_speech_prob": 5.2842286095255986e-05}, {"id": 1049, "seek": 737888, "start": 7400.08, "end": 7407.36, "text": " partially reductionist framework exists. The rest is learning and it can be done in any human", "tokens": [18886, 11004, 468, 8388, 8198, 13, 440, 1472, 307, 2539, 293, 309, 393, 312, 1096, 294, 604, 1952], "temperature": 0.0, "avg_logprob": -0.07215789730629224, "compression_ratio": 1.6212765957446809, "no_speech_prob": 5.2842286095255986e-05}, {"id": 1050, "seek": 740736, "start": 7407.36, "end": 7414.08, "text": " language with equal ease. We are on the right track towards something worthy of the name AI", "tokens": [2856, 365, 2681, 12708, 13, 492, 366, 322, 264, 558, 2837, 3030, 746, 14829, 295, 264, 1315, 7318], "temperature": 0.0, "avg_logprob": -0.04797894696155226, "compression_ratio": 1.5964912280701755, "no_speech_prob": 7.859801553422585e-05}, {"id": 1051, "seek": 740736, "start": 7414.08, "end": 7420.96, "text": " with current machine learning. Going forward, there are thousands of paths to choose from,", "tokens": [365, 2190, 3479, 2539, 13, 10963, 2128, 11, 456, 366, 5383, 295, 14518, 281, 2826, 490, 11], "temperature": 0.0, "avg_logprob": -0.04797894696155226, "compression_ratio": 1.5964912280701755, "no_speech_prob": 7.859801553422585e-05}, {"id": 1052, "seek": 740736, "start": 7420.96, "end": 7427.36, "text": " and the ability to choose wisely will depend on our ability to understand and adopt a holistic", "tokens": [293, 264, 3485, 281, 2826, 37632, 486, 5672, 322, 527, 3485, 281, 1223, 293, 6878, 257, 30334], "temperature": 0.0, "avg_logprob": -0.04797894696155226, "compression_ratio": 1.5964912280701755, "no_speech_prob": 7.859801553422585e-05}, {"id": 1053, "seek": 740736, "start": 7427.36, "end": 7434.48, "text": " stance. Reductionism and Holism. These are important terms of the art in epistemology.", "tokens": [21033, 13, 4477, 27549, 1434, 293, 11086, 1434, 13, 1981, 366, 1021, 2115, 295, 264, 1523, 294, 2388, 43958, 1793, 13], "temperature": 0.0, "avg_logprob": -0.04797894696155226, "compression_ratio": 1.5964912280701755, "no_speech_prob": 7.859801553422585e-05}, {"id": 1054, "seek": 743448, "start": 7434.48, "end": 7442.32, "text": " Both of them have numerous correct, useful, and compatible definitions. We will henceforth", "tokens": [6767, 295, 552, 362, 12546, 3006, 11, 4420, 11, 293, 18218, 21988, 13, 492, 486, 16678, 44779], "temperature": 0.0, "avg_logprob": -0.1174188467172476, "compression_ratio": 1.569060773480663, "no_speech_prob": 0.00011149618512718007}, {"id": 1055, "seek": 743448, "start": 7442.32, "end": 7449.36, "text": " use the following definitions for reasons of usefulness and simplicity. Reductionism is the", "tokens": [764, 264, 3480, 21988, 337, 4112, 295, 4420, 1287, 293, 25632, 13, 4477, 27549, 1434, 307, 264], "temperature": 0.0, "avg_logprob": -0.1174188467172476, "compression_ratio": 1.569060773480663, "no_speech_prob": 0.00011149618512718007}, {"id": 1056, "seek": 743448, "start": 7449.36, "end": 7459.04, "text": " use of models. Holism is the avoidance of models. Models are scientific models, theories, hypotheses,", "tokens": [764, 295, 5245, 13, 11086, 1434, 307, 264, 5042, 719, 295, 5245, 13, 6583, 1625, 366, 8134, 5245, 11, 13667, 11, 49969, 11], "temperature": 0.0, "avg_logprob": -0.1174188467172476, "compression_ratio": 1.569060773480663, "no_speech_prob": 0.00011149618512718007}, {"id": 1057, "seek": 745904, "start": 7459.04, "end": 7467.04, "text": " formulas, equations, naive models based on personal experiences, superstitions, if you can", "tokens": [30546, 11, 11787, 11, 29052, 5245, 2361, 322, 2973, 5235, 11, 29423, 2451, 11, 498, 291, 393], "temperature": 0.0, "avg_logprob": -0.08244950430733818, "compression_ratio": 1.5714285714285714, "no_speech_prob": 8.282473572762683e-05}, {"id": 1058, "seek": 745904, "start": 7467.04, "end": 7474.64, "text": " believe that, and traditional computer programs. In the reductionist paradigm, these models are", "tokens": [1697, 300, 11, 293, 5164, 3820, 4268, 13, 682, 264, 11004, 468, 24709, 11, 613, 5245, 366], "temperature": 0.0, "avg_logprob": -0.08244950430733818, "compression_ratio": 1.5714285714285714, "no_speech_prob": 8.282473572762683e-05}, {"id": 1059, "seek": 745904, "start": 7474.64, "end": 7482.8, "text": " created by humans, ostensibly by scientists, and are then used, ostensibly by engineers,", "tokens": [2942, 538, 6255, 11, 32946, 694, 3545, 538, 7708, 11, 293, 366, 550, 1143, 11, 32946, 694, 3545, 538, 11955, 11], "temperature": 0.0, "avg_logprob": -0.08244950430733818, "compression_ratio": 1.5714285714285714, "no_speech_prob": 8.282473572762683e-05}, {"id": 1060, "seek": 748280, "start": 7482.8, "end": 7489.6, "text": " to solve real world problems. Model creation and model use both require that these humans", "tokens": [281, 5039, 957, 1002, 2740, 13, 17105, 8016, 293, 2316, 764, 1293, 3651, 300, 613, 6255], "temperature": 0.0, "avg_logprob": -0.06388627347492036, "compression_ratio": 1.676991150442478, "no_speech_prob": 5.19422537763603e-05}, {"id": 1061, "seek": 748280, "start": 7489.6, "end": 7497.12, "text": " understand the problem domain, the problem at hand, the previously known shared models available,", "tokens": [1223, 264, 1154, 9274, 11, 264, 1154, 412, 1011, 11, 264, 8046, 2570, 5507, 5245, 2435, 11], "temperature": 0.0, "avg_logprob": -0.06388627347492036, "compression_ratio": 1.676991150442478, "no_speech_prob": 5.19422537763603e-05}, {"id": 1062, "seek": 748280, "start": 7497.12, "end": 7504.400000000001, "text": " and how to design and use models. A PhD degree could be seen as a formal license to create new", "tokens": [293, 577, 281, 1715, 293, 764, 5245, 13, 316, 14476, 4314, 727, 312, 1612, 382, 257, 9860, 10476, 281, 1884, 777], "temperature": 0.0, "avg_logprob": -0.06388627347492036, "compression_ratio": 1.676991150442478, "no_speech_prob": 5.19422537763603e-05}, {"id": 1063, "seek": 748280, "start": 7504.400000000001, "end": 7512.72, "text": " models. Mathematics can be seen as a discipline for model manipulation. But now, by avoiding the", "tokens": [5245, 13, 15776, 37541, 393, 312, 1612, 382, 257, 13635, 337, 2316, 26475, 13, 583, 586, 11, 538, 20220, 264], "temperature": 0.0, "avg_logprob": -0.06388627347492036, "compression_ratio": 1.676991150442478, "no_speech_prob": 5.19422537763603e-05}, {"id": 1064, "seek": 751272, "start": 7512.72, "end": 7520.56, "text": " use of human-made models and switching to holistic methods, data scientists, programmers, and others", "tokens": [764, 295, 1952, 12, 10341, 5245, 293, 16493, 281, 30334, 7150, 11, 1412, 7708, 11, 41504, 11, 293, 2357], "temperature": 0.0, "avg_logprob": -0.06552025250026158, "compression_ratio": 1.6569037656903767, "no_speech_prob": 4.766263737110421e-05}, {"id": 1065, "seek": 751272, "start": 7520.56, "end": 7528.08, "text": " do not themselves have to understand the problems they are given. They are no longer asked to provide", "tokens": [360, 406, 2969, 362, 281, 1223, 264, 2740, 436, 366, 2212, 13, 814, 366, 572, 2854, 2351, 281, 2893], "temperature": 0.0, "avg_logprob": -0.06552025250026158, "compression_ratio": 1.6569037656903767, "no_speech_prob": 4.766263737110421e-05}, {"id": 1066, "seek": 751272, "start": 7528.08, "end": 7534.72, "text": " a computer program or to otherwise solve a problem in a traditional reductionist or scientific way.", "tokens": [257, 3820, 1461, 420, 281, 5911, 5039, 257, 1154, 294, 257, 5164, 11004, 468, 420, 8134, 636, 13], "temperature": 0.0, "avg_logprob": -0.06552025250026158, "compression_ratio": 1.6569037656903767, "no_speech_prob": 4.766263737110421e-05}, {"id": 1067, "seek": 751272, "start": 7535.4400000000005, "end": 7542.0, "text": " Holistic systems like DNNs can provide solutions to many problems by first learning about the", "tokens": [11086, 3142, 3652, 411, 21500, 45, 82, 393, 2893, 6547, 281, 867, 2740, 538, 700, 2539, 466, 264], "temperature": 0.0, "avg_logprob": -0.06552025250026158, "compression_ratio": 1.6569037656903767, "no_speech_prob": 4.766263737110421e-05}, {"id": 1068, "seek": 754200, "start": 7542.0, "end": 7549.36, "text": " domain from data-insult examples, and then, in production, to match new situations to this", "tokens": [9274, 490, 1412, 12, 1292, 723, 5110, 11, 293, 550, 11, 294, 4265, 11, 281, 2995, 777, 6851, 281, 341], "temperature": 0.0, "avg_logprob": -0.06032719263216344, "compression_ratio": 1.5564516129032258, "no_speech_prob": 5.858342774445191e-05}, {"id": 1069, "seek": 754200, "start": 7549.36, "end": 7556.0, "text": " gathered experience. These matches are guesses, but with sufficient learning, the results can be", "tokens": [13032, 1752, 13, 1981, 10676, 366, 42703, 11, 457, 365, 11563, 2539, 11, 264, 3542, 393, 312], "temperature": 0.0, "avg_logprob": -0.06032719263216344, "compression_ratio": 1.5564516129032258, "no_speech_prob": 5.858342774445191e-05}, {"id": 1070, "seek": 754200, "start": 7556.0, "end": 7563.44, "text": " highly reliable. We will initially use computer-based holistic methods to solve individual and specific", "tokens": [5405, 12924, 13, 492, 486, 9105, 764, 3820, 12, 6032, 30334, 7150, 281, 5039, 2609, 293, 2685], "temperature": 0.0, "avg_logprob": -0.06032719263216344, "compression_ratio": 1.5564516129032258, "no_speech_prob": 5.858342774445191e-05}, {"id": 1071, "seek": 754200, "start": 7563.44, "end": 7571.52, "text": " problems, such as self-driving cars. Over time, increasing numbers of artificial understanders", "tokens": [2740, 11, 1270, 382, 2698, 12, 47094, 5163, 13, 4886, 565, 11, 5662, 3547, 295, 11677, 1223, 433], "temperature": 0.0, "avg_logprob": -0.06032719263216344, "compression_ratio": 1.5564516129032258, "no_speech_prob": 5.858342774445191e-05}, {"id": 1072, "seek": 757152, "start": 7571.52, "end": 7578.320000000001, "text": " will be able to provide immediate answers, guesses, to wider and wider ranges of problems.", "tokens": [486, 312, 1075, 281, 2893, 11629, 6338, 11, 42703, 11, 281, 11842, 293, 11842, 22526, 295, 2740, 13], "temperature": 0.0, "avg_logprob": -0.05982363791692825, "compression_ratio": 1.5720338983050848, "no_speech_prob": 5.429769225884229e-05}, {"id": 1073, "seek": 757152, "start": 7579.040000000001, "end": 7584.72, "text": " We can expect to see cell phone apps with such good command of language that it feels like", "tokens": [492, 393, 2066, 281, 536, 2815, 2593, 7733, 365, 1270, 665, 5622, 295, 2856, 300, 309, 3417, 411], "temperature": 0.0, "avg_logprob": -0.05982363791692825, "compression_ratio": 1.5720338983050848, "no_speech_prob": 5.429769225884229e-05}, {"id": 1074, "seek": 757152, "start": 7584.72, "end": 7591.120000000001, "text": " talking to a competent co-worker. Voice will become the preferred way to interact with our", "tokens": [1417, 281, 257, 29998, 598, 12, 49402, 13, 15229, 486, 1813, 264, 16494, 636, 281, 4648, 365, 527], "temperature": 0.0, "avg_logprob": -0.05982363791692825, "compression_ratio": 1.5720338983050848, "no_speech_prob": 5.429769225884229e-05}, {"id": 1075, "seek": 757152, "start": 7591.120000000001, "end": 7598.88, "text": " personal AIs. Early and low-level but useful AI will manifest as computers that can solve problems", "tokens": [2973, 316, 6802, 13, 18344, 293, 2295, 12, 12418, 457, 4420, 7318, 486, 10067, 382, 10807, 300, 393, 5039, 2740], "temperature": 0.0, "avg_logprob": -0.05982363791692825, "compression_ratio": 1.5720338983050848, "no_speech_prob": 5.429769225884229e-05}, {"id": 1076, "seek": 759888, "start": 7598.88, "end": 7605.52, "text": " we ourselves cannot or cannot be bothered to solve. They need not be superhuman.", "tokens": [321, 4175, 2644, 420, 2644, 312, 22996, 281, 5039, 13, 814, 643, 406, 312, 1687, 18796, 13], "temperature": 0.0, "avg_logprob": -0.06140173217396677, "compression_ratio": 1.575221238938053, "no_speech_prob": 5.934211367275566e-05}, {"id": 1077, "seek": 759888, "start": 7606.08, "end": 7612.400000000001, "text": " All they need to have in order to be extremely useful is exactly the ability to autonomously", "tokens": [1057, 436, 643, 281, 362, 294, 1668, 281, 312, 4664, 4420, 307, 2293, 264, 3485, 281, 18203, 5098], "temperature": 0.0, "avg_logprob": -0.06140173217396677, "compression_ratio": 1.575221238938053, "no_speech_prob": 5.934211367275566e-05}, {"id": 1078, "seek": 759888, "start": 7612.400000000001, "end": 7619.04, "text": " discover higher-level abstractions in some given problem domain, starting from low-level sensory", "tokens": [4411, 2946, 12, 12418, 12649, 626, 294, 512, 2212, 1154, 9274, 11, 2891, 490, 2295, 12, 12418, 27233], "temperature": 0.0, "avg_logprob": -0.06140173217396677, "compression_ratio": 1.575221238938053, "no_speech_prob": 5.934211367275566e-05}, {"id": 1079, "seek": 759888, "start": 7619.04, "end": 7626.32, "text": " input, for example, by learning from images or reading books. Such systems now exist.", "tokens": [4846, 11, 337, 1365, 11, 538, 2539, 490, 5267, 420, 3760, 3642, 13, 9653, 3652, 586, 2514, 13], "temperature": 0.0, "avg_logprob": -0.06140173217396677, "compression_ratio": 1.575221238938053, "no_speech_prob": 5.934211367275566e-05}, {"id": 1080, "seek": 762632, "start": 7626.32, "end": 7632.5599999999995, "text": " If we want to understand machine learning, then we need to understand all the strategies in the", "tokens": [759, 321, 528, 281, 1223, 3479, 2539, 11, 550, 321, 643, 281, 1223, 439, 264, 9029, 294, 264], "temperature": 0.0, "avg_logprob": -0.18779026943704355, "compression_ratio": 1.6858407079646018, "no_speech_prob": 0.00010616217332426459}, {"id": 1081, "seek": 762632, "start": 7632.5599999999995, "end": 7639.04, "text": " right most column in the tables that follow. They are all part of a holistic stance, and if we are", "tokens": [558, 881, 7738, 294, 264, 8020, 300, 1524, 13, 814, 366, 439, 644, 295, 257, 30334, 21033, 11, 293, 498, 321, 366], "temperature": 0.0, "avg_logprob": -0.18779026943704355, "compression_ratio": 1.6858407079646018, "no_speech_prob": 0.00010616217332426459}, {"id": 1082, "seek": 762632, "start": 7639.04, "end": 7646.0, "text": " working in machine learning, we need to adopt as many of them as possible. Differences at the level", "tokens": [1364, 294, 3479, 2539, 11, 321, 643, 281, 6878, 382, 867, 295, 552, 382, 1944, 13, 413, 12612, 2667, 412, 264, 1496], "temperature": 0.0, "avg_logprob": -0.18779026943704355, "compression_ratio": 1.6858407079646018, "no_speech_prob": 0.00010616217332426459}, {"id": 1083, "seek": 762632, "start": 7646.0, "end": 7655.599999999999, "text": " of epistemology. Reductionism in Science versus Holism in Machine Learning. The use of", "tokens": [295, 2388, 43958, 1793, 13, 4477, 27549, 1434, 294, 8976, 5717, 11086, 1434, 294, 22155, 15205, 13, 440, 764, 295], "temperature": 0.0, "avg_logprob": -0.18779026943704355, "compression_ratio": 1.6858407079646018, "no_speech_prob": 0.00010616217332426459}, {"id": 1084, "seek": 765560, "start": 7655.6, "end": 7666.320000000001, "text": " models versus the avoidance of models. Raising versus understanding requires human understanding", "tokens": [5245, 5717, 264, 5042, 719, 295, 5245, 13, 43374, 278, 5717, 3701, 7029, 1952, 3701], "temperature": 0.0, "avg_logprob": -0.1754241841179984, "compression_ratio": 1.7239263803680982, "no_speech_prob": 5.0294540415052325e-05}, {"id": 1085, "seek": 765560, "start": 7666.320000000001, "end": 7674.8, "text": " versus provides human-like understanding. Problems are solved in an abstract model space versus", "tokens": [5717, 6417, 1952, 12, 4092, 3701, 13, 11676, 82, 366, 13041, 294, 364, 12649, 2316, 1901, 5717], "temperature": 0.0, "avg_logprob": -0.1754241841179984, "compression_ratio": 1.7239263803680982, "no_speech_prob": 5.0294540415052325e-05}, {"id": 1086, "seek": 765560, "start": 7674.8, "end": 7681.52, "text": " problems are solved directly in the problem domain. Unbeatable strategy for dealing with", "tokens": [2740, 366, 13041, 3838, 294, 264, 1154, 9274, 13, 1156, 4169, 712, 5206, 337, 6260, 365], "temperature": 0.0, "avg_logprob": -0.1754241841179984, "compression_ratio": 1.7239263803680982, "no_speech_prob": 5.0294540415052325e-05}, {"id": 1087, "seek": 768152, "start": 7681.52, "end": 7688.080000000001, "text": " a wide range of suitable problems faced by humans. Versus may handle some problems in", "tokens": [257, 4874, 3613, 295, 12873, 2740, 11446, 538, 6255, 13, 12226, 301, 815, 4813, 512, 2740, 294], "temperature": 0.0, "avg_logprob": -0.1994741185506185, "compression_ratio": 1.7326732673267327, "no_speech_prob": 4.8181231250055134e-05}, {"id": 1088, "seek": 768152, "start": 7688.080000000001, "end": 7694.56, "text": " domains where reductionist models cannot be created or used, known as bizarre domains.", "tokens": [25514, 689, 11004, 468, 5245, 2644, 312, 2942, 420, 1143, 11, 2570, 382, 18265, 25514, 13], "temperature": 0.0, "avg_logprob": -0.1994741185506185, "compression_ratio": 1.7326732673267327, "no_speech_prob": 4.8181231250055134e-05}, {"id": 1089, "seek": 768152, "start": 7695.84, "end": 7701.76, "text": " Handles many important complicated problems such as going to the moon or a highway system.", "tokens": [8854, 904, 867, 1021, 6179, 2740, 1270, 382, 516, 281, 264, 7135, 420, 257, 17205, 1185, 13], "temperature": 0.0, "avg_logprob": -0.1994741185506185, "compression_ratio": 1.7326732673267327, "no_speech_prob": 4.8181231250055134e-05}, {"id": 1090, "seek": 768152, "start": 7702.400000000001, "end": 7708.400000000001, "text": " Versus handles many important complex problems such as protein folding and playing go.", "tokens": [12226, 301, 18722, 867, 1021, 3997, 2740, 1270, 382, 7944, 25335, 293, 2433, 352, 13], "temperature": 0.0, "avg_logprob": -0.1994741185506185, "compression_ratio": 1.7326732673267327, "no_speech_prob": 4.8181231250055134e-05}, {"id": 1091, "seek": 770840, "start": 7708.4, "end": 7717.839999999999, "text": " Handles problems requiring planning or cooperation. Versus handles simple mundane problems such as", "tokens": [8854, 904, 2740, 24165, 5038, 420, 14968, 13, 12226, 301, 18722, 2199, 43497, 2740, 1270, 382], "temperature": 0.0, "avg_logprob": -0.12956233535494124, "compression_ratio": 1.5, "no_speech_prob": 4.523230381892063e-05}, {"id": 1092, "seek": 770840, "start": 7717.839999999999, "end": 7725.36, "text": " understanding language or vision or making breakfast. Money rows in these tables discuss", "tokens": [3701, 2856, 420, 5201, 420, 1455, 8201, 13, 16631, 13241, 294, 613, 8020, 2248], "temperature": 0.0, "avg_logprob": -0.12956233535494124, "compression_ratio": 1.5, "no_speech_prob": 4.523230381892063e-05}, {"id": 1093, "seek": 770840, "start": 7725.36, "end": 7732.5599999999995, "text": " hard trade-offs where compromises are impossible or prohibitively expensive. These are identified", "tokens": [1152, 4923, 12, 19231, 689, 11482, 3598, 366, 6243, 420, 16015, 2187, 356, 5124, 13, 1981, 366, 9234], "temperature": 0.0, "avg_logprob": -0.12956233535494124, "compression_ratio": 1.5, "no_speech_prob": 4.523230381892063e-05}, {"id": 1094, "seek": 773256, "start": 7732.56, "end": 7739.120000000001, "text": " by bold face numbers in the first column. The meaning rows may not be clear trade-offs or even", "tokens": [538, 11928, 1851, 3547, 294, 264, 700, 7738, 13, 440, 3620, 13241, 815, 406, 312, 1850, 4923, 12, 19231, 420, 754], "temperature": 0.0, "avg_logprob": -0.09488877383145419, "compression_ratio": 1.5611814345991561, "no_speech_prob": 4.414163049659692e-05}, {"id": 1095, "seek": 773256, "start": 7739.120000000001, "end": 7746.64, "text": " disjoint alternatives. Mixed systems are described in a separate chapter. These form the core of", "tokens": [717, 48613, 20478, 13, 12769, 292, 3652, 366, 7619, 294, 257, 4994, 7187, 13, 1981, 1254, 264, 4965, 295], "temperature": 0.0, "avg_logprob": -0.09488877383145419, "compression_ratio": 1.5611814345991561, "no_speech_prob": 4.414163049659692e-05}, {"id": 1096, "seek": 773256, "start": 7746.64, "end": 7752.240000000001, "text": " these dichotomies and are discussed in most of what follows, but also in detail at the", "tokens": [613, 10390, 42939, 530, 293, 366, 7152, 294, 881, 295, 437, 10002, 11, 457, 611, 294, 2607, 412, 264], "temperature": 0.0, "avg_logprob": -0.09488877383145419, "compression_ratio": 1.5611814345991561, "no_speech_prob": 4.414163049659692e-05}, {"id": 1097, "seek": 773256, "start": 7752.240000000001, "end": 7759.120000000001, "text": " chapter on introducing AI epistemology and in videos of talks. A leather report is based on", "tokens": [7187, 322, 15424, 7318, 2388, 43958, 1793, 293, 294, 2145, 295, 6686, 13, 316, 12821, 2275, 307, 2361, 322], "temperature": 0.0, "avg_logprob": -0.09488877383145419, "compression_ratio": 1.5611814345991561, "no_speech_prob": 4.414163049659692e-05}, {"id": 1098, "seek": 775912, "start": 7759.12, "end": 7766.24, "text": " models in meteorology. To solve the problem directly in the problem domain, open a window", "tokens": [5245, 294, 25313, 1793, 13, 1407, 5039, 264, 1154, 3838, 294, 264, 1154, 9274, 11, 1269, 257, 4910], "temperature": 0.0, "avg_logprob": -0.06796620650724931, "compression_ratio": 1.5982905982905984, "no_speech_prob": 4.50344814453274e-05}, {"id": 1099, "seek": 775912, "start": 7766.24, "end": 7774.0, "text": " to check if it smells like rain. Reductionism is the greatest invention our species has ever made.", "tokens": [281, 1520, 498, 309, 10036, 411, 4830, 13, 4477, 27549, 1434, 307, 264, 6636, 22265, 527, 6172, 575, 1562, 1027, 13], "temperature": 0.0, "avg_logprob": -0.06796620650724931, "compression_ratio": 1.5982905982905984, "no_speech_prob": 4.50344814453274e-05}, {"id": 1100, "seek": 775912, "start": 7774.64, "end": 7780.48, "text": " But reductionist models cannot be created or used when any one of the multitudes of blocking", "tokens": [583, 11004, 468, 5245, 2644, 312, 2942, 420, 1143, 562, 604, 472, 295, 264, 2120, 16451, 295, 17776], "temperature": 0.0, "avg_logprob": -0.06796620650724931, "compression_ratio": 1.5982905982905984, "no_speech_prob": 4.50344814453274e-05}, {"id": 1101, "seek": 775912, "start": 7780.48, "end": 7788.5599999999995, "text": " issues are present. Models work, in theory or in a laboratory where we can isolate a device,", "tokens": [2663, 366, 1974, 13, 6583, 1625, 589, 11, 294, 5261, 420, 294, 257, 16523, 689, 321, 393, 25660, 257, 4302, 11], "temperature": 0.0, "avg_logprob": -0.06796620650724931, "compression_ratio": 1.5982905982905984, "no_speech_prob": 4.50344814453274e-05}, {"id": 1102, "seek": 778856, "start": 7788.56, "end": 7797.120000000001, "text": " organism or phenomenon from a changing environment. However, complex situations may involve tracking", "tokens": [24128, 420, 14029, 490, 257, 4473, 2823, 13, 2908, 11, 3997, 6851, 815, 9494, 11603], "temperature": 0.0, "avg_logprob": -0.0598975077067336, "compression_ratio": 1.6163793103448276, "no_speech_prob": 2.6861080186790787e-05}, {"id": 1103, "seek": 778856, "start": 7797.120000000001, "end": 7803.360000000001, "text": " and responding to a large number of conflicting and unreliable signals from a constantly changing", "tokens": [293, 16670, 281, 257, 2416, 1230, 295, 43784, 293, 20584, 2081, 712, 12354, 490, 257, 6460, 4473], "temperature": 0.0, "avg_logprob": -0.0598975077067336, "compression_ratio": 1.6163793103448276, "no_speech_prob": 2.6861080186790787e-05}, {"id": 1104, "seek": 778856, "start": 7803.360000000001, "end": 7810.0, "text": " world or environment. Reductionism is here at a severe disadvantage and can rarely perform", "tokens": [1002, 420, 2823, 13, 4477, 27549, 1434, 307, 510, 412, 257, 8922, 24292, 293, 393, 13752, 2042], "temperature": 0.0, "avg_logprob": -0.0598975077067336, "compression_ratio": 1.6163793103448276, "no_speech_prob": 2.6861080186790787e-05}, {"id": 1105, "seek": 778856, "start": 7810.0, "end": 7816.56, "text": " above the level of statistical models. In contrast, holistic machine learning methods", "tokens": [3673, 264, 1496, 295, 22820, 5245, 13, 682, 8712, 11, 30334, 3479, 2539, 7150], "temperature": 0.0, "avg_logprob": -0.0598975077067336, "compression_ratio": 1.6163793103448276, "no_speech_prob": 2.6861080186790787e-05}, {"id": 1106, "seek": 781656, "start": 7816.56, "end": 7822.400000000001, "text": " learning from unfiltered inputs can discover correlations that humans might miss and can", "tokens": [2539, 490, 3971, 388, 40665, 15743, 393, 4411, 13983, 763, 300, 6255, 1062, 1713, 293, 393], "temperature": 0.0, "avg_logprob": -0.09026021115920123, "compression_ratio": 1.6192660550458715, "no_speech_prob": 7.07172293914482e-05}, {"id": 1107, "seek": 781656, "start": 7822.400000000001, "end": 7828.8, "text": " construct internal pattern-based structures to provide recognition, epistemic reduction,", "tokens": [7690, 6920, 5102, 12, 6032, 9227, 281, 2893, 11150, 11, 2388, 468, 3438, 11004, 11], "temperature": 0.0, "avg_logprob": -0.09026021115920123, "compression_ratio": 1.6192660550458715, "no_speech_prob": 7.07172293914482e-05}, {"id": 1108, "seek": 781656, "start": 7828.8, "end": 7836.88, "text": " abstraction, prediction, noise rejection and other cognitive capabilities. Humans generally", "tokens": [37765, 11, 17630, 11, 5658, 26044, 293, 661, 15605, 10862, 13, 35809, 5101], "temperature": 0.0, "avg_logprob": -0.09026021115920123, "compression_ratio": 1.6192660550458715, "no_speech_prob": 7.07172293914482e-05}, {"id": 1109, "seek": 781656, "start": 7836.88, "end": 7844.080000000001, "text": " use holistic methods for seemingly simple, but in reality, complex mundane problems", "tokens": [764, 30334, 7150, 337, 18709, 2199, 11, 457, 294, 4103, 11, 3997, 43497, 2740], "temperature": 0.0, "avg_logprob": -0.09026021115920123, "compression_ratio": 1.6192660550458715, "no_speech_prob": 7.07172293914482e-05}, {"id": 1110, "seek": 784408, "start": 7844.08, "end": 7850.72, "text": " like understanding vision, human language, learning to walk, or making breakfast.", "tokens": [411, 3701, 5201, 11, 1952, 2856, 11, 2539, 281, 1792, 11, 420, 1455, 8201, 13], "temperature": 0.0, "avg_logprob": -0.1501709014650375, "compression_ratio": 1.434782608695652, "no_speech_prob": 7.796334102749825e-05}, {"id": 1111, "seek": 784408, "start": 7851.36, "end": 7858.96, "text": " Computers use them for very complex problems and mel-based AI in general, such as protein folding", "tokens": [37804, 433, 764, 552, 337, 588, 3997, 2740, 293, 4795, 12, 6032, 7318, 294, 2674, 11, 1270, 382, 7944, 25335], "temperature": 0.0, "avg_logprob": -0.1501709014650375, "compression_ratio": 1.434782608695652, "no_speech_prob": 7.796334102749825e-05}, {"id": 1112, "seek": 784408, "start": 7858.96, "end": 7866.0, "text": " and playing go, but also simpler ones, such as real estate pricing. Main trade-offs.", "tokens": [293, 2433, 352, 11, 457, 611, 18587, 2306, 11, 1270, 382, 957, 9749, 17621, 13, 12383, 4923, 12, 19231, 13], "temperature": 0.0, "avg_logprob": -0.1501709014650375, "compression_ratio": 1.434782608695652, "no_speech_prob": 7.796334102749825e-05}, {"id": 1113, "seek": 786600, "start": 7866.0, "end": 7875.6, "text": " Reductionism in science versus realism in machine learning. Optimality, the best answer,", "tokens": [4477, 27549, 1434, 294, 3497, 5717, 38484, 294, 3479, 2539, 13, 35013, 1860, 11, 264, 1151, 1867, 11], "temperature": 0.0, "avg_logprob": -0.3032645563925466, "compression_ratio": 1.6477987421383649, "no_speech_prob": 0.00025154478498734534}, {"id": 1114, "seek": 786600, "start": 7875.6, "end": 7884.88, "text": " versus economy, reuse no useful answers. Completeness, all answers, versus promptness,", "tokens": [5717, 5010, 11, 26225, 572, 4420, 6338, 13, 31804, 15264, 11, 439, 6338, 11, 5717, 12391, 1287, 11], "temperature": 0.0, "avg_logprob": -0.3032645563925466, "compression_ratio": 1.6477987421383649, "no_speech_prob": 0.00025154478498734534}, {"id": 1115, "seek": 786600, "start": 7884.88, "end": 7893.6, "text": " except first use for a answer. Repeatability, same answer every time, versus learning,", "tokens": [3993, 700, 764, 337, 257, 1867, 13, 28523, 2310, 11, 912, 1867, 633, 565, 11, 5717, 2539, 11], "temperature": 0.0, "avg_logprob": -0.3032645563925466, "compression_ratio": 1.6477987421383649, "no_speech_prob": 0.00025154478498734534}, {"id": 1116, "seek": 789360, "start": 7893.6, "end": 7902.96, "text": " versus learning, results improve with practice. Extrapolation, in low-dimensionality domains,", "tokens": [5717, 2539, 11, 3542, 3470, 365, 3124, 13, 9881, 4007, 401, 399, 11, 294, 2295, 12, 13595, 3378, 1860, 25514, 11], "temperature": 0.0, "avg_logprob": -0.08928598574738005, "compression_ratio": 1.603448275862069, "no_speech_prob": 9.252863674191758e-05}, {"id": 1117, "seek": 789360, "start": 7902.96, "end": 7912.160000000001, "text": " versus interpolation, even in high-dimensionality domains. Transparency, understand the process", "tokens": [5717, 44902, 399, 11, 754, 294, 1090, 12, 13595, 3378, 1860, 25514, 13, 6531, 79, 4484, 1344, 11, 1223, 264, 1399], "temperature": 0.0, "avg_logprob": -0.08928598574738005, "compression_ratio": 1.603448275862069, "no_speech_prob": 9.252863674191758e-05}, {"id": 1118, "seek": 789360, "start": 7912.160000000001, "end": 7919.280000000001, "text": " to get the answer, versus intuition, accept useful answers even if achieved by unknown or", "tokens": [281, 483, 264, 1867, 11, 5717, 24002, 11, 3241, 4420, 6338, 754, 498, 11042, 538, 9841, 420], "temperature": 0.0, "avg_logprob": -0.08928598574738005, "compression_ratio": 1.603448275862069, "no_speech_prob": 9.252863674191758e-05}, {"id": 1119, "seek": 791928, "start": 7919.28, "end": 7928.88, "text": " subconscious means. Explainability, understand the answer, versus positive ignorance, no need to", "tokens": [27389, 1355, 13, 39574, 2310, 11, 1223, 264, 1867, 11, 5717, 3353, 25390, 11, 572, 643, 281], "temperature": 0.0, "avg_logprob": -0.14047176497323172, "compression_ratio": 1.5674157303370786, "no_speech_prob": 7.200720574473962e-05}, {"id": 1120, "seek": 791928, "start": 7928.88, "end": 7936.8, "text": " even understand the problem or problem domain. Shareability, abstract models are taught in", "tokens": [754, 1223, 264, 1154, 420, 1154, 9274, 13, 14945, 2310, 11, 12649, 5245, 366, 5928, 294], "temperature": 0.0, "avg_logprob": -0.14047176497323172, "compression_ratio": 1.5674157303370786, "no_speech_prob": 7.200720574473962e-05}, {"id": 1121, "seek": 791928, "start": 7936.8, "end": 7944.96, "text": " communicated using language or software, versus copyability. ML understanding, a competence", "tokens": [34989, 1228, 2856, 420, 4722, 11, 5717, 5055, 2310, 13, 21601, 3701, 11, 257, 39965], "temperature": 0.0, "avg_logprob": -0.14047176497323172, "compression_ratio": 1.5674157303370786, "no_speech_prob": 7.200720574473962e-05}, {"id": 1122, "seek": 794496, "start": 7944.96, "end": 7953.2, "text": " can be copied as a memory image. Optimality, completeness, and repeatability are only available", "tokens": [393, 312, 25365, 382, 257, 4675, 3256, 13, 35013, 1860, 11, 1557, 15264, 11, 293, 7149, 2310, 366, 787, 2435], "temperature": 0.0, "avg_logprob": -0.06986998875935872, "compression_ratio": 1.6077586206896552, "no_speech_prob": 4.395408177515492e-05}, {"id": 1123, "seek": 794496, "start": 7953.2, "end": 7960.32, "text": " in theoretical model spaces and sometimes under laboratory conditions. Economy and promptness", "tokens": [294, 20864, 2316, 7673, 293, 2171, 833, 16523, 4487, 13, 48223, 293, 12391, 1287], "temperature": 0.0, "avg_logprob": -0.06986998875935872, "compression_ratio": 1.6077586206896552, "no_speech_prob": 4.395408177515492e-05}, {"id": 1124, "seek": 794496, "start": 7960.32, "end": 7966.16, "text": " had much higher survival value in evolutionary history than optimality and completeness.", "tokens": [632, 709, 2946, 12559, 2158, 294, 27567, 2503, 813, 5028, 1860, 293, 1557, 15264, 13], "temperature": 0.0, "avg_logprob": -0.06986998875935872, "compression_ratio": 1.6077586206896552, "no_speech_prob": 4.395408177515492e-05}, {"id": 1125, "seek": 794496, "start": 7966.8, "end": 7972.96, "text": " The strongest hint that a system is holistic is that the results improve with practice because", "tokens": [440, 16595, 12075, 300, 257, 1185, 307, 30334, 307, 300, 264, 3542, 3470, 365, 3124, 570], "temperature": 0.0, "avg_logprob": -0.06986998875935872, "compression_ratio": 1.6077586206896552, "no_speech_prob": 4.395408177515492e-05}, {"id": 1126, "seek": 797296, "start": 7972.96, "end": 7979.84, "text": " the system learns from its mistakes. In machine learning, a larger learning corpus is in general", "tokens": [264, 1185, 27152, 490, 1080, 8038, 13, 682, 3479, 2539, 11, 257, 4833, 2539, 1181, 31624, 307, 294, 2674], "temperature": 0.0, "avg_logprob": -0.06399450054416408, "compression_ratio": 1.6200873362445414, "no_speech_prob": 0.00010255281813442707}, {"id": 1127, "seek": 797296, "start": 7979.84, "end": 7986.4800000000005, "text": " better than the smaller one because it provides more opportunities for making mistakes to learn from,", "tokens": [1101, 813, 264, 4356, 472, 570, 309, 6417, 544, 4786, 337, 1455, 8038, 281, 1466, 490, 11], "temperature": 0.0, "avg_logprob": -0.06399450054416408, "compression_ratio": 1.6200873362445414, "no_speech_prob": 0.00010255281813442707}, {"id": 1128, "seek": 797296, "start": 7986.4800000000005, "end": 7993.68, "text": " such as corner cases. Models created by humans have manageable numbers of parameters because", "tokens": [1270, 382, 4538, 3331, 13, 6583, 1625, 2942, 538, 6255, 362, 38798, 3547, 295, 9834, 570], "temperature": 0.0, "avg_logprob": -0.06399450054416408, "compression_ratio": 1.6200873362445414, "no_speech_prob": 0.00010255281813442707}, {"id": 1129, "seek": 797296, "start": 7993.68, "end": 7999.28, "text": " the scientist or engineer working on the problem has done a, hopefully correct,", "tokens": [264, 12662, 420, 11403, 1364, 322, 264, 1154, 575, 1096, 257, 11, 4696, 3006, 11], "temperature": 0.0, "avg_logprob": -0.06399450054416408, "compression_ratio": 1.6200873362445414, "no_speech_prob": 0.00010255281813442707}, {"id": 1130, "seek": 799928, "start": 7999.28, "end": 8006.88, "text": " epistemic reduction from a complex and messy world to a computable model. This allows experimentation", "tokens": [2388, 468, 3438, 11004, 490, 257, 3997, 293, 16191, 1002, 281, 257, 2807, 712, 2316, 13, 639, 4045, 37142], "temperature": 0.0, "avg_logprob": -0.050975069215026085, "compression_ratio": 1.619047619047619, "no_speech_prob": 5.086384044261649e-05}, {"id": 1131, "seek": 799928, "start": 8006.88, "end": 8013.599999999999, "text": " with what if scenarios by varying model parameters. It is up to the model user to determine which", "tokens": [365, 437, 498, 15077, 538, 22984, 2316, 9834, 13, 467, 307, 493, 281, 264, 2316, 4195, 281, 6997, 597], "temperature": 0.0, "avg_logprob": -0.050975069215026085, "compression_ratio": 1.619047619047619, "no_speech_prob": 5.086384044261649e-05}, {"id": 1132, "seek": 799928, "start": 8013.599999999999, "end": 8020.8, "text": " extrapolations are reasonable. In holistic ML systems, we are getting used to systems with", "tokens": [48224, 763, 366, 10585, 13, 682, 30334, 21601, 3652, 11, 321, 366, 1242, 1143, 281, 3652, 365], "temperature": 0.0, "avg_logprob": -0.050975069215026085, "compression_ratio": 1.619047619047619, "no_speech_prob": 5.086384044261649e-05}, {"id": 1133, "seek": 799928, "start": 8020.8, "end": 8027.2, "text": " millions or billions of parameters. These structures are very difficult to analyze,", "tokens": [6803, 420, 17375, 295, 9834, 13, 1981, 9227, 366, 588, 2252, 281, 12477, 11], "temperature": 0.0, "avg_logprob": -0.050975069215026085, "compression_ratio": 1.619047619047619, "no_speech_prob": 5.086384044261649e-05}, {"id": 1134, "seek": 802720, "start": 8027.2, "end": 8033.28, "text": " and just like with human intelligences, the best way to estimate their competence is through", "tokens": [293, 445, 411, 365, 1952, 5613, 2667, 11, 264, 1151, 636, 281, 12539, 641, 39965, 307, 807], "temperature": 0.0, "avg_logprob": -0.0479529014853544, "compression_ratio": 1.5991735537190082, "no_speech_prob": 5.367755875340663e-05}, {"id": 1135, "seek": 802720, "start": 8033.28, "end": 8041.12, "text": " testing. Extrapolation is typically out of scope for holistic systems. The majority of end users", "tokens": [4997, 13, 9881, 4007, 401, 399, 307, 5850, 484, 295, 11923, 337, 30334, 3652, 13, 440, 6286, 295, 917, 5022], "temperature": 0.0, "avg_logprob": -0.0479529014853544, "compression_ratio": 1.5991735537190082, "no_speech_prob": 5.367755875340663e-05}, {"id": 1136, "seek": 802720, "start": 8041.12, "end": 8047.44, "text": " will have no interest in how some machine came up with some obviously correct answer. They will", "tokens": [486, 362, 572, 1179, 294, 577, 512, 3479, 1361, 493, 365, 512, 2745, 3006, 1867, 13, 814, 486], "temperature": 0.0, "avg_logprob": -0.0479529014853544, "compression_ratio": 1.5991735537190082, "no_speech_prob": 5.367755875340663e-05}, {"id": 1137, "seek": 802720, "start": 8047.44, "end": 8054.8, "text": " just accept it the way we accept our own understanding of language, even though we do not know how we", "tokens": [445, 3241, 309, 264, 636, 321, 3241, 527, 1065, 3701, 295, 2856, 11, 754, 1673, 321, 360, 406, 458, 577, 321], "temperature": 0.0, "avg_logprob": -0.0479529014853544, "compression_ratio": 1.5991735537190082, "no_speech_prob": 5.367755875340663e-05}, {"id": 1138, "seek": 805480, "start": 8054.8, "end": 8062.72, "text": " do it. We now find ourselves asking our machines to solve problems we either don't know how to solve,", "tokens": [360, 309, 13, 492, 586, 915, 4175, 3365, 527, 8379, 281, 5039, 2740, 321, 2139, 500, 380, 458, 577, 281, 5039, 11], "temperature": 0.0, "avg_logprob": -0.06779777086698092, "compression_ratio": 1.6302521008403361, "no_speech_prob": 9.865743049886078e-05}, {"id": 1139, "seek": 805480, "start": 8062.72, "end": 8070.320000000001, "text": " or can't be bothered to figure out how to solve. We have reached a major benefit of AI. We can be", "tokens": [420, 393, 380, 312, 22996, 281, 2573, 484, 577, 281, 5039, 13, 492, 362, 6488, 257, 2563, 5121, 295, 7318, 13, 492, 393, 312], "temperature": 0.0, "avg_logprob": -0.06779777086698092, "compression_ratio": 1.6302521008403361, "no_speech_prob": 9.865743049886078e-05}, {"id": 1140, "seek": 805480, "start": 8070.320000000001, "end": 8076.24, "text": " positively ignorant of many mundane things and will be happy to delegate such matters to our", "tokens": [25795, 29374, 295, 867, 43497, 721, 293, 486, 312, 2055, 281, 40999, 1270, 7001, 281, 527], "temperature": 0.0, "avg_logprob": -0.06779777086698092, "compression_ratio": 1.6302521008403361, "no_speech_prob": 9.865743049886078e-05}, {"id": 1141, "seek": 805480, "start": 8076.24, "end": 8083.68, "text": " machines so that we may play or focus on more important things. Some schools of thought tend to", "tokens": [8379, 370, 300, 321, 815, 862, 420, 1879, 322, 544, 1021, 721, 13, 2188, 4656, 295, 1194, 3928, 281], "temperature": 0.0, "avg_logprob": -0.06779777086698092, "compression_ratio": 1.6302521008403361, "no_speech_prob": 9.865743049886078e-05}, {"id": 1142, "seek": 808368, "start": 8083.68, "end": 8091.92, "text": " overvalue explainability. To them, ML is a serious step down from results obtained scientifically", "tokens": [670, 29155, 2903, 2310, 13, 1407, 552, 11, 21601, 307, 257, 3156, 1823, 760, 490, 3542, 14879, 39719], "temperature": 0.0, "avg_logprob": -0.09642984790186729, "compression_ratio": 1.4867724867724867, "no_speech_prob": 0.00010620362445479259}, {"id": 1143, "seek": 808368, "start": 8091.92, "end": 8098.56, "text": " where we can all inspect the causality, for instance in a reductionist production, expert", "tokens": [689, 321, 393, 439, 15018, 264, 3302, 1860, 11, 337, 5197, 294, 257, 11004, 468, 4265, 11, 5844], "temperature": 0.0, "avg_logprob": -0.09642984790186729, "compression_ratio": 1.4867724867724867, "no_speech_prob": 0.00010620362445479259}, {"id": 1144, "seek": 808368, "start": 8098.56, "end": 8106.0, "text": " systems. But the bottom line is that today we can often choose between one, understanding the", "tokens": [3652, 13, 583, 264, 2767, 1622, 307, 300, 965, 321, 393, 2049, 2826, 1296, 472, 11, 3701, 264], "temperature": 0.0, "avg_logprob": -0.09642984790186729, "compression_ratio": 1.4867724867724867, "no_speech_prob": 0.00010620362445479259}, {"id": 1145, "seek": 810600, "start": 8106.0, "end": 8114.64, "text": " problem domain, problem, the use of science and relevant models, and the answer. Or two, just", "tokens": [1154, 9274, 11, 1154, 11, 264, 764, 295, 3497, 293, 7340, 5245, 11, 293, 264, 1867, 13, 1610, 732, 11, 445], "temperature": 0.0, "avg_logprob": -0.07143794531109689, "compression_ratio": 1.6609442060085837, "no_speech_prob": 2.8863853003713302e-05}, {"id": 1146, "seek": 810600, "start": 8114.64, "end": 8120.48, "text": " getting a useful answer without even bothering to understand the problem or the problem domain.", "tokens": [1242, 257, 4420, 1867, 1553, 754, 31432, 281, 1223, 264, 1154, 420, 264, 1154, 9274, 13], "temperature": 0.0, "avg_logprob": -0.07143794531109689, "compression_ratio": 1.6609442060085837, "no_speech_prob": 2.8863853003713302e-05}, {"id": 1147, "seek": 810600, "start": 8121.12, "end": 8128.48, "text": " The latter, positive ignorance, is a lot closer to AI than the first, and we can expect the use", "tokens": [440, 18481, 11, 3353, 25390, 11, 307, 257, 688, 4966, 281, 7318, 813, 264, 700, 11, 293, 321, 393, 2066, 264, 764], "temperature": 0.0, "avg_logprob": -0.07143794531109689, "compression_ratio": 1.6609442060085837, "no_speech_prob": 2.8863853003713302e-05}, {"id": 1148, "seek": 810600, "start": 8128.48, "end": 8135.76, "text": " of holistic methods to continue to increase. Science strives towards a consensus world model in order", "tokens": [295, 30334, 7150, 281, 2354, 281, 3488, 13, 8976, 3575, 977, 3030, 257, 19115, 1002, 2316, 294, 1668], "temperature": 0.0, "avg_logprob": -0.07143794531109689, "compression_ratio": 1.6609442060085837, "no_speech_prob": 2.8863853003713302e-05}, {"id": 1149, "seek": 813576, "start": 8135.76, "end": 8141.84, "text": " to facilitate communication and minimize costly engineering mistakes caused by ignorance and", "tokens": [281, 20207, 6101, 293, 17522, 28328, 7043, 8038, 7008, 538, 25390, 293], "temperature": 0.0, "avg_logprob": -0.09726132974996195, "compression_ratio": 1.6444444444444444, "no_speech_prob": 6.053141987649724e-05}, {"id": 1150, "seek": 813576, "start": 8141.84, "end": 8149.84, "text": " misunderstandings. Scientific communication requires a high-level context, a world model,", "tokens": [35736, 1109, 13, 47437, 6101, 7029, 257, 1090, 12, 12418, 4319, 11, 257, 1002, 2316, 11], "temperature": 0.0, "avg_logprob": -0.09726132974996195, "compression_ratio": 1.6444444444444444, "no_speech_prob": 6.053141987649724e-05}, {"id": 1151, "seek": 813576, "start": 8149.84, "end": 8157.84, "text": " shared by participants, and agreed upon signals such as words, math, and software. But direct", "tokens": [5507, 538, 10503, 11, 293, 9166, 3564, 12354, 1270, 382, 2283, 11, 5221, 11, 293, 4722, 13, 583, 2047], "temperature": 0.0, "avg_logprob": -0.09726132974996195, "compression_ratio": 1.6444444444444444, "no_speech_prob": 6.053141987649724e-05}, {"id": 1152, "seek": 813576, "start": 8157.84, "end": 8164.96, "text": " understanding, such as the skills to become a just grandmaster or a downhill skier, cannot be", "tokens": [3701, 11, 1270, 382, 264, 3942, 281, 1813, 257, 445, 2697, 21640, 420, 257, 29929, 1110, 811, 11, 2644, 312], "temperature": 0.0, "avg_logprob": -0.09726132974996195, "compression_ratio": 1.6444444444444444, "no_speech_prob": 6.053141987649724e-05}, {"id": 1153, "seek": 816496, "start": 8164.96, "end": 8172.8, "text": " shared using words. The experience must be acquired using individual practice. Computer-based systems", "tokens": [5507, 1228, 2283, 13, 440, 1752, 1633, 312, 17554, 1228, 2609, 3124, 13, 22289, 12, 6032, 3652], "temperature": 0.0, "avg_logprob": -0.1269832919625675, "compression_ratio": 1.5454545454545454, "no_speech_prob": 7.148226723074913e-05}, {"id": 1154, "seek": 816496, "start": 8172.8, "end": 8178.72, "text": " that learn a skill through practice can share the entire understanding so acquired by copying the", "tokens": [300, 1466, 257, 5389, 807, 3124, 393, 2073, 264, 2302, 3701, 370, 17554, 538, 27976, 264], "temperature": 0.0, "avg_logprob": -0.1269832919625675, "compression_ratio": 1.5454545454545454, "no_speech_prob": 7.148226723074913e-05}, {"id": 1155, "seek": 816496, "start": 8178.72, "end": 8184.0, "text": " memory content to another machine. Advantages of Holistic Methods", "tokens": [4675, 2701, 281, 1071, 3479, 13, 1999, 5219, 1660, 295, 11086, 3142, 25285, 82], "temperature": 0.0, "avg_logprob": -0.1269832919625675, "compression_ratio": 1.5454545454545454, "no_speech_prob": 7.148226723074913e-05}, {"id": 1156, "seek": 816496, "start": 8185.2, "end": 8190.0, "text": " Reductionism in Science versus Holism in Machine Learning", "tokens": [4477, 27549, 1434, 294, 8976, 5717, 11086, 1434, 294, 22155, 15205], "temperature": 0.0, "avg_logprob": -0.1269832919625675, "compression_ratio": 1.5454545454545454, "no_speech_prob": 7.148226723074913e-05}, {"id": 1157, "seek": 819000, "start": 8190.0, "end": 8198.08, "text": " N.P. Hard Problems cannot be solved, versus fines-valid solutions by guessing well-based", "tokens": [426, 13, 47, 13, 11817, 11676, 82, 2644, 312, 13041, 11, 5717, 37989, 12, 3337, 327, 6547, 538, 17939, 731, 12, 6032], "temperature": 0.0, "avg_logprob": -0.24319940900045728, "compression_ratio": 1.4181818181818182, "no_speech_prob": 0.00012843875447288156}, {"id": 1158, "seek": 819000, "start": 8198.08, "end": 8206.16, "text": " on a lifetime of experience. Geigo, garbage in, garbage out is a recognized problem,", "tokens": [322, 257, 11364, 295, 1752, 13, 2876, 7483, 11, 14150, 294, 11, 14150, 484, 307, 257, 9823, 1154, 11], "temperature": 0.0, "avg_logprob": -0.24319940900045728, "compression_ratio": 1.4181818181818182, "no_speech_prob": 0.00012843875447288156}, {"id": 1159, "seek": 819000, "start": 8206.8, "end": 8211.44, "text": " versus copes with missing, erroneous, and misleading inputs.", "tokens": [5717, 2971, 279, 365, 5361, 11, 1189, 26446, 563, 11, 293, 36429, 15743, 13], "temperature": 0.0, "avg_logprob": -0.24319940900045728, "compression_ratio": 1.4181818181818182, "no_speech_prob": 0.00012843875447288156}, {"id": 1160, "seek": 821144, "start": 8211.44, "end": 8219.2, "text": " Brightness. Experience catastrophic failures at edges of competence, versus anti-fragile.", "tokens": [24271, 1287, 13, 28503, 34915, 20774, 412, 8819, 295, 39965, 11, 5717, 6061, 12, 69, 3731, 794, 13], "temperature": 0.0, "avg_logprob": -0.24433482532769862, "compression_ratio": 1.644859813084112, "no_speech_prob": 7.007530803093687e-05}, {"id": 1161, "seek": 821144, "start": 8219.2, "end": 8227.04, "text": " Learns from mistakes, especially almost correct guesses in small, correctable failures.", "tokens": [17216, 82, 490, 8038, 11, 2318, 1920, 3006, 42703, 294, 1359, 11, 3006, 712, 20774, 13], "temperature": 0.0, "avg_logprob": -0.24433482532769862, "compression_ratio": 1.644859813084112, "no_speech_prob": 7.007530803093687e-05}, {"id": 1162, "seek": 821144, "start": 8228.24, "end": 8233.6, "text": " The models of a constantly changing world are obsolete the moment they are created,", "tokens": [440, 5245, 295, 257, 6460, 4473, 1002, 366, 46333, 264, 1623, 436, 366, 2942, 11], "temperature": 0.0, "avg_logprob": -0.24433482532769862, "compression_ratio": 1.644859813084112, "no_speech_prob": 7.007530803093687e-05}, {"id": 1163, "seek": 821144, "start": 8234.16, "end": 8240.08, "text": " versus incremental learning provides continuous adaptation to a constantly changing world.", "tokens": [5717, 35759, 2539, 6417, 10957, 21549, 281, 257, 6460, 4473, 1002, 13], "temperature": 0.0, "avg_logprob": -0.24433482532769862, "compression_ratio": 1.644859813084112, "no_speech_prob": 7.007530803093687e-05}, {"id": 1164, "seek": 824008, "start": 8240.08, "end": 8245.36, "text": " Algorithms may be incorrect or may be incorrectly implemented,", "tokens": [35014, 6819, 2592, 815, 312, 18424, 420, 815, 312, 42892, 12270, 11], "temperature": 0.0, "avg_logprob": -0.10613438020269554, "compression_ratio": 1.5694444444444444, "no_speech_prob": 0.00015129234816413373}, {"id": 1165, "seek": 824008, "start": 8246.0, "end": 8252.64, "text": " versus self-repairing systems can tolerate or correct internal errors. It is because we desire", "tokens": [5717, 2698, 12, 19919, 1246, 278, 3652, 393, 25773, 420, 3006, 6920, 13603, 13, 467, 307, 570, 321, 7516], "temperature": 0.0, "avg_logprob": -0.10613438020269554, "compression_ratio": 1.5694444444444444, "no_speech_prob": 0.00015129234816413373}, {"id": 1166, "seek": 824008, "start": 8252.64, "end": 8261.52, "text": " certainty, optimality, completeness, etc. L.N.P. Hardness becomes a problem. There are many", "tokens": [27022, 11, 5028, 1860, 11, 1557, 15264, 11, 5183, 13, 441, 13, 45, 13, 47, 13, 11817, 1287, 3643, 257, 1154, 13, 821, 366, 867], "temperature": 0.0, "avg_logprob": -0.10613438020269554, "compression_ratio": 1.5694444444444444, "no_speech_prob": 0.00015129234816413373}, {"id": 1167, "seek": 824008, "start": 8261.52, "end": 8267.44, "text": " problems where it is relatively easy to find a provably valid solution, but where finding", "tokens": [2740, 689, 309, 307, 7226, 1858, 281, 915, 257, 1439, 1188, 7363, 3827, 11, 457, 689, 5006], "temperature": 0.0, "avg_logprob": -0.10613438020269554, "compression_ratio": 1.5694444444444444, "no_speech_prob": 0.00015129234816413373}, {"id": 1168, "seek": 826744, "start": 8267.44, "end": 8274.32, "text": " all solutions can be very expensive. Real-world traveling salesmen merrily travel long reasonable", "tokens": [439, 6547, 393, 312, 588, 5124, 13, 8467, 12, 13217, 9712, 5763, 2558, 3551, 470, 356, 3147, 938, 10585], "temperature": 0.0, "avg_logprob": -0.08750605015527635, "compression_ratio": 1.6041666666666667, "no_speech_prob": 7.001071935519576e-05}, {"id": 1169, "seek": 826744, "start": 8274.32, "end": 8281.28, "text": " routes. If a reductionist system does not have complete and correct input data, it either cannot", "tokens": [18242, 13, 759, 257, 11004, 468, 1185, 775, 406, 362, 3566, 293, 3006, 4846, 1412, 11, 309, 2139, 2644], "temperature": 0.0, "avg_logprob": -0.08750605015527635, "compression_ratio": 1.6041666666666667, "no_speech_prob": 7.001071935519576e-05}, {"id": 1170, "seek": 826744, "start": 8281.28, "end": 8288.0, "text": " get started or produces questionable output. But it is an important requirement of real-world", "tokens": [483, 1409, 420, 14725, 37158, 5598, 13, 583, 309, 307, 364, 1021, 11695, 295, 957, 12, 13217], "temperature": 0.0, "avg_logprob": -0.08750605015527635, "compression_ratio": 1.6041666666666667, "no_speech_prob": 7.001071935519576e-05}, {"id": 1171, "seek": 826744, "start": 8288.0, "end": 8294.16, "text": " understanding machines that they be able to detect what is salient, important, in their input in", "tokens": [3701, 8379, 300, 436, 312, 1075, 281, 5531, 437, 307, 1845, 1196, 11, 1021, 11, 294, 641, 4846, 294], "temperature": 0.0, "avg_logprob": -0.08750605015527635, "compression_ratio": 1.6041666666666667, "no_speech_prob": 7.001071935519576e-05}, {"id": 1172, "seek": 829416, "start": 8294.16, "end": 8302.08, "text": " order to avoid paying attention to, and learning from, noise. And they have to deal with incomplete,", "tokens": [1668, 281, 5042, 6229, 3202, 281, 11, 293, 2539, 490, 11, 5658, 13, 400, 436, 362, 281, 2028, 365, 31709, 11], "temperature": 0.0, "avg_logprob": -0.04900905821058485, "compression_ratio": 1.6725663716814159, "no_speech_prob": 0.00011428869038354605}, {"id": 1173, "seek": 829416, "start": 8302.08, "end": 8307.84, "text": " erroneous, and misleading input generated by millions of other intelligent agents with", "tokens": [1189, 26446, 563, 11, 293, 36429, 4846, 10833, 538, 6803, 295, 661, 13232, 12554, 365], "temperature": 0.0, "avg_logprob": -0.04900905821058485, "compression_ratio": 1.6725663716814159, "no_speech_prob": 0.00011428869038354605}, {"id": 1174, "seek": 829416, "start": 8307.84, "end": 8315.36, "text": " goals at odds with their own. They need to be able to detect omissions, duplications, errors,", "tokens": [5493, 412, 17439, 365, 641, 1065, 13, 814, 643, 281, 312, 1075, 281, 5531, 3406, 7922, 11, 17154, 763, 11, 13603, 11], "temperature": 0.0, "avg_logprob": -0.04900905821058485, "compression_ratio": 1.6725663716814159, "no_speech_prob": 0.00011428869038354605}, {"id": 1175, "seek": 829416, "start": 8315.36, "end": 8322.8, "text": " noise, lies, etc. And the only epistemologically plausible way to do this is to relate the input", "tokens": [5658, 11, 9134, 11, 5183, 13, 400, 264, 787, 2388, 43958, 17157, 39925, 636, 281, 360, 341, 307, 281, 10961, 264, 4846], "temperature": 0.0, "avg_logprob": -0.04900905821058485, "compression_ratio": 1.6725663716814159, "no_speech_prob": 0.00011428869038354605}, {"id": 1176, "seek": 832280, "start": 8322.8, "end": 8330.16, "text": " to similar input they have understood in the past, what they already know. They need to understand", "tokens": [281, 2531, 4846, 436, 362, 7320, 294, 264, 1791, 11, 437, 436, 1217, 458, 13, 814, 643, 281, 1223], "temperature": 0.0, "avg_logprob": -0.05198256373405456, "compression_ratio": 1.728888888888889, "no_speech_prob": 7.299068965949118e-05}, {"id": 1177, "seek": 832280, "start": 8330.16, "end": 8337.359999999999, "text": " what matters but if they can also understand some of the noise. This is advertising, they can exploit", "tokens": [437, 7001, 457, 498, 436, 393, 611, 1223, 512, 295, 264, 5658, 13, 639, 307, 13097, 11, 436, 393, 25924], "temperature": 0.0, "avg_logprob": -0.05198256373405456, "compression_ratio": 1.728888888888889, "no_speech_prob": 7.299068965949118e-05}, {"id": 1178, "seek": 832280, "start": 8337.359999999999, "end": 8343.92, "text": " that. There are many image and video apps available featuring image understanding based on deep", "tokens": [300, 13, 821, 366, 867, 3256, 293, 960, 7733, 2435, 19742, 3256, 3701, 2361, 322, 2452], "temperature": 0.0, "avg_logprob": -0.05198256373405456, "compression_ratio": 1.728888888888889, "no_speech_prob": 7.299068965949118e-05}, {"id": 1179, "seek": 832280, "start": 8343.92, "end": 8351.439999999999, "text": " learning. These apps can remove backgrounds, sharpen details like eyelashes, restore damaged", "tokens": [2539, 13, 1981, 7733, 393, 4159, 17336, 11, 31570, 4365, 411, 37017, 11, 15227, 14080], "temperature": 0.0, "avg_logprob": -0.05198256373405456, "compression_ratio": 1.728888888888889, "no_speech_prob": 7.299068965949118e-05}, {"id": 1180, "seek": 835144, "start": 8351.44, "end": 8359.12, "text": " photographs, etc. We need to keep in mind that the ability of holistic systems to fill in data", "tokens": [17649, 11, 5183, 13, 492, 643, 281, 1066, 294, 1575, 300, 264, 3485, 295, 30334, 3652, 281, 2836, 294, 1412], "temperature": 0.0, "avg_logprob": -0.0474476785194583, "compression_ratio": 1.6508620689655173, "no_speech_prob": 5.110921119921841e-05}, {"id": 1181, "seek": 835144, "start": 8359.12, "end": 8365.92, "text": " and detect noise depends on them having learned from similar data in the past. We note that all", "tokens": [293, 5531, 5658, 5946, 322, 552, 1419, 3264, 490, 2531, 1412, 294, 264, 1791, 13, 492, 3637, 300, 439], "temperature": 0.0, "avg_logprob": -0.0474476785194583, "compression_ratio": 1.6508620689655173, "no_speech_prob": 5.110921119921841e-05}, {"id": 1182, "seek": 835144, "start": 8365.92, "end": 8371.92, "text": " the image improvements are confabulations based on prior experience from their learning corpora.", "tokens": [264, 3256, 13797, 366, 1497, 455, 4136, 2361, 322, 4059, 1752, 490, 641, 2539, 6804, 64, 13], "temperature": 0.0, "avg_logprob": -0.0474476785194583, "compression_ratio": 1.6508620689655173, "no_speech_prob": 5.110921119921841e-05}, {"id": 1183, "seek": 835144, "start": 8372.560000000001, "end": 8379.28, "text": " But we can also note that image composition using these methods yields totally seamless images,", "tokens": [583, 321, 393, 611, 3637, 300, 3256, 12686, 1228, 613, 7150, 32168, 3879, 28677, 5267, 11], "temperature": 0.0, "avg_logprob": -0.0474476785194583, "compression_ratio": 1.6508620689655173, "no_speech_prob": 5.110921119921841e-05}, {"id": 1184, "seek": 837928, "start": 8379.28, "end": 8387.04, "text": " very far from cut and paste of pixels. And quite similarly, we find language confabulation by", "tokens": [588, 1400, 490, 1723, 293, 9163, 295, 18668, 13, 400, 1596, 14138, 11, 321, 915, 2856, 1497, 455, 2776, 538], "temperature": 0.0, "avg_logprob": -0.06218953974106733, "compression_ratio": 1.524390243902439, "no_speech_prob": 0.00010580845992080867}, {"id": 1185, "seek": 837928, "start": 8387.04, "end": 8394.880000000001, "text": " systems like GPT-3 to flow seamlessly between sentences and topics. They have nothing to say,", "tokens": [3652, 411, 26039, 51, 12, 18, 281, 3095, 38083, 1296, 16579, 293, 8378, 13, 814, 362, 1825, 281, 584, 11], "temperature": 0.0, "avg_logprob": -0.06218953974106733, "compression_ratio": 1.524390243902439, "no_speech_prob": 0.00010580845992080867}, {"id": 1186, "seek": 837928, "start": 8394.880000000001, "end": 8401.76, "text": " but they say it well. However, they bring us closer to meaningful language generation and", "tokens": [457, 436, 584, 309, 731, 13, 2908, 11, 436, 1565, 505, 4966, 281, 10995, 2856, 5125, 293], "temperature": 0.0, "avg_logprob": -0.06218953974106733, "compression_ratio": 1.524390243902439, "no_speech_prob": 0.00010580845992080867}, {"id": 1187, "seek": 837928, "start": 8401.76, "end": 8408.0, "text": " when we achieve that, the public perception of what computers are capable of will totally change.", "tokens": [562, 321, 4584, 300, 11, 264, 1908, 12860, 295, 437, 10807, 366, 8189, 295, 486, 3879, 1319, 13], "temperature": 0.0, "avg_logprob": -0.06218953974106733, "compression_ratio": 1.524390243902439, "no_speech_prob": 0.00010580845992080867}, {"id": 1188, "seek": 840800, "start": 8408.0, "end": 8415.04, "text": " Most of cognition is recognition. Being able to recognize that something has occurred before", "tokens": [4534, 295, 46905, 307, 850, 2912, 849, 13, 8891, 1075, 281, 5521, 300, 746, 575, 11068, 949], "temperature": 0.0, "avg_logprob": -0.11079216003417969, "compression_ratio": 1.6351931330472103, "no_speech_prob": 0.0001780559978215024}, {"id": 1189, "seek": 840800, "start": 8415.04, "end": 8422.56, "text": " and knowing what might happen next has enormous survival value for any animal species. A mature", "tokens": [293, 5276, 437, 1062, 1051, 958, 575, 11322, 12559, 2158, 337, 604, 5496, 6172, 13, 316, 14442], "temperature": 0.0, "avg_logprob": -0.11079216003417969, "compression_ratio": 1.6351931330472103, "no_speech_prob": 0.0001780559978215024}, {"id": 1190, "seek": 840800, "start": 8422.56, "end": 8429.28, "text": " human has used their eyes and other senses for decades. This represents an enormous learning", "tokens": [1952, 575, 1143, 641, 2575, 293, 661, 17057, 337, 7878, 13, 639, 8855, 364, 11322, 2539], "temperature": 0.0, "avg_logprob": -0.11079216003417969, "compression_ratio": 1.6351931330472103, "no_speech_prob": 0.0001780559978215024}, {"id": 1191, "seek": 840800, "start": 8429.28, "end": 8436.88, "text": " corpus and they can understand anything they have prior experience of. The mistakes made by humans,", "tokens": [1181, 31624, 293, 436, 393, 1223, 1340, 436, 362, 4059, 1752, 295, 13, 440, 8038, 1027, 538, 6255, 11], "temperature": 0.0, "avg_logprob": -0.11079216003417969, "compression_ratio": 1.6351931330472103, "no_speech_prob": 0.0001780559978215024}, {"id": 1192, "seek": 843688, "start": 8436.88, "end": 8443.839999999998, "text": " animals and by holistic ML systems are very often of a near-miss variety which provides an", "tokens": [4882, 293, 538, 30334, 21601, 3652, 366, 588, 2049, 295, 257, 2651, 12, 76, 891, 5673, 597, 6417, 364], "temperature": 0.0, "avg_logprob": -0.08395135259053793, "compression_ratio": 1.5739130434782609, "no_speech_prob": 3.533947528921999e-05}, {"id": 1193, "seek": 843688, "start": 8443.839999999998, "end": 8450.64, "text": " opportunity to learn to do a better next time. Contrast is to reductionist software systems", "tokens": [2650, 281, 1466, 281, 360, 257, 1101, 958, 565, 13, 4839, 4148, 307, 281, 11004, 468, 4722, 3652], "temperature": 0.0, "avg_logprob": -0.08395135259053793, "compression_ratio": 1.5739130434782609, "no_speech_prob": 3.533947528921999e-05}, {"id": 1194, "seek": 843688, "start": 8450.64, "end": 8457.119999999999, "text": " created for similar goals. Rule-based systems have long been infamous for their brittleness.", "tokens": [2942, 337, 2531, 5493, 13, 27533, 12, 6032, 3652, 362, 938, 668, 30769, 337, 641, 738, 593, 45887, 13], "temperature": 0.0, "avg_logprob": -0.08395135259053793, "compression_ratio": 1.5739130434782609, "no_speech_prob": 3.533947528921999e-05}, {"id": 1195, "seek": 843688, "start": 8457.759999999998, "end": 8463.359999999999, "text": " As long as the rules and the rules that match the current input and reality perfectly,", "tokens": [1018, 938, 382, 264, 4474, 293, 264, 4474, 300, 2995, 264, 2190, 4846, 293, 4103, 6239, 11], "temperature": 0.0, "avg_logprob": -0.08395135259053793, "compression_ratio": 1.5739130434782609, "no_speech_prob": 3.533947528921999e-05}, {"id": 1196, "seek": 846336, "start": 8463.36, "end": 8470.32, "text": " the results will be useful, repeatable and reliable. But at the edges of their competence,", "tokens": [264, 3542, 486, 312, 4420, 11, 7149, 712, 293, 12924, 13, 583, 412, 264, 8819, 295, 641, 39965, 11], "temperature": 0.0, "avg_logprob": -0.07187768936157227, "compression_ratio": 1.6136363636363635, "no_speech_prob": 3.9672479033470154e-05}, {"id": 1197, "seek": 846336, "start": 8470.32, "end": 8477.28, "text": " where the matches become more tenuous, the quality rapidly drops. Minor mistakes in the", "tokens": [689, 264, 10676, 1813, 544, 2064, 12549, 11, 264, 3125, 12910, 11438, 13, 36117, 8038, 294, 264], "temperature": 0.0, "avg_logprob": -0.07187768936157227, "compression_ratio": 1.6136363636363635, "no_speech_prob": 3.9672479033470154e-05}, {"id": 1198, "seek": 846336, "start": 8477.28, "end": 8484.16, "text": " rule sets in the world modeling may lead such systems to return spectacularly incorrect results.", "tokens": [4978, 6352, 294, 264, 1002, 15983, 815, 1477, 1270, 3652, 281, 2736, 18149, 356, 18424, 3542, 13], "temperature": 0.0, "avg_logprob": -0.07187768936157227, "compression_ratio": 1.6136363636363635, "no_speech_prob": 3.9672479033470154e-05}, {"id": 1199, "seek": 846336, "start": 8484.880000000001, "end": 8490.400000000001, "text": " Sometimes repeatability is important and sometimes tracking a changing world by", "tokens": [4803, 7149, 2310, 307, 1021, 293, 2171, 11603, 257, 4473, 1002, 538], "temperature": 0.0, "avg_logprob": -0.07187768936157227, "compression_ratio": 1.6136363636363635, "no_speech_prob": 3.9672479033470154e-05}, {"id": 1200, "seek": 849040, "start": 8490.4, "end": 8497.6, "text": " continuously learning more about it is important. In ML, continuous incremental learning makes", "tokens": [15684, 2539, 544, 466, 309, 307, 1021, 13, 682, 21601, 11, 10957, 35759, 2539, 1669], "temperature": 0.0, "avg_logprob": -0.07562132123150403, "compression_ratio": 1.6272727272727272, "no_speech_prob": 4.744579564430751e-05}, {"id": 1201, "seek": 849040, "start": 8497.6, "end": 8504.48, "text": " it possible to stay up to date. If we want repeatability, we can emit a condensed,", "tokens": [309, 1944, 281, 1754, 493, 281, 4002, 13, 759, 321, 528, 7149, 2310, 11, 321, 393, 32084, 257, 36398, 11], "temperature": 0.0, "avg_logprob": -0.07562132123150403, "compression_ratio": 1.6272727272727272, "no_speech_prob": 4.744579564430751e-05}, {"id": 1202, "seek": 849040, "start": 8504.48, "end": 8510.8, "text": " cleaned and frozen competence file from a learner that can be loaded into non-learning,", "tokens": [16146, 293, 12496, 39965, 3991, 490, 257, 33347, 300, 393, 312, 13210, 666, 2107, 12, 47204, 11], "temperature": 0.0, "avg_logprob": -0.07562132123150403, "compression_ratio": 1.6272727272727272, "no_speech_prob": 4.744579564430751e-05}, {"id": 1203, "seek": 849040, "start": 8510.8, "end": 8516.88, "text": " read-only, cloud-based understanding machines that serve the world and provide repeatability", "tokens": [1401, 12, 25202, 11, 4588, 12, 6032, 3701, 8379, 300, 4596, 264, 1002, 293, 2893, 7149, 2310], "temperature": 0.0, "avg_logprob": -0.07562132123150403, "compression_ratio": 1.6272727272727272, "no_speech_prob": 4.744579564430751e-05}, {"id": 1204, "seek": 851688, "start": 8516.88, "end": 8524.16, "text": " between scheduled software and competence releases. Three, in the case of reductionist systems,", "tokens": [1296, 15678, 4722, 293, 39965, 16952, 13, 6244, 11, 294, 264, 1389, 295, 11004, 468, 3652, 11], "temperature": 0.0, "avg_logprob": -0.0692274454163342, "compression_ratio": 1.6144067796610169, "no_speech_prob": 3.13836608256679e-05}, {"id": 1205, "seek": 851688, "start": 8524.16, "end": 8530.48, "text": " such as cell phone OS releases, we are used to getting well-tested new versions with minor bug", "tokens": [1270, 382, 2815, 2593, 12731, 16952, 11, 321, 366, 1143, 281, 1242, 731, 12, 83, 21885, 777, 9606, 365, 6696, 7426], "temperature": 0.0, "avg_logprob": -0.0692274454163342, "compression_ratio": 1.6144067796610169, "no_speech_prob": 3.13836608256679e-05}, {"id": 1206, "seek": 851688, "start": 8530.48, "end": 8538.08, "text": " fixes and occasional major features at regular intervals. Such systems learn only in the sense", "tokens": [32539, 293, 31644, 2563, 4122, 412, 3890, 26651, 13, 9653, 3652, 1466, 787, 294, 264, 2020], "temperature": 0.0, "avg_logprob": -0.0692274454163342, "compression_ratio": 1.6144067796610169, "no_speech_prob": 3.13836608256679e-05}, {"id": 1207, "seek": 851688, "start": 8538.08, "end": 8543.519999999999, "text": " that the people who created them have learned more and put these insights into the new release.", "tokens": [300, 264, 561, 567, 2942, 552, 362, 3264, 544, 293, 829, 613, 14310, 666, 264, 777, 4374, 13], "temperature": 0.0, "avg_logprob": -0.0692274454163342, "compression_ratio": 1.6144067796610169, "no_speech_prob": 3.13836608256679e-05}, {"id": 1208, "seek": 854352, "start": 8543.52, "end": 8549.76, "text": " Reductionist systems working with complete incorrect input data are expected to provide", "tokens": [4477, 27549, 468, 3652, 1364, 365, 3566, 18424, 4846, 1412, 366, 5176, 281, 2893], "temperature": 0.0, "avg_logprob": -0.09429752010188691, "compression_ratio": 1.7860696517412935, "no_speech_prob": 4.734303001896478e-05}, {"id": 1209, "seek": 854352, "start": 8549.76, "end": 8554.720000000001, "text": " correct and repeatable results according to the implementation of the algorithm.", "tokens": [3006, 293, 7149, 712, 3542, 4650, 281, 264, 11420, 295, 264, 9284, 13], "temperature": 0.0, "avg_logprob": -0.09429752010188691, "compression_ratio": 1.7860696517412935, "no_speech_prob": 4.734303001896478e-05}, {"id": 1210, "seek": 854352, "start": 8555.36, "end": 8562.08, "text": " But both the algorithm and the implementation may have errors. If the algorithm does not adequately", "tokens": [583, 1293, 264, 9284, 293, 264, 11420, 815, 362, 13603, 13, 759, 264, 9284, 775, 406, 41822], "temperature": 0.0, "avg_logprob": -0.09429752010188691, "compression_ratio": 1.7860696517412935, "no_speech_prob": 4.734303001896478e-05}, {"id": 1211, "seek": 854352, "start": 8562.08, "end": 8569.36, "text": " model its reality, then we have reduction errors. In the implementation, we may have bugs.", "tokens": [2316, 1080, 4103, 11, 550, 321, 362, 11004, 13603, 13, 682, 264, 11420, 11, 321, 815, 362, 15120, 13], "temperature": 0.0, "avg_logprob": -0.09429752010188691, "compression_ratio": 1.7860696517412935, "no_speech_prob": 4.734303001896478e-05}, {"id": 1212, "seek": 856936, "start": 8569.36, "end": 8574.960000000001, "text": " Holistic software systems can be designed to a different standard of correctness.", "tokens": [11086, 3142, 4722, 3652, 393, 312, 4761, 281, 257, 819, 3832, 295, 3006, 1287, 13], "temperature": 0.0, "avg_logprob": -0.12090933812807685, "compression_ratio": 1.588785046728972, "no_speech_prob": 5.1074628572678193e-05}, {"id": 1213, "seek": 856936, "start": 8575.68, "end": 8582.32, "text": " Since input data is normally incomplete and noisy, and results are based on emergent effects,", "tokens": [4162, 4846, 1412, 307, 5646, 31709, 293, 24518, 11, 293, 3542, 366, 2361, 322, 4345, 6930, 5065, 11], "temperature": 0.0, "avg_logprob": -0.12090933812807685, "compression_ratio": 1.588785046728972, "no_speech_prob": 5.1074628572678193e-05}, {"id": 1214, "seek": 856936, "start": 8582.32, "end": 8588.08, "text": " we can expect similar enough results even if parts of the system have been damaged,", "tokens": [321, 393, 2066, 2531, 1547, 3542, 754, 498, 3166, 295, 264, 1185, 362, 668, 14080, 11], "temperature": 0.0, "avg_logprob": -0.12090933812807685, "compression_ratio": 1.588785046728972, "no_speech_prob": 5.1074628572678193e-05}, {"id": 1215, "seek": 856936, "start": 8588.08, "end": 8594.0, "text": " for instance by catastrophic forgetting. Holistic systems can be made capable of", "tokens": [337, 5197, 538, 34915, 25428, 13, 11086, 3142, 3652, 393, 312, 1027, 8189, 295], "temperature": 0.0, "avg_logprob": -0.12090933812807685, "compression_ratio": 1.588785046728972, "no_speech_prob": 5.1074628572678193e-05}, {"id": 1216, "seek": 859400, "start": 8594.0, "end": 8600.16, "text": " self-repair using incremental learning. This has been observed in the deep learning community.", "tokens": [2698, 12, 19919, 1246, 1228, 35759, 2539, 13, 639, 575, 668, 13095, 294, 264, 2452, 2539, 1768, 13], "temperature": 0.0, "avg_logprob": -0.06993967098194165, "compression_ratio": 1.7016129032258065, "no_speech_prob": 5.107931065140292e-05}, {"id": 1217, "seek": 859400, "start": 8600.8, "end": 8605.6, "text": " Another technique is that when using multiple parallel threads in learning,", "tokens": [3996, 6532, 307, 300, 562, 1228, 3866, 8952, 19314, 294, 2539, 11], "temperature": 0.0, "avg_logprob": -0.06993967098194165, "compression_ratio": 1.7016129032258065, "no_speech_prob": 5.107931065140292e-05}, {"id": 1218, "seek": 859400, "start": 8605.6, "end": 8610.08, "text": " there may be conflicts that would normally require locking of some values.", "tokens": [456, 815, 312, 19807, 300, 576, 5646, 3651, 23954, 295, 512, 4190, 13], "temperature": 0.0, "avg_logprob": -0.06993967098194165, "compression_ratio": 1.7016129032258065, "no_speech_prob": 5.107931065140292e-05}, {"id": 1219, "seek": 859400, "start": 8610.8, "end": 8616.24, "text": " But if the operations are simple enough, such as just incrementing a value,", "tokens": [583, 498, 264, 7705, 366, 2199, 1547, 11, 1270, 382, 445, 26200, 278, 257, 2158, 11], "temperature": 0.0, "avg_logprob": -0.06993967098194165, "compression_ratio": 1.7016129032258065, "no_speech_prob": 5.107931065140292e-05}, {"id": 1220, "seek": 859400, "start": 8616.24, "end": 8622.24, "text": " we can forego thread safety in the locking since the worst outcome is the loss of a single increment", "tokens": [321, 393, 2091, 1571, 7207, 4514, 294, 264, 23954, 1670, 264, 5855, 9700, 307, 264, 4470, 295, 257, 2167, 26200], "temperature": 0.0, "avg_logprob": -0.06993967098194165, "compression_ratio": 1.7016129032258065, "no_speech_prob": 5.107931065140292e-05}, {"id": 1221, "seek": 862224, "start": 8622.24, "end": 8628.56, "text": " in a system that uses emergent results from millions of such values. And the mistake would,", "tokens": [294, 257, 1185, 300, 4960, 4345, 6930, 3542, 490, 6803, 295, 1270, 4190, 13, 400, 264, 6146, 576, 11], "temperature": 0.0, "avg_logprob": -0.042504077429299826, "compression_ratio": 1.5905172413793103, "no_speech_prob": 3.657274282886647e-05}, {"id": 1222, "seek": 862224, "start": 8628.56, "end": 8635.6, "text": " in a well-designed system, be self-correcting in the long run. At the cloud level, absolute", "tokens": [294, 257, 731, 12, 14792, 16690, 1185, 11, 312, 2698, 12, 19558, 2554, 278, 294, 264, 938, 1190, 13, 1711, 264, 4588, 1496, 11, 8236], "temperature": 0.0, "avg_logprob": -0.042504077429299826, "compression_ratio": 1.5905172413793103, "no_speech_prob": 3.657274282886647e-05}, {"id": 1223, "seek": 862224, "start": 8635.6, "end": 8642.64, "text": " consistency may not be as hard a requirement as it is for reductionist systems. Much larger", "tokens": [14416, 815, 406, 312, 382, 1152, 257, 11695, 382, 309, 307, 337, 11004, 468, 3652, 13, 12313, 4833], "temperature": 0.0, "avg_logprob": -0.042504077429299826, "compression_ratio": 1.5905172413793103, "no_speech_prob": 3.657274282886647e-05}, {"id": 1224, "seek": 862224, "start": 8642.64, "end": 8648.88, "text": " mistakes can be expected to be attributable to misunderstandings of the corpus or poor corpus", "tokens": [8038, 393, 312, 5176, 281, 312, 9080, 32148, 281, 35736, 1109, 295, 264, 1181, 31624, 420, 4716, 1181, 31624], "temperature": 0.0, "avg_logprob": -0.042504077429299826, "compression_ratio": 1.5905172413793103, "no_speech_prob": 3.657274282886647e-05}, {"id": 1225, "seek": 864888, "start": 8648.88, "end": 8657.279999999999, "text": " coverage. General strategies, decomposition into smaller problems, versus generalization", "tokens": [9645, 13, 6996, 9029, 11, 48356, 666, 4356, 2740, 11, 5717, 2674, 2144], "temperature": 0.0, "avg_logprob": -0.11952108707068101, "compression_ratio": 1.7375, "no_speech_prob": 0.00022506515961140394}, {"id": 1226, "seek": 864888, "start": 8657.279999999999, "end": 8664.72, "text": " may lead to an easier problem. Assuming discards everything irrelevant based on how new information", "tokens": [815, 1477, 281, 364, 3571, 1154, 13, 6281, 24919, 2983, 2287, 1203, 28682, 2361, 322, 577, 777, 1589], "temperature": 0.0, "avg_logprob": -0.11952108707068101, "compression_ratio": 1.7375, "no_speech_prob": 0.00022506515961140394}, {"id": 1227, "seek": 864888, "start": 8664.72, "end": 8670.88, "text": " matches existing experience, versus a machine discards everything irrelevant based on how", "tokens": [10676, 6741, 1752, 11, 5717, 257, 3479, 2983, 2287, 1203, 28682, 2361, 322, 577], "temperature": 0.0, "avg_logprob": -0.11952108707068101, "compression_ratio": 1.7375, "no_speech_prob": 0.00022506515961140394}, {"id": 1228, "seek": 867088, "start": 8670.88, "end": 8681.199999999999, "text": " new information matches existing experience, modularity, versus composability, gather valid,", "tokens": [777, 1589, 10676, 6741, 1752, 11, 31111, 507, 11, 5717, 10199, 2310, 11, 5448, 7363, 11], "temperature": 0.0, "avg_logprob": -0.12433602809906005, "compression_ratio": 1.6923076923076923, "no_speech_prob": 5.1746774261118844e-05}, {"id": 1229, "seek": 867088, "start": 8681.199999999999, "end": 8688.64, "text": " correct, and complete input data, versus use whatever information is available, and use all", "tokens": [3006, 11, 293, 3566, 4846, 1412, 11, 5717, 764, 2035, 1589, 307, 2435, 11, 293, 764, 439], "temperature": 0.0, "avg_logprob": -0.12433602809906005, "compression_ratio": 1.6923076923076923, "no_speech_prob": 5.1746774261118844e-05}, {"id": 1230, "seek": 868864, "start": 8688.64, "end": 8701.599999999999, "text": " of it. Formal, rigorous methods, versus informal ad hoc methods, absolute control, versus creativity,", "tokens": [295, 309, 13, 10126, 304, 11, 29882, 7150, 11, 5717, 24342, 614, 16708, 7150, 11, 8236, 1969, 11, 5717, 12915, 11], "temperature": 0.0, "avg_logprob": -0.1260650519168738, "compression_ratio": 1.6021505376344085, "no_speech_prob": 7.254345837282017e-05}, {"id": 1231, "seek": 868864, "start": 8701.599999999999, "end": 8710.16, "text": " intelligent design, versus evolution. The reductionist battle cries, the whole is equal to the sum", "tokens": [13232, 1715, 11, 5717, 9303, 13, 440, 11004, 468, 4635, 29206, 11, 264, 1379, 307, 2681, 281, 264, 2408], "temperature": 0.0, "avg_logprob": -0.1260650519168738, "compression_ratio": 1.6021505376344085, "no_speech_prob": 7.254345837282017e-05}, {"id": 1232, "seek": 868864, "start": 8710.16, "end": 8717.119999999999, "text": " of its parts, which gives us a license to split a large complicated problem into smaller problems", "tokens": [295, 1080, 3166, 11, 597, 2709, 505, 257, 10476, 281, 7472, 257, 2416, 6179, 1154, 666, 4356, 2740], "temperature": 0.0, "avg_logprob": -0.1260650519168738, "compression_ratio": 1.6021505376344085, "no_speech_prob": 7.254345837282017e-05}, {"id": 1233, "seek": 871712, "start": 8717.12, "end": 8723.6, "text": " to solve each of those using some suitable model, and then to combine all the sub-solutions", "tokens": [281, 5039, 1184, 295, 729, 1228, 512, 12873, 2316, 11, 293, 550, 281, 10432, 439, 264, 1422, 12, 82, 15892], "temperature": 0.0, "avg_logprob": -0.08335261286040883, "compression_ratio": 1.6157205240174672, "no_speech_prob": 7.214637298602611e-05}, {"id": 1234, "seek": 871712, "start": 8723.6, "end": 8731.12, "text": " into a model-based solution for the original, larger problem, such as in moonshots, highway", "tokens": [666, 257, 2316, 12, 6032, 3827, 337, 264, 3380, 11, 4833, 1154, 11, 1270, 382, 294, 34139, 27495, 11, 17205], "temperature": 0.0, "avg_logprob": -0.08335261286040883, "compression_ratio": 1.6157205240174672, "no_speech_prob": 7.214637298602611e-05}, {"id": 1235, "seek": 871712, "start": 8731.12, "end": 8738.640000000001, "text": " systems, international banking, and generally in industrial intelligent design. This works", "tokens": [3652, 11, 5058, 18261, 11, 293, 5101, 294, 9987, 13232, 1715, 13, 639, 1985], "temperature": 0.0, "avg_logprob": -0.08335261286040883, "compression_ratio": 1.6157205240174672, "no_speech_prob": 7.214637298602611e-05}, {"id": 1236, "seek": 871712, "start": 8738.640000000001, "end": 8745.52, "text": " in simple and some complicated domains, but cannot be done in complex domains, where everything", "tokens": [294, 2199, 293, 512, 6179, 25514, 11, 457, 2644, 312, 1096, 294, 3997, 25514, 11, 689, 1203], "temperature": 0.0, "avg_logprob": -0.08335261286040883, "compression_ratio": 1.6157205240174672, "no_speech_prob": 7.214637298602611e-05}, {"id": 1237, "seek": 874552, "start": 8745.52, "end": 8752.24, "text": " potentially affects everything else. Spreading a complex system may cause any emergent effects", "tokens": [7263, 11807, 1203, 1646, 13, 30308, 278, 257, 3997, 1185, 815, 3082, 604, 4345, 6930, 5065], "temperature": 0.0, "avg_logprob": -0.11626775641190379, "compression_ratio": 1.5159574468085106, "no_speech_prob": 7.74226282374002e-05}, {"id": 1238, "seek": 874552, "start": 8752.24, "end": 8760.960000000001, "text": " to disappear, confounding analysis. Examples of complex problem domains are politics, neuroscience,", "tokens": [281, 11596, 11, 1497, 24625, 5215, 13, 48591, 295, 3997, 1154, 25514, 366, 7341, 11, 42762, 11], "temperature": 0.0, "avg_logprob": -0.11626775641190379, "compression_ratio": 1.5159574468085106, "no_speech_prob": 7.74226282374002e-05}, {"id": 1239, "seek": 874552, "start": 8760.960000000001, "end": 8769.2, "text": " ecology, economy, including stock markets, and cellular biology. Our life sciences operate", "tokens": [39683, 11, 5010, 11, 3009, 4127, 8383, 11, 293, 29267, 14956, 13, 2621, 993, 17677, 9651], "temperature": 0.0, "avg_logprob": -0.11626775641190379, "compression_ratio": 1.5159574468085106, "no_speech_prob": 7.74226282374002e-05}, {"id": 1240, "seek": 876920, "start": 8769.2, "end": 8777.52, "text": " in a complex problem domain because life itself is complex. Some say biology has physics envy,", "tokens": [294, 257, 3997, 1154, 9274, 570, 993, 2564, 307, 3997, 13, 2188, 584, 14956, 575, 10649, 30530, 11], "temperature": 0.0, "avg_logprob": -0.045693032535505884, "compression_ratio": 1.6085106382978724, "no_speech_prob": 2.3071354007697664e-05}, {"id": 1241, "seek": 876920, "start": 8777.52, "end": 8783.28, "text": " because in the life sciences, reductionist models are difficult to create and justify.", "tokens": [570, 294, 264, 993, 17677, 11, 11004, 468, 5245, 366, 2252, 281, 1884, 293, 20833, 13], "temperature": 0.0, "avg_logprob": -0.045693032535505884, "compression_ratio": 1.6085106382978724, "no_speech_prob": 2.3071354007697664e-05}, {"id": 1242, "seek": 876920, "start": 8784.0, "end": 8791.52, "text": " On the other hand, physics is for simple problems. Problems with many complex interdependencies and", "tokens": [1282, 264, 661, 1011, 11, 10649, 307, 337, 2199, 2740, 13, 11676, 82, 365, 867, 3997, 728, 36763, 6464, 293], "temperature": 0.0, "avg_logprob": -0.045693032535505884, "compression_ratio": 1.6085106382978724, "no_speech_prob": 2.3071354007697664e-05}, {"id": 1243, "seek": 876920, "start": 8791.52, "end": 8798.640000000001, "text": " unknown webs of causality can now be attacked using deep neural networks. These systems discover", "tokens": [9841, 2859, 295, 3302, 1860, 393, 586, 312, 12692, 1228, 2452, 18161, 9590, 13, 1981, 3652, 4411], "temperature": 0.0, "avg_logprob": -0.045693032535505884, "compression_ratio": 1.6085106382978724, "no_speech_prob": 2.3071354007697664e-05}, {"id": 1244, "seek": 879864, "start": 8798.64, "end": 8804.72, "text": " useful correlations and may often find solutions using mere hints in the input which match their", "tokens": [4420, 13983, 763, 293, 815, 2049, 915, 6547, 1228, 8401, 27271, 294, 264, 4846, 597, 2995, 641], "temperature": 0.0, "avg_logprob": -0.048601687708987464, "compression_ratio": 1.6130434782608696, "no_speech_prob": 2.4559340090490878e-05}, {"id": 1245, "seek": 879864, "start": 8804.72, "end": 8810.4, "text": " prior experience. Reductionist strategies with correctness requirements outlaw this.", "tokens": [4059, 1752, 13, 4477, 27549, 468, 9029, 365, 3006, 1287, 7728, 484, 5901, 341, 13], "temperature": 0.0, "avg_logprob": -0.048601687708987464, "compression_ratio": 1.6130434782608696, "no_speech_prob": 2.4559340090490878e-05}, {"id": 1246, "seek": 879864, "start": 8811.039999999999, "end": 8817.519999999999, "text": " It is notable that one of the larger triumphs of holistic methods is protein folding, which is a", "tokens": [467, 307, 22556, 300, 472, 295, 264, 4833, 29156, 82, 295, 30334, 7150, 307, 7944, 25335, 11, 597, 307, 257], "temperature": 0.0, "avg_logprob": -0.048601687708987464, "compression_ratio": 1.6130434782608696, "no_speech_prob": 2.4559340090490878e-05}, {"id": 1247, "seek": 879864, "start": 8817.519999999999, "end": 8824.4, "text": " problem at the very core of the life sciences. So holistic understanding of a complex system", "tokens": [1154, 412, 264, 588, 4965, 295, 264, 993, 17677, 13, 407, 30334, 3701, 295, 257, 3997, 1185], "temperature": 0.0, "avg_logprob": -0.048601687708987464, "compression_ratio": 1.6130434782608696, "no_speech_prob": 2.4559340090490878e-05}, {"id": 1248, "seek": 882440, "start": 8824.4, "end": 8831.119999999999, "text": " can be acquired by observing it over time and learning from its behavior. There is no need to", "tokens": [393, 312, 17554, 538, 22107, 309, 670, 565, 293, 2539, 490, 1080, 5223, 13, 821, 307, 572, 643, 281], "temperature": 0.0, "avg_logprob": -0.09646521387873469, "compression_ratio": 1.5185185185185186, "no_speech_prob": 5.852707909070887e-05}, {"id": 1249, "seek": 882440, "start": 8831.119999999999, "end": 8838.48, "text": " split the problem into pieces. Part of the holistic stance is that we give the machine everything.", "tokens": [7472, 264, 1154, 666, 3755, 13, 4100, 295, 264, 30334, 21033, 307, 300, 321, 976, 264, 3479, 1203, 13], "temperature": 0.0, "avg_logprob": -0.09646521387873469, "compression_ratio": 1.5185185185185186, "no_speech_prob": 5.852707909070887e-05}, {"id": 1250, "seek": 882440, "start": 8838.48, "end": 8847.68, "text": " Holism comes from the Greek word, holos, amicron, lambda, amicron, sigma, in the written text.", "tokens": [11086, 1434, 1487, 490, 264, 10281, 1349, 11, 4091, 329, 11, 669, 299, 2044, 11, 13607, 11, 669, 299, 2044, 11, 12771, 11, 294, 264, 3720, 2487, 13], "temperature": 0.0, "avg_logprob": -0.09646521387873469, "compression_ratio": 1.5185185185185186, "no_speech_prob": 5.852707909070887e-05}, {"id": 1251, "seek": 884768, "start": 8847.68, "end": 8855.84, "text": " The whole, that is to say, all the information we have. If we start filtering the input data,", "tokens": [440, 1379, 11, 300, 307, 281, 584, 11, 439, 264, 1589, 321, 362, 13, 759, 321, 722, 30822, 264, 4846, 1412, 11], "temperature": 0.0, "avg_logprob": -0.08195043451645795, "compression_ratio": 1.639269406392694, "no_speech_prob": 3.727355942828581e-05}, {"id": 1252, "seek": 884768, "start": 8855.84, "end": 8862.4, "text": " by cleaning it up, then the system will effectively learn from a polyana version of the world,", "tokens": [538, 8924, 309, 493, 11, 550, 264, 1185, 486, 8659, 1466, 490, 257, 6754, 2095, 3037, 295, 264, 1002, 11], "temperature": 0.0, "avg_logprob": -0.08195043451645795, "compression_ratio": 1.639269406392694, "no_speech_prob": 3.727355942828581e-05}, {"id": 1253, "seek": 884768, "start": 8862.4, "end": 8868.4, "text": " which will be confusing once it has to deal with real life inputs in a production environment.", "tokens": [597, 486, 312, 13181, 1564, 309, 575, 281, 2028, 365, 957, 993, 15743, 294, 257, 4265, 2823, 13], "temperature": 0.0, "avg_logprob": -0.08195043451645795, "compression_ratio": 1.639269406392694, "no_speech_prob": 3.727355942828581e-05}, {"id": 1254, "seek": 884768, "start": 8868.960000000001, "end": 8874.32, "text": " If we want our machines to learn to understand the world all by themselves,", "tokens": [759, 321, 528, 527, 8379, 281, 1466, 281, 1223, 264, 1002, 439, 538, 2969, 11], "temperature": 0.0, "avg_logprob": -0.08195043451645795, "compression_ratio": 1.639269406392694, "no_speech_prob": 3.727355942828581e-05}, {"id": 1255, "seek": 887432, "start": 8874.32, "end": 8880.72, "text": " then we should not start by applying heavy-handed heuristic cleanup operations of our own design", "tokens": [550, 321, 820, 406, 722, 538, 9275, 4676, 12, 25407, 415, 374, 3142, 40991, 7705, 295, 527, 1065, 1715], "temperature": 0.0, "avg_logprob": -0.06332581294210334, "compression_ratio": 1.5789473684210527, "no_speech_prob": 2.1188530809013173e-05}, {"id": 1256, "seek": 887432, "start": 8880.72, "end": 8888.08, "text": " on their input data. Sometimes, reductionist strategies are clearly inferior. The natural", "tokens": [322, 641, 4846, 1412, 13, 4803, 11, 11004, 468, 9029, 366, 4448, 24249, 13, 440, 3303], "temperature": 0.0, "avg_logprob": -0.06332581294210334, "compression_ratio": 1.5789473684210527, "no_speech_prob": 2.1188530809013173e-05}, {"id": 1257, "seek": 887432, "start": 8888.08, "end": 8894.08, "text": " language understanding is such a domain. Language understanding in a fluent speaker", "tokens": [2856, 3701, 307, 1270, 257, 9274, 13, 24445, 3701, 294, 257, 40799, 8145], "temperature": 0.0, "avg_logprob": -0.06332581294210334, "compression_ratio": 1.5789473684210527, "no_speech_prob": 2.1188530809013173e-05}, {"id": 1258, "seek": 887432, "start": 8894.08, "end": 8901.84, "text": " is almost 100% holistic because it is almost entirely based on prior exposure. We are now", "tokens": [307, 1920, 2319, 4, 30334, 570, 309, 307, 1920, 7696, 2361, 322, 4059, 10420, 13, 492, 366, 586], "temperature": 0.0, "avg_logprob": -0.06332581294210334, "compression_ratio": 1.5789473684210527, "no_speech_prob": 2.1188530809013173e-05}, {"id": 1259, "seek": 890184, "start": 8901.84, "end": 8907.76, "text": " finding out that it is much easier to build a machine to learn any language on the planet from", "tokens": [5006, 484, 300, 309, 307, 709, 3571, 281, 1322, 257, 3479, 281, 1466, 604, 2856, 322, 264, 5054, 490], "temperature": 0.0, "avg_logprob": -0.07043186048182046, "compression_ratio": 1.6160337552742616, "no_speech_prob": 4.897238250123337e-05}, {"id": 1260, "seek": 890184, "start": 8907.76, "end": 8914.56, "text": " scratch than it is to build a good old-fashioned artificial intelligence, 20th century reductionist", "tokens": [8459, 813, 309, 307, 281, 1322, 257, 665, 1331, 12, 37998, 11677, 7599, 11, 945, 392, 4901, 11004, 468], "temperature": 0.0, "avg_logprob": -0.07043186048182046, "compression_ratio": 1.6160337552742616, "no_speech_prob": 4.897238250123337e-05}, {"id": 1261, "seek": 890184, "start": 8914.56, "end": 8921.52, "text": " AI-based style machine that understands a single language such as English. The process where a", "tokens": [7318, 12, 6032, 3758, 3479, 300, 15146, 257, 2167, 2856, 1270, 382, 3669, 13, 440, 1399, 689, 257], "temperature": 0.0, "avg_logprob": -0.07043186048182046, "compression_ratio": 1.6160337552742616, "no_speech_prob": 4.897238250123337e-05}, {"id": 1262, "seek": 890184, "start": 8921.52, "end": 8928.08, "text": " human, by using their understanding, discards everything irrelevant to arrive at what matters", "tokens": [1952, 11, 538, 1228, 641, 3701, 11, 2983, 2287, 1203, 28682, 281, 8881, 412, 437, 7001], "temperature": 0.0, "avg_logprob": -0.07043186048182046, "compression_ratio": 1.6160337552742616, "no_speech_prob": 4.897238250123337e-05}, {"id": 1263, "seek": 892808, "start": 8928.08, "end": 8934.24, "text": " is called the epistemic reduction and is discussed in the first five chapters in this book.", "tokens": [307, 1219, 264, 2388, 468, 3438, 11004, 293, 307, 7152, 294, 264, 700, 1732, 20013, 294, 341, 1446, 13], "temperature": 0.0, "avg_logprob": -0.07343224273330864, "compression_ratio": 1.670940170940171, "no_speech_prob": 0.0001841499615693465}, {"id": 1264, "seek": 892808, "start": 8934.96, "end": 8941.76, "text": " This is the most important operation in reductionism, but for some reason discussions of reductionism", "tokens": [639, 307, 264, 881, 1021, 6916, 294, 11004, 1434, 11, 457, 337, 512, 1778, 11088, 295, 11004, 1434], "temperature": 0.0, "avg_logprob": -0.07343224273330864, "compression_ratio": 1.670940170940171, "no_speech_prob": 0.0001841499615693465}, {"id": 1265, "seek": 892808, "start": 8941.76, "end": 8949.92, "text": " in the past have tended to focus on other aspects. Perhaps this is a new result. ML systems discard", "tokens": [294, 264, 1791, 362, 34732, 281, 1879, 322, 661, 7270, 13, 10517, 341, 307, 257, 777, 1874, 13, 21601, 3652, 31597], "temperature": 0.0, "avg_logprob": -0.07343224273330864, "compression_ratio": 1.670940170940171, "no_speech_prob": 0.0001841499615693465}, {"id": 1266, "seek": 892808, "start": 8949.92, "end": 8956.56, "text": " with little fanfare anything that was expected and that has been seen before as boring, harmless,", "tokens": [365, 707, 3429, 11079, 1340, 300, 390, 5176, 293, 300, 575, 668, 1612, 949, 382, 9989, 11, 40160, 11], "temperature": 0.0, "avg_logprob": -0.07343224273330864, "compression_ratio": 1.670940170940171, "no_speech_prob": 0.0001841499615693465}, {"id": 1267, "seek": 895656, "start": 8956.56, "end": 8963.519999999999, "text": " or otherwise ignorable. They may also discard things significantly outside of their experience", "tokens": [420, 5911, 14698, 712, 13, 814, 815, 611, 31597, 721, 10591, 2380, 295, 641, 1752], "temperature": 0.0, "avg_logprob": -0.08532034158706665, "compression_ratio": 1.6995515695067265, "no_speech_prob": 4.124994302401319e-05}, {"id": 1268, "seek": 895656, "start": 8963.519999999999, "end": 8970.16, "text": " as noise. Things can only be reduced away at the semantic level. They can be recognized that", "tokens": [382, 5658, 13, 9514, 393, 787, 312, 9212, 1314, 412, 264, 47982, 1496, 13, 814, 393, 312, 9823, 300], "temperature": 0.0, "avg_logprob": -0.08532034158706665, "compression_ratio": 1.6995515695067265, "no_speech_prob": 4.124994302401319e-05}, {"id": 1269, "seek": 895656, "start": 8970.88, "end": 8977.199999999999, "text": " operations capable of epistemic reduction at multiple layers discard anything that's understood", "tokens": [7705, 8189, 295, 2388, 468, 3438, 11004, 412, 3866, 7914, 31597, 1340, 300, 311, 7320], "temperature": 0.0, "avg_logprob": -0.08532034158706665, "compression_ratio": 1.6995515695067265, "no_speech_prob": 4.124994302401319e-05}, {"id": 1270, "seek": 895656, "start": 8977.199999999999, "end": 8983.76, "text": " at that layer, and they may pass on upward to the next higher semantic layer, a summary of what", "tokens": [412, 300, 4583, 11, 293, 436, 815, 1320, 322, 23452, 281, 264, 958, 2946, 47982, 4583, 11, 257, 12691, 295, 437], "temperature": 0.0, "avg_logprob": -0.08532034158706665, "compression_ratio": 1.6995515695067265, "no_speech_prob": 4.124994302401319e-05}, {"id": 1271, "seek": 898376, "start": 8983.76, "end": 8990.4, "text": " they discarded plus everything they did not understand at their level. Empire levels do the", "tokens": [436, 45469, 1804, 1203, 436, 630, 406, 1223, 412, 641, 1496, 13, 12197, 4358, 360, 264], "temperature": 0.0, "avg_logprob": -0.09053796376937474, "compression_ratio": 1.6167400881057268, "no_speech_prob": 3.63402723451145e-05}, {"id": 1272, "seek": 898376, "start": 8990.4, "end": 8997.52, "text": " same. This is why deep learning is deep. Intelligently designed systems are often made up out of", "tokens": [912, 13, 639, 307, 983, 2452, 2539, 307, 2452, 13, 18762, 328, 2276, 4761, 3652, 366, 2049, 1027, 493, 484, 295], "temperature": 0.0, "avg_logprob": -0.09053796376937474, "compression_ratio": 1.6167400881057268, "no_speech_prob": 3.63402723451145e-05}, {"id": 1273, "seek": 898376, "start": 8997.52, "end": 9004.16, "text": " interchangeable modules, which allow for easy replacement in case of failure, and in some", "tokens": [30358, 712, 16679, 11, 597, 2089, 337, 1858, 14419, 294, 1389, 295, 7763, 11, 293, 294, 512], "temperature": 0.0, "avg_logprob": -0.09053796376937474, "compression_ratio": 1.6167400881057268, "no_speech_prob": 3.63402723451145e-05}, {"id": 1274, "seek": 898376, "start": 9004.16, "end": 9010.960000000001, "text": " cases, and especially in software, allow for customization of functionality by replacing", "tokens": [3331, 11, 293, 2318, 294, 4722, 11, 2089, 337, 39387, 295, 14980, 538, 19139], "temperature": 0.0, "avg_logprob": -0.09053796376937474, "compression_ratio": 1.6167400881057268, "no_speech_prob": 3.63402723451145e-05}, {"id": 1275, "seek": 901096, "start": 9010.96, "end": 9017.279999999999, "text": " or adding modules. These modules have well specified interfaces that allow for such", "tokens": [420, 5127, 16679, 13, 1981, 16679, 362, 731, 22206, 28416, 300, 2089, 337, 1270], "temperature": 0.0, "avg_logprob": -0.09235627409340678, "compression_ratio": 1.6097560975609757, "no_speech_prob": 4.9678947107167915e-05}, {"id": 1276, "seek": 901096, "start": 9017.279999999999, "end": 9024.24, "text": " interconnections. In the holistic case we can consider a human cell with thousands of proteins", "tokens": [26253, 626, 13, 682, 264, 30334, 1389, 321, 393, 1949, 257, 1952, 2815, 365, 5383, 295, 15577], "temperature": 0.0, "avg_logprob": -0.09235627409340678, "compression_ratio": 1.6097560975609757, "no_speech_prob": 4.9678947107167915e-05}, {"id": 1277, "seek": 901096, "start": 9024.24, "end": 9031.119999999999, "text": " interact on contact or as required with many substances floating around in the cellular fluid.", "tokens": [4648, 322, 3385, 420, 382, 4739, 365, 867, 25455, 12607, 926, 294, 264, 29267, 9113, 13], "temperature": 0.0, "avg_logprob": -0.09235627409340678, "compression_ratio": 1.6097560975609757, "no_speech_prob": 4.9678947107167915e-05}, {"id": 1278, "seek": 901096, "start": 9031.679999999998, "end": 9035.679999999998, "text": " It is not the result of intelligent design, and it shows", "tokens": [467, 307, 406, 264, 1874, 295, 13232, 1715, 11, 293, 309, 3110], "temperature": 0.0, "avg_logprob": -0.09235627409340678, "compression_ratio": 1.6097560975609757, "no_speech_prob": 4.9678947107167915e-05}, {"id": 1279, "seek": 903568, "start": 9035.68, "end": 9042.0, "text": " there are overlaps and redundancies that may contribute to more reliable operation, and there", "tokens": [456, 366, 15986, 2382, 293, 27830, 32286, 300, 815, 10586, 281, 544, 12924, 6916, 11, 293, 456], "temperature": 0.0, "avg_logprob": -0.17612496614456177, "compression_ratio": 1.7022222222222223, "no_speech_prob": 9.354840585729107e-05}, {"id": 1280, "seek": 903568, "start": 9042.0, "end": 9049.68, "text": " are multiple potentially complex mechanisms keeping each other in check, or we can consider music,", "tokens": [366, 3866, 7263, 3997, 15902, 5145, 1184, 661, 294, 1520, 11, 420, 321, 393, 1949, 1318, 11], "temperature": 0.0, "avg_logprob": -0.17612496614456177, "compression_ratio": 1.7022222222222223, "no_speech_prob": 9.354840585729107e-05}, {"id": 1281, "seek": 903568, "start": 9049.68, "end": 9056.0, "text": " or multiple notes in accord in different timbres in a symphony orchestra in a composition will", "tokens": [420, 3866, 5570, 294, 18640, 294, 819, 524, 46823, 294, 257, 6697, 28616, 25280, 294, 257, 12686, 486], "temperature": 0.0, "avg_logprob": -0.17612496614456177, "compression_ratio": 1.7022222222222223, "no_speech_prob": 9.354840585729107e-05}, {"id": 1282, "seek": 903568, "start": 9056.0, "end": 9062.64, "text": " conjure an emerging harmonic whole that sounds different than the sum of its parts, or consider", "tokens": [20295, 540, 364, 14989, 32270, 1379, 300, 3263, 819, 813, 264, 2408, 295, 1080, 3166, 11, 420, 1949], "temperature": 0.0, "avg_logprob": -0.17612496614456177, "compression_ratio": 1.7022222222222223, "no_speech_prob": 9.354840585729107e-05}, {"id": 1283, "seek": 906264, "start": 9062.64, "end": 9070.56, "text": " spices in a soup, or opinions in a meeting that leads to a consensus. The word, composability,", "tokens": [19608, 294, 257, 7884, 11, 420, 11819, 294, 257, 3440, 300, 6689, 281, 257, 19115, 13, 440, 1349, 11, 10199, 2310, 11], "temperature": 0.0, "avg_logprob": -0.08722956601311178, "compression_ratio": 1.6106194690265487, "no_speech_prob": 3.535712312441319e-05}, {"id": 1284, "seek": 906264, "start": 9070.56, "end": 9077.439999999999, "text": " fits this capability in the holistic case. Unfortunately, in much literature it is merely", "tokens": [9001, 341, 13759, 294, 264, 30334, 1389, 13, 8590, 11, 294, 709, 10394, 309, 307, 17003], "temperature": 0.0, "avg_logprob": -0.08722956601311178, "compression_ratio": 1.6106194690265487, "no_speech_prob": 3.535712312441319e-05}, {"id": 1285, "seek": 906264, "start": 9077.439999999999, "end": 9085.039999999999, "text": " used as a synonym for its reductionist counterpart, modularity. As discussed in the Geico case,", "tokens": [1143, 382, 257, 5451, 12732, 337, 1080, 11004, 468, 22335, 11, 31111, 507, 13, 1018, 7152, 294, 264, 2876, 2789, 1389, 11], "temperature": 0.0, "avg_logprob": -0.08722956601311178, "compression_ratio": 1.6106194690265487, "no_speech_prob": 3.535712312441319e-05}, {"id": 1286, "seek": 906264, "start": 9085.039999999999, "end": 9090.96, "text": " in the section above, holistic ML systems can fill in missing details starting from", "tokens": [294, 264, 3541, 3673, 11, 30334, 21601, 3652, 393, 2836, 294, 5361, 4365, 2891, 490], "temperature": 0.0, "avg_logprob": -0.08722956601311178, "compression_ratio": 1.6106194690265487, "no_speech_prob": 3.535712312441319e-05}, {"id": 1287, "seek": 909096, "start": 9090.96, "end": 9098.24, "text": " various scant evidence. Compare for example confabulations of systems like GPT-3 and image", "tokens": [3683, 795, 394, 4467, 13, 48523, 337, 1365, 1497, 455, 4136, 295, 3652, 411, 26039, 51, 12, 18, 293, 3256], "temperature": 0.0, "avg_logprob": -0.07920248588819183, "compression_ratio": 1.5743801652892562, "no_speech_prob": 0.00019211028120480478}, {"id": 1288, "seek": 909096, "start": 9098.24, "end": 9105.359999999999, "text": " enhancement apps. They supply the missing details by jumping to conclusions based on few clues", "tokens": [40776, 7733, 13, 814, 5847, 264, 5361, 4365, 538, 11233, 281, 22865, 2361, 322, 1326, 20936], "temperature": 0.0, "avg_logprob": -0.07920248588819183, "compression_ratio": 1.5743801652892562, "no_speech_prob": 0.00019211028120480478}, {"id": 1289, "seek": 909096, "start": 9105.359999999999, "end": 9111.839999999998, "text": " and lots of experience. Since we are not omniscient and don't even know what is happening behind our", "tokens": [293, 3195, 295, 1752, 13, 4162, 321, 366, 406, 3406, 10661, 5412, 293, 500, 380, 754, 458, 437, 307, 2737, 2261, 527], "temperature": 0.0, "avg_logprob": -0.07920248588819183, "compression_ratio": 1.5743801652892562, "no_speech_prob": 0.00019211028120480478}, {"id": 1290, "seek": 909096, "start": 9111.839999999998, "end": 9118.48, "text": " backs, scant evidence is all we will ever have, but it is amazing how effective scant evidence", "tokens": [19513, 11, 795, 394, 4467, 307, 439, 321, 486, 1562, 362, 11, 457, 309, 307, 2243, 577, 4942, 795, 394, 4467], "temperature": 0.0, "avg_logprob": -0.07920248588819183, "compression_ratio": 1.5743801652892562, "no_speech_prob": 0.00019211028120480478}, {"id": 1291, "seek": 911848, "start": 9118.48, "end": 9125.92, "text": " can be in a familiar context. We can drive a car through fog or find an alarm clock in absolute", "tokens": [393, 312, 294, 257, 4963, 4319, 13, 492, 393, 3332, 257, 1032, 807, 13648, 420, 915, 364, 14183, 7830, 294, 8236], "temperature": 0.0, "avg_logprob": -0.048915305733680726, "compression_ratio": 1.5420168067226891, "no_speech_prob": 8.331726712640375e-05}, {"id": 1292, "seek": 911848, "start": 9125.92, "end": 9132.72, "text": " darkness. The more the system has learned, the less input is needed to arrive at a reasonable", "tokens": [11262, 13, 440, 544, 264, 1185, 575, 3264, 11, 264, 1570, 4846, 307, 2978, 281, 8881, 412, 257, 10585], "temperature": 0.0, "avg_logprob": -0.048915305733680726, "compression_ratio": 1.5420168067226891, "no_speech_prob": 8.331726712640375e-05}, {"id": 1293, "seek": 911848, "start": 9132.72, "end": 9138.56, "text": " identification of the problem and hence retrieve a previously discovered working solution.", "tokens": [22065, 295, 264, 1154, 293, 16678, 30254, 257, 8046, 6941, 1364, 3827, 13], "temperature": 0.0, "avg_logprob": -0.048915305733680726, "compression_ratio": 1.5420168067226891, "no_speech_prob": 8.331726712640375e-05}, {"id": 1294, "seek": 911848, "start": 9139.439999999999, "end": 9145.76, "text": " Formal methods and experimental rigorousness make for good science. On the other hand,", "tokens": [10126, 304, 7150, 293, 17069, 29882, 1287, 652, 337, 665, 3497, 13, 1282, 264, 661, 1011, 11], "temperature": 0.0, "avg_logprob": -0.048915305733680726, "compression_ratio": 1.5420168067226891, "no_speech_prob": 8.331726712640375e-05}, {"id": 1295, "seek": 914576, "start": 9145.76, "end": 9152.48, "text": " holistic methods can follow tenuous threads, hoping for stronger threads or some solution,", "tokens": [30334, 7150, 393, 1524, 2064, 12549, 19314, 11, 7159, 337, 7249, 19314, 420, 512, 3827, 11], "temperature": 0.0, "avg_logprob": -0.08043186257525188, "compression_ratio": 1.6726457399103138, "no_speech_prob": 3.990836921730079e-05}, {"id": 1296, "seek": 914576, "start": 9152.48, "end": 9158.800000000001, "text": " with little effort spent on backtracking or documentation because once a solution is found,", "tokens": [365, 707, 4630, 4418, 322, 646, 6903, 14134, 420, 14333, 570, 1564, 257, 3827, 307, 1352, 11], "temperature": 0.0, "avg_logprob": -0.08043186257525188, "compression_ratio": 1.6726457399103138, "no_speech_prob": 3.990836921730079e-05}, {"id": 1297, "seek": 914576, "start": 9158.800000000001, "end": 9165.2, "text": " it is the only thing that matters. Tracking has little value in non-repeating situations", "tokens": [309, 307, 264, 787, 551, 300, 7001, 13, 1765, 14134, 575, 707, 2158, 294, 2107, 12, 265, 494, 990, 6851], "temperature": 0.0, "avg_logprob": -0.08043186257525188, "compression_ratio": 1.6726457399103138, "no_speech_prob": 3.990836921730079e-05}, {"id": 1298, "seek": 914576, "start": 9165.2, "end": 9173.12, "text": " or when using holistic methods at massive scales, such as in deep learning. Absolute control requires", "tokens": [420, 562, 1228, 30334, 7150, 412, 5994, 17408, 11, 1270, 382, 294, 2452, 2539, 13, 43965, 1169, 1969, 7029], "temperature": 0.0, "avg_logprob": -0.08043186257525188, "compression_ratio": 1.6726457399103138, "no_speech_prob": 3.990836921730079e-05}, {"id": 1299, "seek": 917312, "start": 9173.12, "end": 9179.12, "text": " that we know exactly what the problems and solutions are and all we need to do is implement them.", "tokens": [300, 321, 458, 2293, 437, 264, 2740, 293, 6547, 366, 293, 439, 321, 643, 281, 360, 307, 4445, 552, 13], "temperature": 0.0, "avg_logprob": -0.0637626957583737, "compression_ratio": 1.6940639269406392, "no_speech_prob": 5.67104943911545e-05}, {"id": 1300, "seek": 917312, "start": 9179.84, "end": 9186.320000000002, "text": " Once deployed, systems frozen in this manner, which are exactly implementing the models of", "tokens": [3443, 17826, 11, 3652, 12496, 294, 341, 9060, 11, 597, 366, 2293, 18114, 264, 5245, 295], "temperature": 0.0, "avg_logprob": -0.0637626957583737, "compression_ratio": 1.6940639269406392, "no_speech_prob": 5.67104943911545e-05}, {"id": 1301, "seek": 917312, "start": 9186.320000000002, "end": 9193.2, "text": " their creators, cannot improve by learning since there is no room for variation in the existing", "tokens": [641, 16039, 11, 2644, 3470, 538, 2539, 1670, 456, 307, 572, 1808, 337, 12990, 294, 264, 6741], "temperature": 0.0, "avg_logprob": -0.0637626957583737, "compression_ratio": 1.6940639269406392, "no_speech_prob": 5.67104943911545e-05}, {"id": 1302, "seek": 917312, "start": 9193.2, "end": 9200.560000000001, "text": " process and hence no experimentation and no way to discover further improvements. Only", "tokens": [1399, 293, 16678, 572, 37142, 293, 572, 636, 281, 4411, 3052, 13797, 13, 5686], "temperature": 0.0, "avg_logprob": -0.0637626957583737, "compression_ratio": 1.6940639269406392, "no_speech_prob": 5.67104943911545e-05}, {"id": 1303, "seek": 920056, "start": 9200.56, "end": 9208.64, "text": " holistic systems can provide creativity and useful novelty. We also observe that, learning itself", "tokens": [30334, 3652, 393, 2893, 12915, 293, 4420, 44805, 13, 492, 611, 11441, 300, 11, 2539, 2564], "temperature": 0.0, "avg_logprob": -0.08487944169477983, "compression_ratio": 1.550660792951542, "no_speech_prob": 3.2103813282446936e-05}, {"id": 1304, "seek": 920056, "start": 9208.64, "end": 9214.64, "text": " is a creative act, since it must fit new information into an existing network of prior", "tokens": [307, 257, 5880, 605, 11, 1670, 309, 1633, 3318, 777, 1589, 666, 364, 6741, 3209, 295, 4059], "temperature": 0.0, "avg_logprob": -0.08487944169477983, "compression_ratio": 1.550660792951542, "no_speech_prob": 3.2103813282446936e-05}, {"id": 1305, "seek": 920056, "start": 9214.64, "end": 9222.88, "text": " experience. Just like the term, holism has been abused, so has intelligent design,", "tokens": [1752, 13, 1449, 411, 264, 1433, 11, 4091, 1434, 575, 668, 27075, 11, 370, 575, 13232, 1715, 11], "temperature": 0.0, "avg_logprob": -0.08487944169477983, "compression_ratio": 1.550660792951542, "no_speech_prob": 3.2103813282446936e-05}, {"id": 1306, "seek": 920056, "start": 9222.88, "end": 9228.4, "text": " which is a perfectly reasonable term for reductionist industrial end-to-end practice", "tokens": [597, 307, 257, 6239, 10585, 1433, 337, 11004, 468, 9987, 917, 12, 1353, 12, 521, 3124], "temperature": 0.0, "avg_logprob": -0.08487944169477983, "compression_ratio": 1.550660792951542, "no_speech_prob": 3.2103813282446936e-05}, {"id": 1307, "seek": 922840, "start": 9228.4, "end": 9235.6, "text": " that consistently provides excellent results. On the holistic side, evolution in nature has", "tokens": [300, 14961, 6417, 7103, 3542, 13, 1282, 264, 30334, 1252, 11, 9303, 294, 3687, 575], "temperature": 0.0, "avg_logprob": -0.06641879207209538, "compression_ratio": 1.5866666666666667, "no_speech_prob": 1.9743816665140912e-05}, {"id": 1308, "seek": 922840, "start": 9235.6, "end": 9241.84, "text": " created wonderful solutions to all kinds of problems that plants and animals need to handle.", "tokens": [2942, 3715, 6547, 281, 439, 3685, 295, 2740, 300, 5972, 293, 4882, 643, 281, 4813, 13], "temperature": 0.0, "avg_logprob": -0.06641879207209538, "compression_ratio": 1.5866666666666667, "no_speech_prob": 1.9743816665140912e-05}, {"id": 1309, "seek": 922840, "start": 9242.56, "end": 9247.76, "text": " But we can put evolution, also known in the general sense as selectionism,", "tokens": [583, 321, 393, 829, 9303, 11, 611, 2570, 294, 264, 2674, 2020, 382, 9450, 1434, 11], "temperature": 0.0, "avg_logprob": -0.06641879207209538, "compression_ratio": 1.5866666666666667, "no_speech_prob": 1.9743816665140912e-05}, {"id": 1310, "seek": 922840, "start": 9247.76, "end": 9255.44, "text": " to work for us in our holistic machines. They can create new, wonderful designs with a biological", "tokens": [281, 589, 337, 505, 294, 527, 30334, 8379, 13, 814, 393, 1884, 777, 11, 3715, 11347, 365, 257, 13910], "temperature": 0.0, "avg_logprob": -0.06641879207209538, "compression_ratio": 1.5866666666666667, "no_speech_prob": 1.9743816665140912e-05}, {"id": 1311, "seek": 925544, "start": 9255.44, "end": 9262.08, "text": " flavor to them that sometimes, depending on the problem, cannot perform intelligently designed", "tokens": [6813, 281, 552, 300, 2171, 11, 5413, 322, 264, 1154, 11, 2644, 2042, 5613, 2276, 4761], "temperature": 0.0, "avg_logprob": -0.06551878054936727, "compression_ratio": 1.4840425531914894, "no_speech_prob": 0.00010030618432210758}, {"id": 1312, "seek": 925544, "start": 9262.08, "end": 9270.480000000001, "text": " alternatives. Evolution is the most holistic phenomenon we know. No goal functions, no models,", "tokens": [20478, 13, 40800, 307, 264, 881, 30334, 14029, 321, 458, 13, 883, 3387, 6828, 11, 572, 5245, 11], "temperature": 0.0, "avg_logprob": -0.06551878054936727, "compression_ratio": 1.4840425531914894, "no_speech_prob": 0.00010030618432210758}, {"id": 1313, "seek": 925544, "start": 9270.480000000001, "end": 9278.560000000001, "text": " no equations. Evolution is not a scientific theory. Science cannot contain it. It must be", "tokens": [572, 11787, 13, 40800, 307, 406, 257, 8134, 5261, 13, 8976, 2644, 5304, 309, 13, 467, 1633, 312], "temperature": 0.0, "avg_logprob": -0.06551878054936727, "compression_ratio": 1.4840425531914894, "no_speech_prob": 0.00010030618432210758}, {"id": 1314, "seek": 927856, "start": 9278.56, "end": 9286.32, "text": " discussed in epistemology. Mixed systems, deep neural networks can perform autonomous epistemic", "tokens": [7152, 294, 2388, 43958, 1793, 13, 12769, 292, 3652, 11, 2452, 18161, 9590, 393, 2042, 23797, 2388, 468, 3438], "temperature": 0.0, "avg_logprob": -0.0556046209837261, "compression_ratio": 1.6077586206896552, "no_speech_prob": 7.944543904159218e-05}, {"id": 1315, "seek": 927856, "start": 9286.32, "end": 9293.039999999999, "text": " reduction to find high-level representations for low-level input, such as pixels in an image", "tokens": [11004, 281, 915, 1090, 12, 12418, 33358, 337, 2295, 12, 12418, 4846, 11, 1270, 382, 18668, 294, 364, 3256], "temperature": 0.0, "avg_logprob": -0.0556046209837261, "compression_ratio": 1.6077586206896552, "no_speech_prob": 7.944543904159218e-05}, {"id": 1316, "seek": 927856, "start": 9293.039999999999, "end": 9300.0, "text": " or characters in text. Current vision understanding systems can reliably identify thousands of", "tokens": [420, 4342, 294, 2487, 13, 15629, 5201, 3701, 3652, 393, 49927, 5876, 5383, 295], "temperature": 0.0, "avg_logprob": -0.0556046209837261, "compression_ratio": 1.6077586206896552, "no_speech_prob": 7.944543904159218e-05}, {"id": 1317, "seek": 927856, "start": 9300.0, "end": 9305.519999999999, "text": " different kinds of objects from many different angles in a variety of lighting conditions", "tokens": [819, 3685, 295, 6565, 490, 867, 819, 14708, 294, 257, 5673, 295, 9577, 4487], "temperature": 0.0, "avg_logprob": -0.0556046209837261, "compression_ratio": 1.6077586206896552, "no_speech_prob": 7.944543904159218e-05}, {"id": 1318, "seek": 930552, "start": 9305.52, "end": 9313.28, "text": " and weather. They can classify what they see, but do not necessarily understand much beyond that,", "tokens": [293, 5503, 13, 814, 393, 33872, 437, 436, 536, 11, 457, 360, 406, 4725, 1223, 709, 4399, 300, 11], "temperature": 0.0, "avg_logprob": -0.07382029102694604, "compression_ratio": 1.4705882352941178, "no_speech_prob": 6.641125946771353e-05}, {"id": 1319, "seek": 930552, "start": 9313.28, "end": 9320.16, "text": " such as the expected behaviors of other, intentional Asians like cars, pedestrians,", "tokens": [1270, 382, 264, 5176, 15501, 295, 661, 11, 21935, 47724, 411, 5163, 11, 48339, 11], "temperature": 0.0, "avg_logprob": -0.07382029102694604, "compression_ratio": 1.4705882352941178, "no_speech_prob": 6.641125946771353e-05}, {"id": 1320, "seek": 930552, "start": 9320.16, "end": 9328.960000000001, "text": " or cats. Therefore, at the moment in 2022, most deployments of machine learning use a mixture", "tokens": [420, 11111, 13, 7504, 11, 412, 264, 1623, 294, 20229, 11, 881, 7274, 1117, 295, 3479, 2539, 764, 257, 9925], "temperature": 0.0, "avg_logprob": -0.07382029102694604, "compression_ratio": 1.4705882352941178, "no_speech_prob": 6.641125946771353e-05}, {"id": 1321, "seek": 932896, "start": 9328.96, "end": 9336.0, "text": " of reductionist and holistic methods, equations and formulas devised by humans implemented as", "tokens": [295, 11004, 468, 293, 30334, 7150, 11, 11787, 293, 30546, 1905, 2640, 538, 6255, 12270, 382], "temperature": 0.0, "avg_logprob": -0.06659346535092309, "compression_ratio": 1.5932203389830508, "no_speech_prob": 4.592305049300194e-05}, {"id": 1322, "seek": 932896, "start": 9336.0, "end": 9343.039999999999, "text": " computer code, and some inputs from a deep neural network solving a sub-problem that requires it,", "tokens": [3820, 3089, 11, 293, 512, 15743, 490, 257, 2452, 18161, 3209, 12606, 257, 1422, 12, 47419, 300, 7029, 309, 11], "temperature": 0.0, "avg_logprob": -0.06659346535092309, "compression_ratio": 1.5932203389830508, "no_speech_prob": 4.592305049300194e-05}, {"id": 1323, "seek": 932896, "start": 9343.039999999999, "end": 9351.199999999999, "text": " such as vision understanding. Self-driving cars use DNNs for understanding vision, radar,", "tokens": [1270, 382, 5201, 3701, 13, 16348, 12, 47094, 5163, 764, 21500, 45, 82, 337, 3701, 5201, 11, 16544, 11], "temperature": 0.0, "avg_logprob": -0.06659346535092309, "compression_ratio": 1.5932203389830508, "no_speech_prob": 4.592305049300194e-05}, {"id": 1324, "seek": 932896, "start": 9351.199999999999, "end": 9358.32, "text": " and lidar images, discovering high-level information like a pedestrian on the side of the road", "tokens": [293, 10252, 289, 5267, 11, 24773, 1090, 12, 12418, 1589, 411, 257, 33947, 322, 264, 1252, 295, 264, 3060], "temperature": 0.0, "avg_logprob": -0.06659346535092309, "compression_ratio": 1.5932203389830508, "no_speech_prob": 4.592305049300194e-05}, {"id": 1325, "seek": 935832, "start": 9358.32, "end": 9365.039999999999, "text": " from pixel-based images and this understanding has, until recently, been fed to logic and", "tokens": [490, 19261, 12, 6032, 5267, 293, 341, 3701, 575, 11, 1826, 3938, 11, 668, 4636, 281, 9952, 293], "temperature": 0.0, "avg_logprob": -0.07041557268662886, "compression_ratio": 1.6266094420600858, "no_speech_prob": 3.44680629495997e-05}, {"id": 1326, "seek": 935832, "start": 9365.039999999999, "end": 9372.88, "text": " role-based programs that implement the decision-making. Avoid driving into anything, period, that is used", "tokens": [3090, 12, 6032, 4268, 300, 4445, 264, 3537, 12, 12402, 13, 41061, 4840, 666, 1340, 11, 2896, 11, 300, 307, 1143], "temperature": 0.0, "avg_logprob": -0.07041557268662886, "compression_ratio": 1.6266094420600858, "no_speech_prob": 3.44680629495997e-05}, {"id": 1327, "seek": 935832, "start": 9372.88, "end": 9379.279999999999, "text": " to control the car. The trend here is to move more and more responsibilities into the deep", "tokens": [281, 1969, 264, 1032, 13, 440, 6028, 510, 307, 281, 1286, 544, 293, 544, 16190, 666, 264, 2452], "temperature": 0.0, "avg_logprob": -0.07041557268662886, "compression_ratio": 1.6266094420600858, "no_speech_prob": 3.44680629495997e-05}, {"id": 1328, "seek": 935832, "start": 9379.279999999999, "end": 9386.56, "text": " neural network, and over time to remove the hand-coded parts. In essence, the network learns", "tokens": [18161, 3209, 11, 293, 670, 565, 281, 4159, 264, 1011, 12, 66, 12340, 3166, 13, 682, 12801, 11, 264, 3209, 27152], "temperature": 0.0, "avg_logprob": -0.07041557268662886, "compression_ratio": 1.6266094420600858, "no_speech_prob": 3.44680629495997e-05}, {"id": 1329, "seek": 938656, "start": 9386.56, "end": 9393.76, "text": " not only to see, but learns to understand traffic. We are delegating more and more of our", "tokens": [406, 787, 281, 536, 11, 457, 27152, 281, 1223, 6419, 13, 492, 366, 15824, 990, 544, 293, 544, 295, 527], "temperature": 0.0, "avg_logprob": -0.09125292479102291, "compression_ratio": 1.5271739130434783, "no_speech_prob": 0.00012401571439113468}, {"id": 1330, "seek": 938656, "start": 9393.76, "end": 9402.48, "text": " understanding of how to drive to the vehicle itself. This is desirable. Experimental Epistemology", "tokens": [3701, 295, 577, 281, 3332, 281, 264, 5864, 2564, 13, 639, 307, 30533, 13, 37933, 304, 9970, 43958, 1793], "temperature": 0.0, "avg_logprob": -0.09125292479102291, "compression_ratio": 1.5271739130434783, "no_speech_prob": 0.00012401571439113468}, {"id": 1331, "seek": 938656, "start": 9403.68, "end": 9410.24, "text": " Epistemology is the theory of knowledge. It is concerned with the mind's relation to reality.", "tokens": [9970, 43958, 1793, 307, 264, 5261, 295, 3601, 13, 467, 307, 5922, 365, 264, 1575, 311, 9721, 281, 4103, 13], "temperature": 0.0, "avg_logprob": -0.09125292479102291, "compression_ratio": 1.5271739130434783, "no_speech_prob": 0.00012401571439113468}, {"id": 1332, "seek": 941024, "start": 9410.24, "end": 9417.68, "text": " This includes artificial minds. An introduction to epistemology should benefit anyone working in", "tokens": [639, 5974, 11677, 9634, 13, 1107, 9339, 281, 2388, 43958, 1793, 820, 5121, 2878, 1364, 294], "temperature": 0.0, "avg_logprob": -0.15762367844581604, "compression_ratio": 1.4088669950738917, "no_speech_prob": 0.00013308122288435698}, {"id": 1333, "seek": 941024, "start": 9417.68, "end": 9426.4, "text": " the IML field. Scientific statements look like F equals MA, Newton's second law, or E equals MC", "tokens": [264, 286, 12683, 2519, 13, 47437, 12363, 574, 411, 479, 6915, 12191, 11, 19541, 311, 1150, 2101, 11, 420, 462, 6915, 8797], "temperature": 0.0, "avg_logprob": -0.15762367844581604, "compression_ratio": 1.4088669950738917, "no_speech_prob": 0.00013308122288435698}, {"id": 1334, "seek": 941024, "start": 9426.4, "end": 9433.36, "text": " squared, Einstein's famous equation, and can all be proven and or derived from other accepted", "tokens": [8889, 11, 23486, 311, 4618, 5367, 11, 293, 393, 439, 312, 12785, 293, 420, 18949, 490, 661, 9035], "temperature": 0.0, "avg_logprob": -0.15762367844581604, "compression_ratio": 1.4088669950738917, "no_speech_prob": 0.00013308122288435698}, {"id": 1335, "seek": 943336, "start": 9433.36, "end": 9441.2, "text": " results or verified experimentally. Algebra is built on lemurs that are not part of algebra.", "tokens": [3542, 420, 31197, 5120, 379, 13, 967, 19983, 307, 3094, 322, 7495, 2156, 300, 366, 406, 644, 295, 21989, 13], "temperature": 0.0, "avg_logprob": -0.06824044483463938, "compression_ratio": 1.6849315068493151, "no_speech_prob": 5.7476463553030044e-05}, {"id": 1336, "seek": 943336, "start": 9441.2, "end": 9447.84, "text": " They cannot be proven inside of algebra. Similarly, epistemological statements are", "tokens": [814, 2644, 312, 12785, 1854, 295, 21989, 13, 13157, 11, 2388, 43958, 4383, 12363, 366], "temperature": 0.0, "avg_logprob": -0.06824044483463938, "compression_ratio": 1.6849315068493151, "no_speech_prob": 5.7476463553030044e-05}, {"id": 1337, "seek": 943336, "start": 9447.84, "end": 9454.960000000001, "text": " not provable in science because science is built on top of epistemology. But when science is not", "tokens": [406, 1439, 712, 294, 3497, 570, 3497, 307, 3094, 322, 1192, 295, 2388, 43958, 1793, 13, 583, 562, 3497, 307, 406], "temperature": 0.0, "avg_logprob": -0.06824044483463938, "compression_ratio": 1.6849315068493151, "no_speech_prob": 5.7476463553030044e-05}, {"id": 1338, "seek": 943336, "start": 9454.960000000001, "end": 9462.880000000001, "text": " helping, such as in bizarre domains, then setting scientific methodology aside and dropping down", "tokens": [4315, 11, 1270, 382, 294, 18265, 25514, 11, 550, 3287, 8134, 24850, 7359, 293, 13601, 760], "temperature": 0.0, "avg_logprob": -0.06824044483463938, "compression_ratio": 1.6849315068493151, "no_speech_prob": 5.7476463553030044e-05}, {"id": 1339, "seek": 946288, "start": 9462.88, "end": 9470.88, "text": " to the level of epistemology sometimes works. Epistemology is, just like philosophy in general,", "tokens": [281, 264, 1496, 295, 2388, 43958, 1793, 2171, 1985, 13, 9970, 43958, 1793, 307, 11, 445, 411, 10675, 294, 2674, 11], "temperature": 0.0, "avg_logprob": -0.04828839243194203, "compression_ratio": 1.5892116182572613, "no_speech_prob": 5.919837349210866e-05}, {"id": 1340, "seek": 946288, "start": 9470.88, "end": 9477.599999999999, "text": " an armchair thinking exercise, and the results are judged on internal coherence and consistency", "tokens": [364, 3726, 17892, 1953, 5380, 11, 293, 264, 3542, 366, 27485, 322, 6920, 26528, 655, 293, 14416], "temperature": 0.0, "avg_logprob": -0.04828839243194203, "compression_ratio": 1.5892116182572613, "no_speech_prob": 5.919837349210866e-05}, {"id": 1341, "seek": 946288, "start": 9477.599999999999, "end": 9484.72, "text": " with other accepted theory rather than by proofs or experiments. However, the availability of", "tokens": [365, 661, 9035, 5261, 2831, 813, 538, 8177, 82, 420, 12050, 13, 2908, 11, 264, 17945, 295], "temperature": 0.0, "avg_logprob": -0.04828839243194203, "compression_ratio": 1.5892116182572613, "no_speech_prob": 5.919837349210866e-05}, {"id": 1342, "seek": 946288, "start": 9484.72, "end": 9492.16, "text": " understanding machines, such as DNNs now suddenly provides the opportunity for actual experiments", "tokens": [3701, 8379, 11, 1270, 382, 21500, 45, 82, 586, 5800, 6417, 264, 2650, 337, 3539, 12050], "temperature": 0.0, "avg_logprob": -0.04828839243194203, "compression_ratio": 1.5892116182572613, "no_speech_prob": 5.919837349210866e-05}, {"id": 1343, "seek": 949216, "start": 9492.16, "end": 9498.72, "text": " in epistemology. Consider the following statements from the domain of epistemology,", "tokens": [294, 2388, 43958, 1793, 13, 17416, 264, 3480, 12363, 490, 264, 9274, 295, 2388, 43958, 1793, 11], "temperature": 0.0, "avg_logprob": -0.06728468736012777, "compression_ratio": 1.461111111111111, "no_speech_prob": 6.766787555534393e-05}, {"id": 1344, "seek": 949216, "start": 9498.72, "end": 9505.039999999999, "text": " and how each of them can be viewed as an implementation hint for AI designers. We are", "tokens": [293, 577, 1184, 295, 552, 393, 312, 19174, 382, 364, 11420, 12075, 337, 7318, 16196, 13, 492, 366], "temperature": 0.0, "avg_logprob": -0.06728468736012777, "compression_ratio": 1.461111111111111, "no_speech_prob": 6.766787555534393e-05}, {"id": 1345, "seek": 949216, "start": 9505.039999999999, "end": 9512.08, "text": " already able to measure their effects on system competence. You can only learn that which you", "tokens": [1217, 1075, 281, 3481, 641, 5065, 322, 1185, 39965, 13, 509, 393, 787, 1466, 300, 597, 291], "temperature": 0.0, "avg_logprob": -0.06728468736012777, "compression_ratio": 1.461111111111111, "no_speech_prob": 6.766787555534393e-05}, {"id": 1346, "seek": 951208, "start": 9512.08, "end": 9522.72, "text": " already almost know. Patrick Winston, MIT. Our intelligences are fallible. Monica Anderson.", "tokens": [1217, 1920, 458, 13, 13980, 33051, 11, 13100, 13, 2621, 5613, 2667, 366, 2100, 964, 13, 25363, 18768, 13], "temperature": 0.0, "avg_logprob": -0.11883188039064407, "compression_ratio": 1.5730337078651686, "no_speech_prob": 0.000175008230144158}, {"id": 1347, "seek": 951208, "start": 9522.72, "end": 9529.36, "text": " In order to detect that something is new, you need to recognize everything old. Monica Anderson.", "tokens": [682, 1668, 281, 5531, 300, 746, 307, 777, 11, 291, 643, 281, 5521, 1203, 1331, 13, 25363, 18768, 13], "temperature": 0.0, "avg_logprob": -0.11883188039064407, "compression_ratio": 1.5730337078651686, "no_speech_prob": 0.000175008230144158}, {"id": 1348, "seek": 951208, "start": 9530.64, "end": 9538.32, "text": " You cannot reason about that which you do not understand. Monica Anderson. You are known by", "tokens": [509, 2644, 1778, 466, 300, 597, 291, 360, 406, 1223, 13, 25363, 18768, 13, 509, 366, 2570, 538], "temperature": 0.0, "avg_logprob": -0.11883188039064407, "compression_ratio": 1.5730337078651686, "no_speech_prob": 0.000175008230144158}, {"id": 1349, "seek": 953832, "start": 9538.32, "end": 9545.36, "text": " the company you keep, simple version of the yanni dilemma from category theory and the justification", "tokens": [264, 2237, 291, 1066, 11, 2199, 3037, 295, 264, 288, 35832, 34312, 490, 7719, 5261, 293, 264, 31591], "temperature": 0.0, "avg_logprob": -0.14147414535772604, "compression_ratio": 1.576271186440678, "no_speech_prob": 9.756674990057945e-05}, {"id": 1350, "seek": 953832, "start": 9545.36, "end": 9552.24, "text": " for embeddings in deep learning. All useful novelty in the universe is due to processes", "tokens": [337, 12240, 29432, 294, 2452, 2539, 13, 1057, 4420, 44805, 294, 264, 6445, 307, 3462, 281, 7555], "temperature": 0.0, "avg_logprob": -0.14147414535772604, "compression_ratio": 1.576271186440678, "no_speech_prob": 9.756674990057945e-05}, {"id": 1351, "seek": 953832, "start": 9552.24, "end": 9560.16, "text": " of variation and selection. The selectionist manifesto. Selectionism is the generalization", "tokens": [295, 12990, 293, 9450, 13, 440, 9450, 468, 10067, 78, 13, 1100, 5450, 1434, 307, 264, 2674, 2144], "temperature": 0.0, "avg_logprob": -0.14147414535772604, "compression_ratio": 1.576271186440678, "no_speech_prob": 9.756674990057945e-05}, {"id": 1352, "seek": 956016, "start": 9560.16, "end": 9568.88, "text": " of Darwinism. This is right genetic algorithms work. Science has no equations for concepts like", "tokens": [295, 30233, 1434, 13, 639, 307, 558, 12462, 14642, 589, 13, 8976, 575, 572, 11787, 337, 10392, 411], "temperature": 0.0, "avg_logprob": -0.057558584958314896, "compression_ratio": 1.5208333333333333, "no_speech_prob": 7.28520390111953e-05}, {"id": 1353, "seek": 956016, "start": 9568.88, "end": 9576.24, "text": " understanding, reasoning, learning, abstraction, or modeling since they are all epistemology level", "tokens": [3701, 11, 21577, 11, 2539, 11, 37765, 11, 420, 15983, 1670, 436, 366, 439, 2388, 43958, 1793, 1496], "temperature": 0.0, "avg_logprob": -0.057558584958314896, "compression_ratio": 1.5208333333333333, "no_speech_prob": 7.28520390111953e-05}, {"id": 1354, "seek": 956016, "start": 9576.24, "end": 9584.16, "text": " concepts. We cannot even start using science until we have decided what model to use. We must use", "tokens": [10392, 13, 492, 2644, 754, 722, 1228, 3497, 1826, 321, 362, 3047, 437, 2316, 281, 764, 13, 492, 1633, 764], "temperature": 0.0, "avg_logprob": -0.057558584958314896, "compression_ratio": 1.5208333333333333, "no_speech_prob": 7.28520390111953e-05}, {"id": 1355, "seek": 958416, "start": 9584.16, "end": 9590.64, "text": " our experience to perform epistemic reductions, discarding the irrelevant, starting from the", "tokens": [527, 1752, 281, 2042, 2388, 468, 3438, 40296, 11, 31597, 278, 264, 28682, 11, 2891, 490, 264], "temperature": 0.0, "avg_logprob": -0.09332608602133143, "compression_ratio": 1.6322869955156951, "no_speech_prob": 3.0468980185105465e-05}, {"id": 1356, "seek": 958416, "start": 9590.64, "end": 9597.119999999999, "text": " messy real world problem situation until we are left with a scientific model we can use,", "tokens": [16191, 957, 1002, 1154, 2590, 1826, 321, 366, 1411, 365, 257, 8134, 2316, 321, 393, 764, 11], "temperature": 0.0, "avg_logprob": -0.09332608602133143, "compression_ratio": 1.6322869955156951, "no_speech_prob": 3.0468980185105465e-05}, {"id": 1357, "seek": 958416, "start": 9597.119999999999, "end": 9604.48, "text": " such as an equation. The focus in AI research should be on exactly how we can get our machines", "tokens": [1270, 382, 364, 5367, 13, 440, 1879, 294, 7318, 2132, 820, 312, 322, 2293, 577, 321, 393, 483, 527, 8379], "temperature": 0.0, "avg_logprob": -0.09332608602133143, "compression_ratio": 1.6322869955156951, "no_speech_prob": 3.0468980185105465e-05}, {"id": 1358, "seek": 958416, "start": 9604.48, "end": 9610.96, "text": " to perform this pre-scientific epistemic reduction by themselves and the answer to that", "tokens": [281, 2042, 341, 659, 12, 82, 5412, 1089, 2388, 468, 3438, 11004, 538, 2969, 293, 264, 1867, 281, 300], "temperature": 0.0, "avg_logprob": -0.09332608602133143, "compression_ratio": 1.6322869955156951, "no_speech_prob": 3.0468980185105465e-05}, {"id": 1359, "seek": 961096, "start": 9610.96, "end": 9618.4, "text": " cannot be found inside of science. Artificial General Intelligence Artificial General Intelligence,", "tokens": [2644, 312, 1352, 1854, 295, 3497, 13, 5735, 10371, 6996, 27274, 5735, 10371, 6996, 27274, 11], "temperature": 0.0, "avg_logprob": -0.16124547322591146, "compression_ratio": 1.5635359116022098, "no_speech_prob": 6.154642323963344e-05}, {"id": 1360, "seek": 961096, "start": 9618.4, "end": 9626.24, "text": " AGI, was a theoretical 20th century reductionist AI attempt to go beyond the narrow AIA of domain", "tokens": [316, 26252, 11, 390, 257, 20864, 945, 392, 4901, 11004, 468, 7318, 5217, 281, 352, 4399, 264, 9432, 316, 6914, 295, 9274], "temperature": 0.0, "avg_logprob": -0.16124547322591146, "compression_ratio": 1.5635359116022098, "no_speech_prob": 6.154642323963344e-05}, {"id": 1361, "seek": 961096, "start": 9626.24, "end": 9633.839999999998, "text": " specific expert systems closer to a general intelligence they thought humans had. The", "tokens": [2685, 5844, 3652, 4966, 281, 257, 2674, 7599, 436, 1194, 6255, 632, 13, 440], "temperature": 0.0, "avg_logprob": -0.16124547322591146, "compression_ratio": 1.5635359116022098, "no_speech_prob": 6.154642323963344e-05}, {"id": 1362, "seek": 963384, "start": 9633.84, "end": 9642.24, "text": " term was mostly used by independent researchers, amateurs and enthusiasts. But the AGI term was", "tokens": [1433, 390, 5240, 1143, 538, 6695, 10309, 11, 669, 25929, 293, 45873, 13, 583, 264, 316, 26252, 1433, 390], "temperature": 0.0, "avg_logprob": -0.07452905972798665, "compression_ratio": 1.5263157894736843, "no_speech_prob": 1.5611032722517848e-05}, {"id": 1363, "seek": 963384, "start": 9642.24, "end": 9648.4, "text": " not well enough defined and was not backed by sufficient theory to provide any AI implementation", "tokens": [406, 731, 1547, 7642, 293, 390, 406, 20391, 538, 11563, 5261, 281, 2893, 604, 7318, 11420], "temperature": 0.0, "avg_logprob": -0.07452905972798665, "compression_ratio": 1.5263157894736843, "no_speech_prob": 1.5611032722517848e-05}, {"id": 1364, "seek": 963384, "start": 9648.4, "end": 9655.2, "text": " guidance and what little progress had been made by these groups was overtaken by holistic methods", "tokens": [10056, 293, 437, 707, 4205, 632, 668, 1027, 538, 613, 3935, 390, 17038, 9846, 538, 30334, 7150], "temperature": 0.0, "avg_logprob": -0.07452905972798665, "compression_ratio": 1.5263157894736843, "no_speech_prob": 1.5611032722517848e-05}, {"id": 1365, "seek": 965520, "start": 9655.2, "end": 9663.6, "text": " after 2012. Today we know that the entire premise of 20th century reductionist AGI was wrong.", "tokens": [934, 9125, 13, 2692, 321, 458, 300, 264, 2302, 22045, 295, 945, 392, 4901, 11004, 468, 316, 26252, 390, 2085, 13], "temperature": 0.0, "avg_logprob": -0.09191663672284382, "compression_ratio": 1.5142857142857142, "no_speech_prob": 4.492805601330474e-05}, {"id": 1366, "seek": 965520, "start": 9664.16, "end": 9672.320000000002, "text": " Humans are not general intelligences at birth. Instead, we are general learners capable of", "tokens": [35809, 366, 406, 2674, 5613, 2667, 412, 3965, 13, 7156, 11, 321, 366, 2674, 23655, 8189, 295], "temperature": 0.0, "avg_logprob": -0.09191663672284382, "compression_ratio": 1.5142857142857142, "no_speech_prob": 4.492805601330474e-05}, {"id": 1367, "seek": 965520, "start": 9672.320000000002, "end": 9678.880000000001, "text": " learning almost any skill or knowledge required in a wide range of problem domains. If we want", "tokens": [2539, 1920, 604, 5389, 420, 3601, 4739, 294, 257, 4874, 3613, 295, 1154, 25514, 13, 759, 321, 528], "temperature": 0.0, "avg_logprob": -0.09191663672284382, "compression_ratio": 1.5142857142857142, "no_speech_prob": 4.492805601330474e-05}, {"id": 1368, "seek": 967888, "start": 9678.88, "end": 9685.759999999998, "text": " human compatible cognitive systems, then we should build them in our image in this respect to build", "tokens": [1952, 18218, 15605, 3652, 11, 550, 321, 820, 1322, 552, 294, 527, 3256, 294, 341, 3104, 281, 1322], "temperature": 0.0, "avg_logprob": -0.08255948424339295, "compression_ratio": 1.5933609958506223, "no_speech_prob": 4.330995216150768e-05}, {"id": 1369, "seek": 967888, "start": 9685.759999999998, "end": 9693.759999999998, "text": " machines that learn and jump to conclusions on scant evidence. Decades ago, AGI implied a human", "tokens": [8379, 300, 1466, 293, 3012, 281, 22865, 322, 795, 394, 4467, 13, 12427, 2977, 2057, 11, 316, 26252, 32614, 257, 1952], "temperature": 0.0, "avg_logprob": -0.08255948424339295, "compression_ratio": 1.5933609958506223, "no_speech_prob": 4.330995216150768e-05}, {"id": 1370, "seek": 967888, "start": 9693.759999999998, "end": 9700.56, "text": " programmed reductionist hand coded program based on logic and reasoning that can solve any problem", "tokens": [31092, 11004, 468, 1011, 34874, 1461, 2361, 322, 9952, 293, 21577, 300, 393, 5039, 604, 1154], "temperature": 0.0, "avg_logprob": -0.08255948424339295, "compression_ratio": 1.5933609958506223, "no_speech_prob": 4.330995216150768e-05}, {"id": 1371, "seek": 967888, "start": 9700.56, "end": 9707.039999999999, "text": " because the programmers anticipated it. To argue against claims that this was impossible,", "tokens": [570, 264, 41504, 23267, 309, 13, 1407, 9695, 1970, 9441, 300, 341, 390, 6243, 11], "temperature": 0.0, "avg_logprob": -0.08255948424339295, "compression_ratio": 1.5933609958506223, "no_speech_prob": 4.330995216150768e-05}, {"id": 1372, "seek": 970704, "start": 9707.04, "end": 9713.84, "text": " the AGI community came up with a promise or threat of self-improving AI. But the amount", "tokens": [264, 316, 26252, 1768, 1361, 493, 365, 257, 6228, 420, 4734, 295, 2698, 12, 332, 4318, 798, 7318, 13, 583, 264, 2372], "temperature": 0.0, "avg_logprob": -0.12603418854461318, "compression_ratio": 1.5183673469387755, "no_speech_prob": 5.659995076712221e-05}, {"id": 1373, "seek": 970704, "start": 9713.84, "end": 9721.04, "text": " of code in our cognitive systems has shrunk from 6 million propositions in sake around 1990", "tokens": [295, 3089, 294, 527, 15605, 3652, 575, 9884, 3197, 490, 1386, 2459, 7532, 2451, 294, 9717, 926, 13384], "temperature": 0.0, "avg_logprob": -0.12603418854461318, "compression_ratio": 1.5183673469387755, "no_speech_prob": 5.659995076712221e-05}, {"id": 1374, "seek": 970704, "start": 9721.04, "end": 9729.84, "text": " to 600 lines of code to play video games around 2017 to about 13 lines of cares code in some", "tokens": [281, 11849, 3876, 295, 3089, 281, 862, 960, 2813, 926, 6591, 281, 466, 3705, 3876, 295, 12310, 3089, 294, 512], "temperature": 0.0, "avg_logprob": -0.12603418854461318, "compression_ratio": 1.5183673469387755, "no_speech_prob": 5.659995076712221e-05}, {"id": 1375, "seek": 970704, "start": 9729.84, "end": 9736.560000000001, "text": " research reports. And now there's AutoML and other efforts at eliminating all remaining programming", "tokens": [2132, 7122, 13, 400, 586, 456, 311, 13738, 12683, 293, 661, 6484, 412, 31203, 439, 8877, 9410], "temperature": 0.0, "avg_logprob": -0.12603418854461318, "compression_ratio": 1.5183673469387755, "no_speech_prob": 5.659995076712221e-05}, {"id": 1376, "seek": 973656, "start": 9736.56, "end": 9743.76, "text": " from ML. The problems are not in the code. There's almost no code left to improve in", "tokens": [490, 21601, 13, 440, 2740, 366, 406, 294, 264, 3089, 13, 821, 311, 1920, 572, 3089, 1411, 281, 3470, 294], "temperature": 0.0, "avg_logprob": -0.06913711323457605, "compression_ratio": 1.537117903930131, "no_speech_prob": 6.493170803878456e-05}, {"id": 1377, "seek": 973656, "start": 9743.76, "end": 9752.16, "text": " modern machine learning systems. All that matters is the corpus. We can now, after 2012,", "tokens": [4363, 3479, 2539, 3652, 13, 1057, 300, 7001, 307, 264, 1181, 31624, 13, 492, 393, 586, 11, 934, 9125, 11], "temperature": 0.0, "avg_logprob": -0.06913711323457605, "compression_ratio": 1.537117903930131, "no_speech_prob": 6.493170803878456e-05}, {"id": 1378, "seek": 973656, "start": 9752.16, "end": 9757.84, "text": " see that machine learning is an absolute requirement for anything worthy of the name AI,", "tokens": [536, 300, 3479, 2539, 307, 364, 8236, 11695, 337, 1340, 14829, 295, 264, 1315, 7318, 11], "temperature": 0.0, "avg_logprob": -0.06913711323457605, "compression_ratio": 1.537117903930131, "no_speech_prob": 6.493170803878456e-05}, {"id": 1379, "seek": 973656, "start": 9758.64, "end": 9764.24, "text": " which makes recursive self-improvement leading to evil superhuman omniscience logic based", "tokens": [597, 1669, 20560, 488, 2698, 12, 332, 46955, 518, 5775, 281, 6724, 1687, 18796, 3406, 10661, 6699, 9952, 2361], "temperature": 0.0, "avg_logprob": -0.06913711323457605, "compression_ratio": 1.537117903930131, "no_speech_prob": 6.493170803878456e-05}, {"id": 1380, "seek": 976424, "start": 9764.24, "end": 9771.52, "text": " godlike artificial general intelligence a 20th century reductionist AI myth. We must focus on", "tokens": [3044, 4092, 11677, 2674, 7599, 257, 945, 392, 4901, 11004, 468, 7318, 9474, 13, 492, 1633, 1879, 322], "temperature": 0.0, "avg_logprob": -0.08916292190551758, "compression_ratio": 1.594142259414226, "no_speech_prob": 5.6944554671645164e-05}, {"id": 1381, "seek": 976424, "start": 9771.52, "end": 9779.84, "text": " artificial general learners. Afterward, science was created to stop people from overrating correlations", "tokens": [11677, 2674, 23655, 13, 2381, 1007, 11, 3497, 390, 2942, 281, 1590, 561, 490, 670, 8754, 13983, 763], "temperature": 0.0, "avg_logprob": -0.08916292190551758, "compression_ratio": 1.594142259414226, "no_speech_prob": 5.6944554671645164e-05}, {"id": 1382, "seek": 976424, "start": 9779.84, "end": 9785.44, "text": " and jumping to erroneous conclusions on scant evidence and then sharing those conclusions", "tokens": [293, 11233, 281, 1189, 26446, 563, 22865, 322, 795, 394, 4467, 293, 550, 5414, 729, 22865], "temperature": 0.0, "avg_logprob": -0.08916292190551758, "compression_ratio": 1.594142259414226, "no_speech_prob": 5.6944554671645164e-05}, {"id": 1383, "seek": 976424, "start": 9785.44, "end": 9792.96, "text": " with others, leading to compounded mistakes and much wasted effort. Consequently, promoting a", "tokens": [365, 2357, 11, 5775, 281, 14154, 292, 8038, 293, 709, 19496, 4630, 13, 2656, 46027, 11, 16383, 257], "temperature": 0.0, "avg_logprob": -0.08916292190551758, "compression_ratio": 1.594142259414226, "no_speech_prob": 5.6944554671645164e-05}, {"id": 1384, "seek": 979296, "start": 9792.96, "end": 9799.839999999998, "text": " holistic stance has long been a career-ending move in academia, and especially in computer science.", "tokens": [30334, 21033, 575, 938, 668, 257, 3988, 12, 2029, 1286, 294, 28937, 11, 293, 2318, 294, 3820, 3497, 13], "temperature": 0.0, "avg_logprob": -0.07861618879364758, "compression_ratio": 1.6593886462882097, "no_speech_prob": 4.983711914974265e-05}, {"id": 1385, "seek": 979296, "start": 9800.56, "end": 9807.439999999999, "text": " But now we suddenly have machine learning that performs cognitive tasks such as protein folding,", "tokens": [583, 586, 321, 5800, 362, 3479, 2539, 300, 26213, 15605, 9608, 1270, 382, 7944, 25335, 11], "temperature": 0.0, "avg_logprob": -0.07861618879364758, "compression_ratio": 1.6593886462882097, "no_speech_prob": 4.983711914974265e-05}, {"id": 1386, "seek": 979296, "start": 9807.439999999999, "end": 9814.24, "text": " playing go, and estimating house prices at useful levels using exactly a holistic stance.", "tokens": [2433, 352, 11, 293, 8017, 990, 1782, 7901, 412, 4420, 4358, 1228, 2293, 257, 30334, 21033, 13], "temperature": 0.0, "avg_logprob": -0.07861618879364758, "compression_ratio": 1.6593886462882097, "no_speech_prob": 4.983711914974265e-05}, {"id": 1387, "seek": 979296, "start": 9814.88, "end": 9822.08, "text": " So now science itself has a cognitive dissonance. This is a conflict about what science is or", "tokens": [407, 586, 3497, 2564, 575, 257, 15605, 717, 3015, 719, 13, 639, 307, 257, 6596, 466, 437, 3497, 307, 420], "temperature": 0.0, "avg_logprob": -0.07861618879364758, "compression_ratio": 1.6593886462882097, "no_speech_prob": 4.983711914974265e-05}, {"id": 1388, "seek": 982208, "start": 9822.08, "end": 9829.12, "text": " should be. Inherence of these stances leads people to develop significant personal cognitive", "tokens": [820, 312, 13, 682, 511, 655, 295, 613, 342, 2676, 6689, 561, 281, 1499, 4776, 2973, 15605], "temperature": 0.0, "avg_logprob": -0.12157020163028798, "compression_ratio": 1.5614754098360655, "no_speech_prob": 0.0001534220646135509}, {"id": 1389, "seek": 982208, "start": 9829.12, "end": 9834.56, "text": " dissonances, which is why discussions about these issues are very unpopular among people", "tokens": [717, 3015, 2676, 11, 597, 307, 983, 11088, 466, 613, 2663, 366, 588, 517, 42376, 3654, 561], "temperature": 0.0, "avg_logprob": -0.12157020163028798, "compression_ratio": 1.5614754098360655, "no_speech_prob": 0.0001534220646135509}, {"id": 1390, "seek": 982208, "start": 9834.56, "end": 9843.36, "text": " with solid STEM educations. But the dichotomy is real. We need to deal with it. Our choices so far", "tokens": [365, 5100, 25043, 2400, 763, 13, 583, 264, 10390, 310, 8488, 307, 957, 13, 492, 643, 281, 2028, 365, 309, 13, 2621, 7994, 370, 1400], "temperature": 0.0, "avg_logprob": -0.12157020163028798, "compression_ratio": 1.5614754098360655, "no_speech_prob": 0.0001534220646135509}, {"id": 1391, "seek": 984336, "start": 9843.36, "end": 9852.960000000001, "text": " seem to have been too. Claim that dichotomy doesn't exist. But Schrodinger and Pursig also discuss it.", "tokens": [1643, 281, 362, 668, 886, 13, 383, 10970, 300, 10390, 310, 8488, 1177, 380, 2514, 13, 583, 2065, 340, 3584, 260, 293, 430, 2156, 328, 611, 2248, 309, 13], "temperature": 0.0, "avg_logprob": -0.12967836229424728, "compression_ratio": 1.5879120879120878, "no_speech_prob": 8.327515388373286e-05}, {"id": 1392, "seek": 984336, "start": 9852.960000000001, "end": 9860.480000000001, "text": " Claim that the holistic stance doesn't work. But deep learning works. Claim that reductionist", "tokens": [383, 10970, 300, 264, 30334, 21033, 1177, 380, 589, 13, 583, 2452, 2539, 1985, 13, 383, 10970, 300, 11004, 468], "temperature": 0.0, "avg_logprob": -0.12967836229424728, "compression_ratio": 1.5879120879120878, "no_speech_prob": 8.327515388373286e-05}, {"id": 1393, "seek": 984336, "start": 9860.480000000001, "end": 9867.84, "text": " methods are requirement, hobbling our toolkits for a principle. The reductionist stance also", "tokens": [7150, 366, 11695, 11, 12959, 18262, 527, 2290, 74, 1208, 337, 257, 8665, 13, 440, 11004, 468, 21033, 611], "temperature": 0.0, "avg_logprob": -0.12967836229424728, "compression_ratio": 1.5879120879120878, "no_speech_prob": 8.327515388373286e-05}, {"id": 1394, "seek": 986784, "start": 9867.84, "end": 9875.2, "text": " makes it difficult to imagine and accept things like systems capable of autonomous epistemic", "tokens": [1669, 309, 2252, 281, 3811, 293, 3241, 721, 411, 3652, 8189, 295, 23797, 2388, 468, 3438], "temperature": 0.0, "avg_logprob": -0.058102348021098545, "compression_ratio": 1.6851851851851851, "no_speech_prob": 3.9622816984774545e-05}, {"id": 1395, "seek": 986784, "start": 9875.2, "end": 9883.36, "text": " reduction, systems that do not have a goal function, systems that improve with practice,", "tokens": [11004, 11, 3652, 300, 360, 406, 362, 257, 3387, 2445, 11, 3652, 300, 3470, 365, 3124, 11], "temperature": 0.0, "avg_logprob": -0.058102348021098545, "compression_ratio": 1.6851851851851851, "no_speech_prob": 3.9622816984774545e-05}, {"id": 1396, "seek": 986784, "start": 9884.72, "end": 9891.84, "text": " systems that exploit emergent effects, systems that by themselves make decisions about what", "tokens": [3652, 300, 25924, 4345, 6930, 5065, 11, 3652, 300, 538, 2969, 652, 5327, 466, 437], "temperature": 0.0, "avg_logprob": -0.058102348021098545, "compression_ratio": 1.6851851851851851, "no_speech_prob": 3.9622816984774545e-05}, {"id": 1397, "seek": 989184, "start": 9891.84, "end": 9898.880000000001, "text": " matters most, systems that occasionally give a wrong answer but are nevertheless very useful.", "tokens": [7001, 881, 11, 3652, 300, 16895, 976, 257, 2085, 1867, 457, 366, 26924, 588, 4420, 13], "temperature": 0.0, "avg_logprob": -0.09689796672147863, "compression_ratio": 1.5591836734693878, "no_speech_prob": 5.211649840930477e-05}, {"id": 1398, "seek": 989184, "start": 9899.6, "end": 9906.32, "text": " So after a serious education in machine learning we don't actually need to do almost any programming", "tokens": [407, 934, 257, 3156, 3309, 294, 3479, 2539, 321, 500, 380, 767, 643, 281, 360, 1920, 604, 9410], "temperature": 0.0, "avg_logprob": -0.09689796672147863, "compression_ratio": 1.5591836734693878, "no_speech_prob": 5.211649840930477e-05}, {"id": 1399, "seek": 989184, "start": 9906.32, "end": 9913.28, "text": " at all. And we don't need to understand anybody else's problem domains. Because we don't have", "tokens": [412, 439, 13, 400, 321, 500, 380, 643, 281, 1223, 4472, 1646, 311, 1154, 25514, 13, 1436, 321, 500, 380, 362], "temperature": 0.0, "avg_logprob": -0.09689796672147863, "compression_ratio": 1.5591836734693878, "no_speech_prob": 5.211649840930477e-05}, {"id": 1400, "seek": 989184, "start": 9913.28, "end": 9921.04, "text": " to perform any epistemic reduction ourselves. We should recognize this for what it is. AI was", "tokens": [281, 2042, 604, 2388, 468, 3438, 11004, 4175, 13, 492, 820, 5521, 341, 337, 437, 309, 307, 13, 7318, 390], "temperature": 0.0, "avg_logprob": -0.09689796672147863, "compression_ratio": 1.5591836734693878, "no_speech_prob": 5.211649840930477e-05}, {"id": 1401, "seek": 992104, "start": 9921.04, "end": 9927.2, "text": " supposed to solve our problems for us so we would not have to learn or understand any new", "tokens": [3442, 281, 5039, 527, 2740, 337, 505, 370, 321, 576, 406, 362, 281, 1466, 420, 1223, 604, 777], "temperature": 0.0, "avg_logprob": -0.053633152052413584, "compression_ratio": 1.6053811659192825, "no_speech_prob": 3.700221350300126e-05}, {"id": 1402, "seek": 992104, "start": 9927.2, "end": 9934.880000000001, "text": " problem domains. To not have to think. And that's what we have today, in machine learning,", "tokens": [1154, 25514, 13, 1407, 406, 362, 281, 519, 13, 400, 300, 311, 437, 321, 362, 965, 11, 294, 3479, 2539, 11], "temperature": 0.0, "avg_logprob": -0.053633152052413584, "compression_ratio": 1.6053811659192825, "no_speech_prob": 3.700221350300126e-05}, {"id": 1403, "seek": 992104, "start": 9934.880000000001, "end": 9941.68, "text": " and with holistic methods in general. Why are some people surprised or unhappy about this?", "tokens": [293, 365, 30334, 7150, 294, 2674, 13, 1545, 366, 512, 561, 6100, 420, 22172, 466, 341, 30], "temperature": 0.0, "avg_logprob": -0.053633152052413584, "compression_ratio": 1.6053811659192825, "no_speech_prob": 3.700221350300126e-05}, {"id": 1404, "seek": 992104, "start": 9942.400000000001, "end": 9948.880000000001, "text": " In my opinion, this is AI, this is what we have been trying to accomplish for decades.", "tokens": [682, 452, 4800, 11, 341, 307, 7318, 11, 341, 307, 437, 321, 362, 668, 1382, 281, 9021, 337, 7878, 13], "temperature": 0.0, "avg_logprob": -0.053633152052413584, "compression_ratio": 1.6053811659192825, "no_speech_prob": 3.700221350300126e-05}, {"id": 1405, "seek": 994888, "start": 9948.88, "end": 9956.16, "text": " People who claim machine understanding is not AI are asking for human level human-centric reasoning", "tokens": [3432, 567, 3932, 3479, 3701, 307, 406, 7318, 366, 3365, 337, 1952, 1496, 1952, 12, 45300, 21577], "temperature": 0.0, "avg_logprob": -0.20094904532799354, "compression_ratio": 1.6462882096069869, "no_speech_prob": 7.689523772569373e-05}, {"id": 1406, "seek": 994888, "start": 9956.16, "end": 9962.88, "text": " and are, at their peril, blind to the nascent ML-based understanding we can achieve today.", "tokens": [293, 366, 11, 412, 641, 46118, 11, 6865, 281, 264, 5382, 2207, 21601, 12, 6032, 3701, 321, 393, 4584, 965, 13], "temperature": 0.0, "avg_logprob": -0.20094904532799354, "compression_ratio": 1.6462882096069869, "no_speech_prob": 7.689523772569373e-05}, {"id": 1407, "seek": 994888, "start": 9963.519999999999, "end": 9969.839999999998, "text": " With expected reasonable improvements in machine understanding capabilities, familiarity and", "tokens": [2022, 5176, 10585, 13797, 294, 3479, 3701, 10862, 11, 49828, 293], "temperature": 0.0, "avg_logprob": -0.20094904532799354, "compression_ratio": 1.6462882096069869, "no_speech_prob": 7.689523772569373e-05}, {"id": 1408, "seek": 994888, "start": 9969.839999999998, "end": 9976.72, "text": " acceptance of the holistic stance will become a requirement for ML and AI-based work. It will", "tokens": [20351, 295, 264, 30334, 21033, 486, 1813, 257, 11695, 337, 21601, 293, 7318, 12, 6032, 589, 13, 467, 486], "temperature": 0.0, "avg_logprob": -0.20094904532799354, "compression_ratio": 1.6462882096069869, "no_speech_prob": 7.689523772569373e-05}, {"id": 1409, "seek": 997672, "start": 9976.72, "end": 9985.039999999999, "text": " likely take years for our educational system to adjust. This has been Anonika's Little Pills,", "tokens": [3700, 747, 924, 337, 527, 10189, 1185, 281, 4369, 13, 639, 575, 668, 1107, 266, 5439, 311, 8022, 430, 2565, 11], "temperature": 0.0, "avg_logprob": -0.2783777588292172, "compression_ratio": 1.2307692307692308, "no_speech_prob": 0.00012261922529432923}, {"id": 1410, "seek": 998504, "start": 9985.04, "end": 10007.76, "text": " led to you by a computer. Thank you for listening.", "tokens": [50364, 4684, 281, 291, 538, 257, 3820, 13, 1044, 291, 337, 4764, 13, 51500], "temperature": 0.0, "avg_logprob": -0.2744297981262207, "compression_ratio": 0.9090909090909091, "no_speech_prob": 0.0003513780829962343}], "language": "en"}